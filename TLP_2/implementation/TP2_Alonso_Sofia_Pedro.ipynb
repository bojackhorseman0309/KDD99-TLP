{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1\n",
    "\n",
    "Estudiantes:\n",
    "\n",
    "- Alonso Araya Calvo\n",
    "- Pedro Soto\n",
    "- Sofia Oviedo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLfMPjjz6Tqf"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lh6x4GjDbzO"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import *\n",
    "from scipy import stats\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.utils import get_file\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_contour,\n",
    "    plot_slice,\n",
    "    plot_edf,\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "2pn_P8kZ6a4G",
    "outputId": "0b7a8865-e1a2-4bd7-f826-4dfff161e1ae"
   },
   "outputs": [],
   "source": [
    "#tomado de https://www.kaggle.com/code/wailinnoo/intrusion-detection-system-using-kdd99-dataset\n",
    "try:\n",
    "    path = get_file('kddcup.data_10_percent.gz',\n",
    "                    origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
    "except:\n",
    "    print('Error downloading')\n",
    "    raise\n",
    "\n",
    "print(path)\n",
    "\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "# Download from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "pd_data_frame = pd.read_csv(path, header=None)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "pd_data_frame.columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'outcome'\n",
    "]\n",
    "\n",
    "#describe the dataset\n",
    "#transform nominal features using a one hot vector encoding\n",
    "pd_data_frame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWfo9Anw8lIj"
   },
   "source": [
    "# Preprocessing and  Dummy encoding transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyQm_Rkb7j-O"
   },
   "source": [
    "Attacks fall into four main categories:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   DOS: denial-of-service, e.g. syn flood;\n",
    "2.   R2L: unauthorized access from a remote machine, e.g. guessing password;\n",
    "\n",
    "3. U2R:  unauthorized access to local superuser (root) privileges, e.g., various ``buffer overflow'' attacks;\n",
    "\n",
    "4. probing: surveillance and other probing, e.g., port scanning.\n",
    "\n",
    "\n",
    "It is important to note that the test data is not from the same probability distribution as the training data, and it includes specific attack types not in the training data.  This makes the task more realistic.  Some intrusion experts believe that most novel attacks are variants of known attacks and the \"signature\" of known attacks can be sufficient to catch novel variants.  The datasets contain a total of 24 training attack types, with an additional 14 types in the test data only.\n",
    "\n",
    "Neptune is the most frequent attack in the dataset, consisting in a type of denial-of-service (DoS) attack overwhelming systems with SYN requests (ynchronize request, is a type of network packet used in the TCP handshake to initiate a connection between a client and a server).\n",
    "\n",
    "**In this work we will focus in the detection of backdrop attacks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gc6X0vQv5ad6",
    "outputId": "871f9829-f324-4483-bf8d-ab2bfe15accf"
   },
   "outputs": [],
   "source": [
    "# For now, just drop NA's (rows with missing values), in case there are\n",
    "pd_data_frame.dropna(inplace=True, axis=1)\n",
    "\n",
    "# Checkng for DUPLICATE values\n",
    "pd_data_frame.drop_duplicates(keep='first', inplace=True)\n",
    "print(pd_data_frame.describe())\n",
    "filtered_df = pd_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "IPAlBcwx6PFC",
    "outputId": "f938e77e-5ab4-4035-d81e-3bb39d415216"
   },
   "outputs": [],
   "source": [
    "# distribution of the attack categories (outcome)\n",
    "# Exploratory data analysis\n",
    "plt.figure(figsize=(15, 7))\n",
    "class_distribution = pd_data_frame['outcome'].value_counts()\n",
    "class_distribution.plot(kind='bar')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Data points per Class')\n",
    "plt.title('Distribution of yi in train data')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#normal and neptune are the more prominent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v8NO_C3d8ox2",
    "outputId": "ce011211-d5af-46c6-951b-189cfdb97f8d"
   },
   "outputs": [],
   "source": [
    "list_nominal_features = [\"flag\", \"protocol_type\", \"service\"]\n",
    "\n",
    "# Apply one-hot encoding to the nominal features\n",
    "df_encoded = pd.get_dummies(filtered_df, columns=list_nominal_features)\n",
    "\n",
    "# Convert boolean columns (from one-hot encoding) to integers (0 or 1) in df_encoded\n",
    "for col in df_encoded.columns:\n",
    "    if df_encoded[col].dtype == 'bool':\n",
    "        df_encoded[col] = df_encoded[col].astype(int)\n",
    "\n",
    "# Display the first few rows of the modified DataFrame to verify\n",
    "print(\"DataFrame with boolean columns converted to integers:\")\n",
    "display(df_encoded.describe())\n",
    "print(\"Columns after nominal attributes encoded: \")\n",
    "for i in df_encoded.columns:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. (30 puntos) Implementacion de un arbol de decision y random forests para clasificar todos los tipos de ataques (scikit learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Genere una funcion split_dataset la cual divida los datos en entrenamiento (70 %), validacion (15 %) y prueba (15 %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, target_column='outcome', test_size=0.15, val_size=0.15, random_state=42, show_sizes=False):\n",
    "    \"\"\"\n",
    "    Función que parte el dataset en conjuntos de entrenamiento, validación y prueba\n",
    "    por medio de los parametros solicitados.\n",
    "    \n",
    "    Parámetros:\n",
    "        df : DataFrame de pandas con los datos del dataset\n",
    "        target_column : Nombre de la columna de las clases en string\n",
    "        test_size : Proporción numerica del conjunto de prueba\n",
    "        val_size : Proporción numerica del conjunto de validación\n",
    "        random_state : Seed para poder reproducir los resultados\n",
    "    \n",
    "    Salida:\n",
    "        tupla con esta forma:\n",
    "            (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "        X_train: Dataframe con las caracteristicas de entrenamiento\n",
    "        X_val: Dataframe con las caracteristicas de validación\n",
    "        X_test: Dataframe con las caracteristicas de prueba\n",
    "        y_train: Series con las clases de entrenamiento\n",
    "        y_val: Series con las clases de validación\n",
    "        y_test: Series con las clases de prueba\n",
    "    \"\"\"\n",
    "\n",
    "    # Separo las caracteristicas de KDD99 y las clases de outcome\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # En el primer split se separan los datos\n",
    "    # de entrenamiento y validación de los de prueba\n",
    "    # generando el split de prueba\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    # Ajustar el tamaño de validación respecto al conjunto temporal\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # 0.15 / 0.85 ≈ 0.176\n",
    "\n",
    "    # Para este segundo split se separa el conjunto de entrenamiento\n",
    "    # y validación, generando el split de validación y entrenamiento\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    if show_sizes:\n",
    "        print(f\"Tamaño del conjunto completo: {len(df)}\")\n",
    "        print(f\"Entrenamiento: {len(X_train)} ({len(X_train) / len(df) * 100}%)\")\n",
    "        print(f\"Validación: {len(X_val)} ({len(X_val) / len(df) * 100}%)\")\n",
    "        print(f\"Prueba: {len(X_test)} ({len(X_test) / len(df) * 100}%)\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df_encoded, show_sizes=True)\n",
    "\n",
    "# Verificación adicional de la distribución de clases\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Distribucion de clases en los distintos dataset\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nDistribución en conjunto completo:\")\n",
    "print(df_encoded['outcome'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nDistribución en conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nDistribución en conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nDistribución en conjunto de prueba:\")\n",
    "print(y_test.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 (15 puntos) Entrene un arbol de decision (de scikitlearn) para clasificar los ataques en todas las categorias originales del dataset:\n",
    "\n",
    "a) Optimice la profundidad maxima, cantidad minima de observaciones\n",
    "por particion, y el criterio de pureza usando optuna o weights and\n",
    "biases. Documente los rangos de cada hiper-parametro y justifique la\n",
    "decision por cada uno. Muestre los graficos de ese proceso de optimizacion\n",
    "y seleccione las 3 mejores arquitecturas.\n",
    "\n",
    "b) Compare las tres mejores arquitecturas, para al menos 10 corridas\n",
    "diferentes (particiones), el F1-score promedio para todas las clases\n",
    "y la tasa de falsos positivos promedio para todas las clases, presente\n",
    "medias y desviaciones estandar usando la particion de prueba.\n",
    "Comente los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización con Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_decision_tree(trial):\n",
    "    \"\"\"\n",
    "    Función para optimizar hiperparámetros del árbol de decisión usando Optuna.\n",
    "    \n",
    "    Parámetros:\n",
    "        trial: Objeto de Optuna que sugiere valores para los hiperparámetros\n",
    "        \n",
    "    Retorna:\n",
    "        f1_macro: F1-Score promedio macro en el conjunto de validación\n",
    "    \"\"\"\n",
    "\n",
    "    # JUSTIFICACIÓN DE RANGOS DE HIPERPARÁMETROS:\n",
    "\n",
    "    # 1. max_depth: Controla la profundidad máxima del árbol\n",
    "    # Rango [3, 20]: \n",
    "    # - Valores < 3: Muy shallow, probable underfitting\n",
    "    # - Valores > 20: Muy profundo, probable overfitting con ruido\n",
    "    # - Para KDD99 con ~100k samples, este rango permite flexibilidad\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "\n",
    "    # 2. min_samples_split: Número mínimo de muestras para dividir un nodo\n",
    "    # Rango [2, 50]:\n",
    "    # - Valor 2: Más agresivo, puede sobreajustar\n",
    "    # - Valores > 50: Muy conservador, puede perder patrones\n",
    "    # - Basado en literatura de IDS, valores medios funcionan mejor\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 50)\n",
    "\n",
    "    # 3. min_samples_leaf: Número mínimo de muestras en una hoja\n",
    "    # Rango [1, 25]:\n",
    "    # - Debe ser ≤ min_samples_split/2 para ser efectivo\n",
    "    # - Previene hojas con muy pocas muestras (posible ruido)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "\n",
    "    # 4. criterion: Criterio de pureza para divisiones\n",
    "    # 'gini': Más rápido computacionalmente\n",
    "    # 'entropy': Puede ser más preciso en datasets multiclase complejos\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    # Crear el modelo con hiperparámetros sugeridos\n",
    "    dt = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        criterion=criterion,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar en conjunto de validación\n",
    "    y_pred = dt.predict(X_val)\n",
    "\n",
    "    # Se calcula el F1-Score promedio macro para todas las clases\n",
    "    f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    return f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ejecuta optuna para crear las optimizaciones\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Maximizar F1-score\n",
    "    study_name='DecisionTree_KDD99_Optimization',\n",
    "    storage='sqlite:///decision_tree_optimization.db'\n",
    ")\n",
    "\n",
    "print(\"Iniciando optimización de hiperparámetros con Optuna...\")\n",
    "\n",
    "# 100 pruebas de Optuna que son suficientes para encontrar las mejores arquitecturas\n",
    "n_trials = 100\n",
    "study.optimize(optimize_decision_tree, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Resultados de la optimización con Optuna\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Número total de trials: {len(study.trials)}\")\n",
    "print(f\"Mejor F1-macro encontrado: {study.best_value:.4f}\")\n",
    "print(\"\\nMejores hiperparámetros:\")\n",
    "for param, value in study.best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Obtener las 3 mejores arquitecturas\n",
    "best_trials = sorted(study.trials, key=lambda x: x.value if x.value is not None else -1, reverse=True)[:3]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Las 3 mejores arquitecturas encontradas\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "top_3_configs = []\n",
    "for i, trial in enumerate(best_trials, 1):\n",
    "    print(f\"\\nArquitectura #{i}:\")\n",
    "    print(f\"  F1-macro: {trial.value:.4f}\")\n",
    "    print(\"  Hiperparámetros:\")\n",
    "    for param, value in trial.params.items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "\n",
    "    # Guardar configuración para usar los datos en las celdas siguientes\n",
    "    top_3_configs.append({\n",
    "        'params': trial.params,\n",
    "        'f1_score': trial.value,\n",
    "        'rank': i\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadisticas y Graficos de Proceso de Optimizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones del proceso de optimización con Optuna\n",
    "\n",
    "# 1) Historia de la optimización\n",
    "plot_optimization_history(study); plt.show()\n",
    "\n",
    "# 2) Importancia de hiperparámetros\n",
    "plot_param_importances(study); plt.show()\n",
    "\n",
    "# 3) Relaciones entre hiperparámetros\n",
    "plot_parallel_coordinate(study); plt.show()\n",
    "plot_contour(\n",
    "    study,\n",
    "    params=['max_depth', 'min_samples_split', 'min_samples_leaf', 'criterion']\n",
    "); plt.show()\n",
    "plot_slice(study); plt.show()\n",
    "\n",
    "# 4) Distribución de valores objetivo \n",
    "plot_edf(study); plt.show()\n",
    "\n",
    "# 5) Comparación visual de las 3 mejores arquitecturas (se mantiene)\n",
    "top_3_f1 = [config['f1_score'] for config in top_3_configs]\n",
    "top_3_names = [f\"Arquitectura #{config['rank']}\" for config in top_3_configs]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(top_3_names, top_3_f1, color=['gold', 'silver', '#CD7F32'], alpha=0.8)\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('Las 3 Mejores Arquitecturas')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, value in zip(bars, top_3_f1):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.0005,\n",
    "             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de la optimización\n",
    "f1_scores = [trial.value for trial in study.trials if trial.value is not None]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Estadísticas de la optimización\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"F1-score promedio: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Desviación estándar: {np.std(f1_scores):.4f}\")\n",
    "print(f\"F1-score mínimo: {np.min(f1_scores):.4f}\")\n",
    "print(f\"F1-score máximo: {np.max(f1_scores):.4f}\")\n",
    "print(f\"Mejora sobre promedio: {((study.best_value - np.mean(f1_scores)) / np.mean(f1_scores) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparacion y Evaluacion de las Arquitecturas con Particiones Diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dt_architecture_multiple_runs(params, n_runs=10, random_seeds=None):\n",
    "    \"\"\"\n",
    "    Evalúa una arquitectura de árbol de decisión con múltiples corridas aleatorias.\n",
    "    \n",
    "    Parámetros:\n",
    "        params: Diccionario con hiperparámetros del árbol\n",
    "        n_runs: Número de corridas independientes (default: 10)\n",
    "        random_seeds: Lista de seeds aleatorios (opcional)\n",
    "        \n",
    "    Retorna:\n",
    "        results: Diccionario con métricas agregadas y detalladas\n",
    "    \"\"\"\n",
    "\n",
    "    if random_seeds is None:\n",
    "        random_seeds = list(range(42, 42 + n_runs))\n",
    "\n",
    "    all_f1_scores = []\n",
    "    all_accuracies = []\n",
    "    all_fpr_rates = []  # Tasas de Falsos Positivos\n",
    "    all_detailed_metrics = []\n",
    "\n",
    "    print(f\"Evaluando arquitectura con {n_runs} corridas independientes...\")\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        seed = random_seeds[run]\n",
    "\n",
    "        # Crear nueva partición aleatoria de datos\n",
    "        X_train_run, X_val_run, X_test_run, y_train_run, y_val_run, y_test_run = split_dataset(\n",
    "            df_encoded, random_state=seed\n",
    "        )\n",
    "\n",
    "        # Crear y entrenar modelo con parámetros dados\n",
    "        dt = DecisionTreeClassifier(**params, random_state=seed)\n",
    "        dt.fit(X_train_run, y_train_run)\n",
    "\n",
    "        # Predicciones en conjunto de prueba\n",
    "        y_pred_test = dt.predict(X_test_run)\n",
    "\n",
    "        # Calcular métricas\n",
    "        f1_macro = f1_score(y_test_run, y_pred_test, average='macro')\n",
    "        accuracy = accuracy_score(y_test_run, y_pred_test)\n",
    "\n",
    "        # Métricas por clase\n",
    "        f1_per_class = f1_score(y_test_run, y_pred_test, average=None, labels=dt.classes_)\n",
    "        precision_per_class, recall_per_class, _, _ = precision_recall_fscore_support(\n",
    "            y_test_run, y_pred_test, average=None, labels=dt.classes_\n",
    "        )\n",
    "\n",
    "        # Calcular matriz de confusión\n",
    "        cm = confusion_matrix(y_test_run, y_pred_test, labels=dt.classes_)\n",
    "\n",
    "        # Calcular tasa de falsos positivos por clase\n",
    "        fpr_per_class = []\n",
    "        for i in range(len(dt.classes_)):\n",
    "            # FPR = FP / (FP + TN)\n",
    "            fp = cm[:, i].sum() - cm[i, i]  # Falsos positivos para clase i\n",
    "            tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])  # Verdaderos negativos\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            fpr_per_class.append(fpr)\n",
    "\n",
    "        avg_fpr = np.mean(fpr_per_class)\n",
    "\n",
    "        # Recopilar los resultados de la corrida\n",
    "        all_f1_scores.append(f1_macro)\n",
    "        all_accuracies.append(accuracy)\n",
    "        all_fpr_rates.append(avg_fpr)\n",
    "        all_detailed_metrics.append({\n",
    "            'run': run + 1,\n",
    "            'seed': seed,\n",
    "            'f1_macro': f1_macro,\n",
    "            'accuracy': accuracy,\n",
    "            'avg_fpr': avg_fpr,\n",
    "            'f1_per_class': f1_per_class,\n",
    "            'precision_per_class': precision_per_class,\n",
    "            'recall_per_class': recall_per_class,\n",
    "            'fpr_per_class': fpr_per_class,\n",
    "            'classes': dt.classes_\n",
    "        })\n",
    "\n",
    "        print(f\"Corrida {run + 1:2d}/{n_runs} - F1: {f1_macro:.4f}, Accuracy: {accuracy:.4f}, FPR: {avg_fpr:.4f}\")\n",
    "\n",
    "    # Calcular estadísticas agregadas\n",
    "    results = {\n",
    "        'n_runs': n_runs,\n",
    "        'params': params,\n",
    "        'f1_scores': all_f1_scores,\n",
    "        'accuracies': all_accuracies,\n",
    "        'fpr_rates': all_fpr_rates,\n",
    "        'detailed_metrics': all_detailed_metrics,\n",
    "        'statistics': {\n",
    "            'f1_mean': np.mean(all_f1_scores),\n",
    "            'f1_std': np.std(all_f1_scores),\n",
    "            'accuracy_mean': np.mean(all_accuracies),\n",
    "            'accuracy_std': np.std(all_accuracies),\n",
    "            'fpr_mean': np.mean(all_fpr_rates),\n",
    "            'fpr_std': np.std(all_fpr_rates)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar las 3 mejores arquitecturas con 10 corridas cada una\n",
    "print(\"=\" * 70)\n",
    "print(\"Evaluación de las 3 mejores arquitecturas con 10 corridas\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for i, config in enumerate(top_3_configs):\n",
    "    print(f\"\\n{'-' * 50}\")\n",
    "    print(f\"Evaluando Arquitectura #{config['rank']}\")\n",
    "    print(\"Hiperparámetros:\")\n",
    "    for param, value in config['params'].items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"{'-' * 50}\")\n",
    "\n",
    "    # Evaluar con 10 corridas independientes usando seeds únicos\n",
    "    results = evaluate_dt_architecture_multiple_runs(\n",
    "        params=config['params'],\n",
    "        n_runs=10,\n",
    "        random_seeds=list(range(100 + i * 10, 110 + i * 10))\n",
    "    )\n",
    "\n",
    "    # Añadir información adicional\n",
    "    results['architecture_rank'] = config['rank']\n",
    "    results['optimization_f1'] = config['f1_score']\n",
    "    evaluation_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla resumen comprehensiva\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"Tabla resumen de resultados - Evaluación de árboles de decisión\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Generar tabla con estadísticas detalladas\n",
    "summary_data = []\n",
    "for result in evaluation_results:\n",
    "    summary_data.append({\n",
    "        'Arquitectura': f\"#{result['architecture_rank']}\",\n",
    "        'F1-macro Media': f\"{result['statistics']['f1_mean']:.4f}\",\n",
    "        'F1-macro Desviación Estandar': f\"{result['statistics']['f1_std']:.4f}\",\n",
    "        'Accuracy Media': f\"{result['statistics']['accuracy_mean']:.4f}\",\n",
    "        'Accuracy Desviación Estandar': f\"{result['statistics']['accuracy_std']:.4f}\",\n",
    "        'FPR Media': f\"{result['statistics']['fpr_mean']:.4f}\",\n",
    "        'FPR Desviación Estandar': f\"{result['statistics']['fpr_std']:.4f}\",\n",
    "        'max_depth': result['params']['max_depth'],\n",
    "        'min_samples_split': result['params']['min_samples_split'],\n",
    "        'min_samples_leaf': result['params']['min_samples_leaf'],\n",
    "        'criterion': result['params']['criterion']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Visualizaciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Comparación de F1-scores con barras de error\n",
    "architectures = [f\"Arq #{r['architecture_rank']}\" for r in evaluation_results]\n",
    "f1_means = [r['statistics']['f1_mean'] for r in evaluation_results]\n",
    "f1_stds = [r['statistics']['f1_std'] for r in evaluation_results]\n",
    "\n",
    "bars1 = axes[0, 0].bar(architectures, f1_means, yerr=f1_stds, capsize=8,\n",
    "                       color=['gold', 'silver', '#CD7F32'], alpha=0.8, edgecolor='black')\n",
    "axes[0, 0].set_ylabel('F1-score')\n",
    "axes[0, 0].set_title('Comparación F1-score (Media ± Desviación Estándar)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores sobre las barras\n",
    "for bar, mean, std in zip(bars1, f1_means, f1_stds):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + std + 0.001,\n",
    "                    f'{mean:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Comparación de tasas de falsos positivos\n",
    "fpr_means = [r['statistics']['fpr_mean'] for r in evaluation_results]\n",
    "fpr_stds = [r['statistics']['fpr_std'] for r in evaluation_results]\n",
    "\n",
    "bars2 = axes[0, 1].bar(architectures, fpr_means, yerr=fpr_stds, capsize=8,\n",
    "                       color=['gold', 'silver', '#CD7F32'], alpha=0.8, edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Tasa de Falsos Positivos')\n",
    "axes[0, 1].set_title('Comparación FPR (Media ± Desviación Estándar)')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores sobre las barras\n",
    "for bar, mean, std in zip(bars2, fpr_means, fpr_stds):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + std + 0.0005,\n",
    "                    f'{mean:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Distribuciones de F1-scores (Box plots)\n",
    "f1_data = [r['f1_scores'] for r in evaluation_results]\n",
    "box1 = axes[1, 0].boxplot(f1_data, labels=architectures, patch_artist=True)\n",
    "colors = ['gold', 'silver', '#CD7F32']\n",
    "for patch, color in zip(box1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.8)\n",
    "axes[1, 0].set_ylabel('F1-score')\n",
    "axes[1, 0].set_title('Distribución de F1-scores por Arquitectura')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Distribuciones de FPR (Box plots)\n",
    "fpr_data = [r['fpr_rates'] for r in evaluation_results]\n",
    "box2 = axes[1, 1].boxplot(fpr_data, labels=architectures, patch_artist=True)\n",
    "for patch, color in zip(box2['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.8)\n",
    "axes[1, 1].set_ylabel('Tasa de Falsos Positivos')\n",
    "axes[1, 1].set_title('Distribución de FPR por Arquitectura')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis estadístico de diferencias\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Análisis estadístico de diferencias entre arquitecturas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extraer datos para análisis estadístico\n",
    "f1_arch1 = evaluation_results[0]['f1_scores']\n",
    "f1_arch2 = evaluation_results[1]['f1_scores']\n",
    "f1_arch3 = evaluation_results[2]['f1_scores']\n",
    "\n",
    "# ANOVA para F1-scores\n",
    "try:\n",
    "    f_stat_f1, p_val_f1 = stats.f_oneway(f1_arch1, f1_arch2, f1_arch3)\n",
    "    print(f\"ANOVA para F1-scores:\")\n",
    "    print(f\"  F-statistic: {f_stat_f1:.4f}\")\n",
    "    print(f\"  p-value: {p_val_f1:.6f}\")\n",
    "    print(f\"  Diferencias significativas: {'Sí' if p_val_f1 < 0.05 else 'No'} (α=0.05)\")\n",
    "\n",
    "    # Pruebas post-hoc si hay diferencias significativas\n",
    "    if p_val_f1 < 0.05:\n",
    "        print(f\"\\nPruebas post-hoc pareadas (t-test):\")\n",
    "\n",
    "        t_stat_12, p_val_12 = stats.ttest_rel(f1_arch1, f1_arch2)\n",
    "        print(f\"  Arq #1 vs Arq #2: t={t_stat_12:.4f}, p={p_val_12:.6f}\")\n",
    "\n",
    "        t_stat_13, p_val_13 = stats.ttest_rel(f1_arch1, f1_arch3)\n",
    "        print(f\"  Arq #1 vs Arq #3: t={t_stat_13:.4f}, p={p_val_13:.6f}\")\n",
    "\n",
    "        t_stat_23, p_val_23 = stats.ttest_rel(f1_arch2, f1_arch3)\n",
    "        print(f\"  Arq #2 vs Arq #3: t={t_stat_23:.4f}, p={p_val_23:.6f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error en análisis estadístico: {e}\")\n",
    "\n",
    "# Análisis de estabilidad (coeficiente de variación)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Análisis de estabilidad de arquitecturas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(evaluation_results):\n",
    "    arch_num = result['architecture_rank']\n",
    "    f1_cv = result['statistics']['f1_std'] / result['statistics']['f1_mean']\n",
    "    fpr_cv = result['statistics']['fpr_std'] / result['statistics']['fpr_mean'] if result['statistics'][\n",
    "                                                                                       'fpr_mean'] > 0 else 0\n",
    "\n",
    "    print(f\"\\nArquitectura #{arch_num}:\")\n",
    "    print(\n",
    "        f\"  Coeficiente de variación F1: {f1_cv:.4f} ({'Muy estable' if f1_cv < 0.005 else 'Estable' if f1_cv < 0.01 else 'Moderada' if f1_cv < 0.02 else 'Inestable'})\")\n",
    "    print(f\"  Coeficiente de variación FPR: {fpr_cv:.4f}\")\n",
    "\n",
    "    # Intervalos de confianza (95%)\n",
    "    f1_ci_lower = result['statistics']['f1_mean'] - 1.96 * result['statistics']['f1_std']\n",
    "    f1_ci_upper = result['statistics']['f1_mean'] + 1.96 * result['statistics']['f1_std']\n",
    "    print(f\"  Intervalo confianza F1 (95%): [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "\n",
    "# Identificar mejor arquitectura\n",
    "best_arch_idx = np.argmax([r['statistics']['f1_mean'] for r in evaluation_results])\n",
    "best_arch = evaluation_results[best_arch_idx]\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Arquitectura recomendada\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Mejor arquitectura: #{best_arch['architecture_rank']}\")\n",
    "print(f\"F1-score: {best_arch['statistics']['f1_mean']:.4f} ± {best_arch['statistics']['f1_std']:.4f}\")\n",
    "print(f\"FPR: {best_arch['statistics']['fpr_mean']:.4f} ± {best_arch['statistics']['fpr_std']:.4f}\")\n",
    "print(f\"Accuracy: {best_arch['statistics']['accuracy_mean']:.4f} ± {best_arch['statistics']['accuracy_std']:.4f}\")\n",
    "print(\"\\nHiperparámetros óptimos:\")\n",
    "for param, value in best_arch['params'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Comentarios sobre los resultados\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "ANÁLISIS DE RENDIMIENTO:\n",
    "1. Todas las arquitecturas muestran rendimiento excelente (F1-score > 0.99)\n",
    "2. Las diferencias entre arquitecturas son mínimas pero consistentes\n",
    "3. Tasas de falsos positivos muy bajas (< 0.01), crucial para sistemas IDS\n",
    "\n",
    "ESTABILIDAD:\n",
    "- Todas las arquitecturas muestran alta estabilidad entre corridas\n",
    "- Coeficientes de variación muy bajos indican robustez\n",
    "- Los intervalos de confianza son estrechos, sugiriendo confiabilidad\n",
    "\n",
    "SELECCIÓN DE MODELO:\n",
    "- La arquitectura #{best_arch['architecture_rank']} es la recomendada\n",
    "- Criterio '{best_arch['params']['criterion']}': {'Rápido y eficiente' if best_arch['params']['criterion'] == 'gini' else 'Más preciso para clases múltiples'}\n",
    "- Profundidad {best_arch['params']['max_depth']}: {'Permite interpretabilidad' if best_arch['params']['max_depth'] <= 10 else 'Alta capacidad de modelado'}\n",
    "- min_samples_split {best_arch['params']['min_samples_split']}: {'Previene overfitting' if best_arch['params']['min_samples_split'] > 10 else 'Permite alta flexibilidad'}\n",
    "\n",
    "IMPLICACIONES PRÁCTICAS:\n",
    "- Excelente para detección de intrusiones en tiempo real\n",
    "- Baja tasa de falsas alarmas minimiza fatiga de operadores\n",
    "- Alta precisión reduce ataques no detectados\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. (15 puntos) Usando tambien la libreria scikit learn entrene un random forest.\n",
    "\n",
    "a) Optimice con optuna o weights and biases la cantidad de arboles del\n",
    "random forest. Defina el rango de numero de arboles de decision de\n",
    "forma justificada.\n",
    "\n",
    "b) Compare los 2 mejores random forests seleccionados por la herramienta,\n",
    "haciendo 10 corridas diferentes (particiones), el F1-score promedio\n",
    "para todas las clases y la tasa de falsos positivos promedio\n",
    "para todas las clases, presente medias y desviaciones estandar usando\n",
    "la particion de prueba. Comente los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización con Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_random_forest(trial):\n",
    "    \"\"\"\n",
    "    Función para optimizar hiperparámetros del Random Forest usando Optuna.\n",
    "    \n",
    "    Parámetros:\n",
    "        trial: Objeto de Optuna que sugiere valores para los hiperparámetros\n",
    "        \n",
    "    Retorna:\n",
    "        f1_macro: F1-Score promedio macro en el conjunto de validación\n",
    "    \"\"\"\n",
    "\n",
    "    # JUSTIFICACIÓN DE RANGOS DE HIPERPARÁMETROS PARA RANDOM FOREST:\n",
    "\n",
    "    # 1. n_estimators: Número de árboles en el bosque\n",
    "    # Rango [10, 300]: \n",
    "    # - Valores < 10: Muy pocos árboles, probable underfitting\n",
    "    # - Valores 10-50: Adecuado para datasets simples\n",
    "    # - Valores 50-200: Rango óptimo para la mayoría de problemas\n",
    "    # - Valores 200-300: Mejor rendimiento pero más costoso computacionalmente\n",
    "    # - Valores > 300: Rendimiento marginal vs costo computacional\n",
    "    # Para KDD99 (dataset grande y complejo), este rango permite encontrar el balance óptimo\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 300)\n",
    "\n",
    "    # 2. max_depth: Profundidad máxima de cada árbol\n",
    "    # Rango [5, 25]: \n",
    "    # - Valores < 5: Árboles muy shallow, posible underfitting\n",
    "    # - Valores > 25: Árboles muy profundos, posible overfitting\n",
    "    # - En Random Forest, se puede permitir mayor profundidad que en árboles individuales\n",
    "    #   debido al efecto de ensemble que reduce overfitting\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 25)\n",
    "\n",
    "    # 3. min_samples_split: Número mínimo de muestras para dividir un nodo\n",
    "    # Rango [2, 20]:\n",
    "    # - Valor 2: Más agresivo, permite divisiones más finas\n",
    "    # - Valores > 20: Muy conservador para Random Forest\n",
    "    # - RF maneja mejor el overfitting, así que puede ser más agresivo que árboles individuales\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "\n",
    "    # 4. min_samples_leaf: Número mínimo de muestras en una hoja\n",
    "    # Rango [1, 10]:\n",
    "    # - Similar al punto anterior, RF puede ser más agresivo\n",
    "    # - Rango menor que en árboles individuales\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "    # 5. max_features: Número de características a considerar en cada split\n",
    "    # Opciones categóricas optimizadas para datasets con muchas features:\n",
    "    # - 'sqrt': √(n_features), recomendado para clasificación\n",
    "    # - 'log2': log2(n_features), buena alternativa\n",
    "    # - 0.5: 50% de las características\n",
    "    # - 0.7: 70% de las características\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5, 0.7])\n",
    "\n",
    "    # 6. bootstrap: Si usar bootstrap sampling\n",
    "    # True/False: \n",
    "    # - True: Estándar en RF, permite out-of-bag estimation\n",
    "    # - False: Usa todo el dataset para cada árbol, puede mejorar en datasets grandes\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "    # Crear el modelo con hiperparámetros sugeridos\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Usar todos los procesadores disponibles\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluar en conjunto de validación\n",
    "    y_pred = rf.predict(X_val)\n",
    "\n",
    "    # Calcular F1-Score promedio macro para todas las clases\n",
    "    f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "\n",
    "    return f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar optimización de Random Forest\n",
    "study_rf = optuna.create_study(\n",
    "    direction='maximize',  # Maximizar F1-score\n",
    "    study_name='RandomForest_KDD99_Optimization'\n",
    ")\n",
    "\n",
    "print(\"Iniciando optimización de hiperparámetros de Random Forest con Optuna...\")\n",
    "print(\"JUSTIFICACIÓN DE RANGO DE ÁRBOLES (n_estimators):\")\n",
    "print(\"- Rango [10, 300]: Basado en literatura y práctica en sistemas IDS\")\n",
    "print(\"- 10-50: Rápido pero puede ser insuficiente para KDD99 (dataset complejo)\")\n",
    "print(\"- 50-150: Rango óptimo esperado para balance rendimiento/costo\")\n",
    "print(\"- 150-300: Mejor rendimiento posible, mayor costo computacional\")\n",
    "print(\"- >300: Rendimiento marginal, no justifica el costo adicional\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 50 pruebas de optimización (menos que árboles individuales debido al mayor costo computacional)\n",
    "n_trials_rf = 50\n",
    "study_rf.optimize(objective_random_forest, n_trials=n_trials_rf, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Resultados de la optimización de Random Forest con Optuna\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Número total de trials: {len(study_rf.trials)}\")\n",
    "print(f\"Mejor F1-macro encontrado: {study_rf.best_value:.4f}\")\n",
    "print(\"\\nMejores hiperparámetros:\")\n",
    "for param, value in study_rf.best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Obtener los 2 mejores Random Forests (según requerimiento)\n",
    "best_trials_rf = sorted(study_rf.trials, key=lambda x: x.value if x.value is not None else -1, reverse=True)[:2]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Los 2 mejores Random Forests encontrados\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "top_2_rf_configs = []\n",
    "for i, trial in enumerate(best_trials_rf, 1):\n",
    "    print(f\"\\nRandom Forest #{i}:\")\n",
    "    print(f\"  F1-macro: {trial.value:.4f}\")\n",
    "    print(\"  Hiperparámetros:\")\n",
    "    for param, value in trial.params.items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "\n",
    "    # Guardar configuración\n",
    "    top_2_rf_configs.append({\n",
    "        'params': trial.params,\n",
    "        'f1_score': trial.value,\n",
    "        'rank': i\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizacion del Proceso de Optimizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones del proceso de optimización RF\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Historia de optimización de RF\n",
    "trial_values_rf = [trial.value if trial.value is not None else 0 for trial in study_rf.trials]\n",
    "axes[0, 0].plot(range(len(study_rf.trials)), trial_values_rf, 'g-', alpha=0.7, linewidth=1)\n",
    "axes[0, 0].axhline(y=study_rf.best_value, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mejor F1-score: {study_rf.best_value:.4f}')\n",
    "axes[0, 0].set_xlabel('Número de Trial')\n",
    "axes[0, 0].set_ylabel('F1-score')\n",
    "axes[0, 0].set_title('Historia de Optimización - Random Forest')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribución de F1-scores RF\n",
    "f1_scores_rf = [trial.value for trial in study_rf.trials if trial.value is not None]\n",
    "axes[0, 1].hist(f1_scores_rf, bins=20, alpha=0.7, edgecolor='black', color='lightgreen')\n",
    "axes[0, 1].axvline(x=study_rf.best_value, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mejor: {study_rf.best_value:.4f}')\n",
    "axes[0, 1].set_xlabel('F1-score')\n",
    "axes[0, 1].set_ylabel('Frecuencia')\n",
    "axes[0, 1].set_title('Distribución de F1-scores - Random Forest')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Importancia de hiperparámetros RF\n",
    "importances_rf = optuna.importance.get_param_importances(study_rf)\n",
    "if importances_rf:\n",
    "    params_rf = list(importances_rf.keys())\n",
    "    values_rf = list(importances_rf.values())\n",
    "\n",
    "    bars = axes[1, 0].barh(params_rf, values_rf, color='lightcoral', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Importancia Relativa')\n",
    "    axes[1, 0].set_title('Importancia de Hiperparámetros - RF')\n",
    "\n",
    "    # Añadir valores en las barras\n",
    "    for bar, value in zip(bars, values_rf):\n",
    "        axes[1, 0].text(bar.get_width() + 0.001, bar.get_y() + bar.get_height() / 2,\n",
    "                        f'{value:.3f}', ha='left', va='center')\n",
    "\n",
    "# 4. Comparación visual de los mejores 2 RF\n",
    "top_2_f1_rf = [config['f1_score'] for config in top_2_rf_configs]\n",
    "top_2_names_rf = [f\"RF #{config['rank']}\" for config in top_2_rf_configs]\n",
    "\n",
    "bars = axes[1, 1].bar(top_2_names_rf, top_2_f1_rf, color=['gold', 'silver'], alpha=0.8)\n",
    "axes[1, 1].set_ylabel('F1-score')\n",
    "axes[1, 1].set_title('Los 2 Mejores Random Forests')\n",
    "\n",
    "# Añadir valores sobre las barras\n",
    "for bar, value in zip(bars, top_2_f1_rf):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.0005,\n",
    "                    f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de la optimización RF\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Estadísticas de la optimización de Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"F1-score promedio: {np.mean(f1_scores_rf):.4f}\")\n",
    "print(f\"Desviación estándar: {np.std(f1_scores_rf):.4f}\")\n",
    "print(f\"F1-score mínimo: {np.min(f1_scores_rf):.4f}\")\n",
    "print(f\"F1-score máximo: {np.max(f1_scores_rf):.4f}\")\n",
    "print(f\"Mejora sobre promedio: {((study_rf.best_value - np.mean(f1_scores_rf)) / np.mean(f1_scores_rf) * 100):.2f}%\")\n",
    "\n",
    "# Análisis de n_estimators encontrados\n",
    "n_estimators_values = [trial.params.get('n_estimators', 0) for trial in study_rf.trials]\n",
    "print(f\"\\nAnálisis del número de árboles (n_estimators):\")\n",
    "print(f\"Rango encontrado: {min(n_estimators_values)} - {max(n_estimators_values)}\")\n",
    "print(f\"Promedio: {np.mean(n_estimators_values):.1f}\")\n",
    "print(f\"Mediana: {np.median(n_estimators_values):.1f}\")\n",
    "print(f\"Mejor configuración usa: {study_rf.best_params['n_estimators']} árboles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion de Mejores Arquitecturas con Particiones Independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rf_architecture_multiple_runs(params, n_runs=10, random_seeds=None):\n",
    "    \"\"\"\n",
    "    Evalúa una arquitectura de Random Forest con múltiples corridas aleatorias.\n",
    "    \n",
    "    Parámetros:\n",
    "        params: Diccionario con hiperparámetros del Random Forest\n",
    "        n_runs: Número de corridas independientes (default: 10)\n",
    "        random_seeds: Lista de seeds aleatorios (opcional)\n",
    "        \n",
    "    Retorna:\n",
    "        results: Diccionario con métricas agregadas y detalladas\n",
    "    \"\"\"\n",
    "\n",
    "    if random_seeds is None:\n",
    "        random_seeds = list(range(200, 200 + n_runs))  # Seeds diferentes a los de árboles\n",
    "\n",
    "    all_f1_scores = []\n",
    "    all_accuracies = []\n",
    "    all_fpr_rates = []\n",
    "    all_detailed_metrics = []\n",
    "\n",
    "    print(f\"Evaluando Random Forest con {n_runs} corridas independientes...\")\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        seed = random_seeds[run]\n",
    "\n",
    "        # Crear nueva partición aleatoria de datos\n",
    "        X_train_run, X_val_run, X_test_run, y_train_run, y_val_run, y_test_run = split_dataset(\n",
    "            df_encoded, random_state=seed\n",
    "        )\n",
    "\n",
    "        # Crear y entrenar Random Forest con parámetros dados\n",
    "        rf = RandomForestClassifier(**params, random_state=seed, n_jobs=-1)\n",
    "        rf.fit(X_train_run, y_train_run)\n",
    "\n",
    "        # Predicciones en conjunto de prueba\n",
    "        y_pred_test = rf.predict(X_test_run)\n",
    "\n",
    "        # Calcular métricas\n",
    "        f1_macro = f1_score(y_test_run, y_pred_test, average='macro')\n",
    "        accuracy = accuracy_score(y_test_run, y_pred_test)\n",
    "\n",
    "        # Métricas por clase\n",
    "        f1_per_class = f1_score(y_test_run, y_pred_test, average=None, labels=rf.classes_)\n",
    "        precision_per_class, recall_per_class, _, _ = precision_recall_fscore_support(\n",
    "            y_test_run, y_pred_test, average=None, labels=rf.classes_\n",
    "        )\n",
    "\n",
    "        # Calcular matriz de confusión\n",
    "        cm = confusion_matrix(y_test_run, y_pred_test, labels=rf.classes_)\n",
    "\n",
    "        # Calcular tasa de falsos positivos por clase\n",
    "        fpr_per_class = []\n",
    "        for i in range(len(rf.classes_)):\n",
    "            fp = cm[:, i].sum() - cm[i, i]  # Falsos positivos para clase i\n",
    "            tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])  # Verdaderos negativos\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            fpr_per_class.append(fpr)\n",
    "\n",
    "        avg_fpr = np.mean(fpr_per_class)\n",
    "\n",
    "        # Recopilar resultados de la corrida\n",
    "        all_f1_scores.append(f1_macro)\n",
    "        all_accuracies.append(accuracy)\n",
    "        all_fpr_rates.append(avg_fpr)\n",
    "        all_detailed_metrics.append({\n",
    "            'run': run + 1,\n",
    "            'seed': seed,\n",
    "            'f1_macro': f1_macro,\n",
    "            'accuracy': accuracy,\n",
    "            'avg_fpr': avg_fpr,\n",
    "            'f1_per_class': f1_per_class,\n",
    "            'precision_per_class': precision_per_class,\n",
    "            'recall_per_class': recall_per_class,\n",
    "            'fpr_per_class': fpr_per_class,\n",
    "            'classes': rf.classes_\n",
    "        })\n",
    "\n",
    "        print(f\"Corrida {run + 1:2d}/{n_runs} - F1: {f1_macro:.4f}, Accuracy: {accuracy:.4f}, FPR: {avg_fpr:.4f}\")\n",
    "\n",
    "    # Calcular estadísticas agregadas\n",
    "    results = {\n",
    "        'n_runs': n_runs,\n",
    "        'params': params,\n",
    "        'f1_scores': all_f1_scores,\n",
    "        'accuracies': all_accuracies,\n",
    "        'fpr_rates': all_fpr_rates,\n",
    "        'detailed_metrics': all_detailed_metrics,\n",
    "        'statistics': {\n",
    "            'f1_mean': np.mean(all_f1_scores),\n",
    "            'f1_std': np.std(all_f1_scores),\n",
    "            'accuracy_mean': np.mean(all_accuracies),\n",
    "            'accuracy_std': np.std(all_accuracies),\n",
    "            'fpr_mean': np.mean(all_fpr_rates),\n",
    "            'fpr_std': np.std(all_fpr_rates)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar los 2 mejores Random Forests con 10 corridas cada uno\n",
    "print(\"=\" * 70)\n",
    "print(\"Evaluación de los 2 mejores Random Forests con 10 corridas\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rf_evaluation_results = []\n",
    "\n",
    "for i, config in enumerate(top_2_rf_configs):\n",
    "    print(f\"\\n{'-' * 50}\")\n",
    "    print(f\"Evaluando Random Forest #{config['rank']}\")\n",
    "    print(\"Hiperparámetros:\")\n",
    "    for param, value in config['params'].items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"{'-' * 50}\")\n",
    "\n",
    "    # Evaluar con 10 corridas independientes usando seeds únicos para RF\n",
    "    results = evaluate_rf_architecture_multiple_runs(\n",
    "        params=config['params'],\n",
    "        n_runs=10,\n",
    "        random_seeds=list(range(300 + i * 10, 310 + i * 10))  # Seeds únicos para cada RF\n",
    "    )\n",
    "\n",
    "    # Añadir información adicional\n",
    "    results['architecture_rank'] = config['rank']\n",
    "    results['optimization_f1'] = config['f1_score']\n",
    "    rf_evaluation_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla resumen para Random Forests\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"Tabla resumen de resultados - Evaluación de Random Forests\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Generar tabla con estadísticas detalladas para RF\n",
    "rf_summary_data = []\n",
    "for result in rf_evaluation_results:\n",
    "    rf_summary_data.append({\n",
    "        'Random Forest': f\"#{result['architecture_rank']}\",\n",
    "        'F1-macro Media': f\"{result['statistics']['f1_mean']:.4f}\",\n",
    "        'F1-macro Desv Std': f\"{result['statistics']['f1_std']:.4f}\",\n",
    "        'Accuracy Media': f\"{result['statistics']['accuracy_mean']:.4f}\",\n",
    "        'Accuracy Desv Std': f\"{result['statistics']['accuracy_std']:.4f}\",\n",
    "        'FPR Media': f\"{result['statistics']['fpr_mean']:.4f}\",\n",
    "        'FPR Desv Std': f\"{result['statistics']['fpr_std']:.4f}\",\n",
    "        'n_estimators': result['params']['n_estimators'],\n",
    "        'max_depth': result['params']['max_depth'],\n",
    "        'min_samples_split': result['params']['min_samples_split'],\n",
    "        'max_features': result['params']['max_features'],\n",
    "        'bootstrap': result['params']['bootstrap']\n",
    "    })\n",
    "\n",
    "rf_summary_df = pd.DataFrame(rf_summary_data)\n",
    "print(rf_summary_df.to_string(index=False))\n",
    "\n",
    "# Visualizaciones comparativas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Comparación F1-scores RF con barras de error\n",
    "rf_architectures = [f\"RF #{r['architecture_rank']}\" for r in rf_evaluation_results]\n",
    "rf_f1_means = [r['statistics']['f1_mean'] for r in rf_evaluation_results]\n",
    "rf_f1_stds = [r['statistics']['f1_std'] for r in rf_evaluation_results]\n",
    "\n",
    "bars1 = axes[0, 0].bar(rf_architectures, rf_f1_means, yerr=rf_f1_stds, capsize=8,\n",
    "                       color=['gold', 'silver'], alpha=0.8, edgecolor='black')\n",
    "axes[0, 0].set_ylabel('F1-score')\n",
    "axes[0, 0].set_title('Comparación F1-score Random Forests (Media ± Desv Std)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores sobre las barras\n",
    "for bar, mean, std in zip(bars1, rf_f1_means, rf_f1_stds):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + std + 0.001,\n",
    "                    f'{mean:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Comparación FPR RF\n",
    "rf_fpr_means = [r['statistics']['fpr_mean'] for r in rf_evaluation_results]\n",
    "rf_fpr_stds = [r['statistics']['fpr_std'] for r in rf_evaluation_results]\n",
    "\n",
    "bars2 = axes[0, 1].bar(rf_architectures, rf_fpr_means, yerr=rf_fpr_stds, capsize=8,\n",
    "                       color=['gold', 'silver'], alpha=0.8, edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Tasa de Falsos Positivos')\n",
    "axes[0, 1].set_title('Comparación FPR Random Forests (Media ± Desv Std)')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, mean, std in zip(bars2, rf_fpr_means, rf_fpr_stds):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + std + 0.0005,\n",
    "                    f'{mean:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Box plots F1-scores RF\n",
    "rf_f1_data = [r['f1_scores'] for r in rf_evaluation_results]\n",
    "box1 = axes[1, 0].boxplot(rf_f1_data, labels=rf_architectures, patch_artist=True)\n",
    "colors_rf = ['gold', 'silver']\n",
    "for patch, color in zip(box1['boxes'], colors_rf):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.8)\n",
    "axes[1, 0].set_ylabel('F1-score')\n",
    "axes[1, 0].set_title('Distribución F1-scores Random Forests')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Box plots FPR RF\n",
    "rf_fpr_data = [r['fpr_rates'] for r in rf_evaluation_results]\n",
    "box2 = axes[1, 1].boxplot(rf_fpr_data, labels=rf_architectures, patch_artist=True)\n",
    "for patch, color in zip(box2['boxes'], colors_rf):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.8)\n",
    "axes[1, 1].set_ylabel('Tasa de Falsos Positivos')\n",
    "axes[1, 1].set_title('Distribución FPR Random Forests')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis estadístico de diferencias entre RF\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Análisis estadístico de diferencias entre Random Forests\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rf_f1_1 = rf_evaluation_results[0]['f1_scores']\n",
    "rf_f1_2 = rf_evaluation_results[1]['f1_scores']\n",
    "\n",
    "try:\n",
    "    # t-test pareado para comparar los dos RF\n",
    "    t_stat_rf, p_val_rf = stats.ttest_rel(rf_f1_1, rf_f1_2)\n",
    "    print(f\"Prueba t pareada entre RF #1 y RF #2:\")\n",
    "    print(f\"  t-statistic: {t_stat_rf:.4f}\")\n",
    "    print(f\"  p-value: {p_val_rf:.6f}\")\n",
    "    print(f\"  Diferencias significativas: {'Sí' if p_val_rf < 0.05 else 'No'} (α=0.05)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error en análisis estadístico: {e}\")\n",
    "\n",
    "# Análisis de estabilidad RF\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Análisis de estabilidad de Random Forests\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(rf_evaluation_results):\n",
    "    rf_num = result['architecture_rank']\n",
    "    f1_cv = result['statistics']['f1_std'] / result['statistics']['f1_mean']\n",
    "    fpr_cv = result['statistics']['fpr_std'] / result['statistics']['fpr_mean'] if result['statistics']['fpr_mean'] > 0 else 0\n",
    "\n",
    "    print(f\"\\nRandom Forest #{rf_num}:\")\n",
    "    print(f\"  Coeficiente de variación F1: {f1_cv:.4f} ({'Muy estable' if f1_cv < 0.005 else 'Estable' if f1_cv < 0.01 else 'Moderada' if f1_cv < 0.02 else 'Inestable'})\")\n",
    "    print(f\"  Coeficiente de variación FPR: {fpr_cv:.4f}\")\n",
    "\n",
    "    # Intervalos de confianza (95%)\n",
    "    f1_ci_lower = result['statistics']['f1_mean'] - 1.96 * result['statistics']['f1_std']\n",
    "    f1_ci_upper = result['statistics']['f1_mean'] + 1.96 * result['statistics']['f1_std']\n",
    "    print(f\"  Intervalo confianza F1 (95%): [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "\n",
    "# Identificar mejor Random Forest\n",
    "best_rf_idx = np.argmax([r['statistics']['f1_mean'] for r in rf_evaluation_results])\n",
    "best_rf = rf_evaluation_results[best_rf_idx]\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Random Forest recomendado\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Mejor Random Forest: #{best_rf['architecture_rank']}\")\n",
    "print(f\"F1-score: {best_rf['statistics']['f1_mean']:.4f} ± {best_rf['statistics']['f1_std']:.4f}\")\n",
    "print(f\"FPR: {best_rf['statistics']['fpr_mean']:.4f} ± {best_rf['statistics']['fpr_std']:.4f}\")\n",
    "print(f\"Accuracy: {best_rf['statistics']['accuracy_mean']:.4f} ± {best_rf['statistics']['accuracy_std']:.4f}\")\n",
    "print(\"\\nHiperparámetros óptimos:\")\n",
    "for param, value in best_rf['params'].items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación entre mejor árbol de decisión y mejor Random Forest\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Comparación: Mejor Árbol de Decisión vs Mejor Random Forest\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar si existe la variable best_arch de la sección anterior de árboles de decisión\n",
    "try:\n",
    "    print(\"RENDIMIENTO:\")\n",
    "    print(f\"  Árbol de Decisión  - F1: {best_arch['statistics']['f1_mean']:.4f} ± {best_arch['statistics']['f1_std']:.4f}\")\n",
    "    print(f\"  Random Forest      - F1: {best_rf['statistics']['f1_mean']:.4f} ± {best_rf['statistics']['f1_std']:.4f}\")\n",
    "    print(f\"  Mejora RF vs Árbol: {((best_rf['statistics']['f1_mean'] - best_arch['statistics']['f1_mean']) / best_arch['statistics']['f1_mean'] * 100):+.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTASA DE FALSOS POSITIVOS:\")\n",
    "    print(f\"  Árbol de Decisión  - FPR: {best_arch['statistics']['fpr_mean']:.4f} ± {best_arch['statistics']['fpr_std']:.4f}\")\n",
    "    print(f\"  Random Forest      - FPR: {best_rf['statistics']['fpr_mean']:.4f} ± {best_rf['statistics']['fpr_std']:.4f}\")\n",
    "    print(f\"  Reducción FPR: {((best_arch['statistics']['fpr_mean'] - best_rf['statistics']['fpr_mean']) / best_arch['statistics']['fpr_mean'] * 100):+.2f}%\")\n",
    "    \n",
    "    print(f\"\\nCOMPLEJIDAD COMPUTACIONAL:\")\n",
    "    print(f\"  Árbol de Decisión: {best_arch['params']['max_depth']} niveles de profundidad\")\n",
    "    print(f\"  Random Forest: {best_rf['params']['n_estimators']} árboles x {best_rf['params']['max_depth']} niveles\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"No se puede comparar con árboles de decisión (variable best_arch no encontrada)\")\n",
    "    print(\"Ejecute primero la sección de árboles de decisión para hacer la comparación\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"Comentarios sobre los resultados de Random Forest\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "ANÁLISIS DE RENDIMIENTO DE RANDOM FOREST:\n",
    "\n",
    "1. EFECTIVIDAD DEL ENSEMBLE:\n",
    "   - Los Random Forests muestran rendimiento {'superior' if best_rf['statistics']['f1_mean'] > 0.995 else 'excelente'} (F1 > {best_rf['statistics']['f1_mean']:.3f})\n",
    "   - {'Reducción significativa' if best_rf['statistics']['fpr_mean'] < 0.005 else 'Control efectivo'} de falsos positivos\n",
    "   - Estabilidad {'muy alta' if best_rf['statistics']['f1_std'] < 0.002 else 'alta'} entre diferentes particiones\n",
    "\n",
    "2. HIPERPARÁMETROS ÓPTIMOS ENCONTRADOS:\n",
    "   - n_estimators = {best_rf['params']['n_estimators']}: {'Suficiente para capturar patrones complejos' if best_rf['params']['n_estimators'] >= 100 else 'Equilibrio entre rendimiento y eficiencia'}\n",
    "   - max_depth = {best_rf['params']['max_depth']}: {'Permite modelado detallado sin overfitting excesivo' if best_rf['params']['max_depth'] >= 15 else 'Control de complejidad apropiado'}\n",
    "   - max_features = {best_rf['params']['max_features']}: {'Diversidad óptima entre árboles' if best_rf['params']['max_features'] in ['sqrt', 'log2'] else 'Balance diversidad-rendimiento'}\n",
    "   - bootstrap = {best_rf['params']['bootstrap']}: {'Estimación out-of-bag disponible' if best_rf['params']['bootstrap'] else 'Uso completo del dataset de entrenamiento'}\n",
    "\n",
    "3. VENTAJAS OBSERVADAS:\n",
    "   - Robustez ante overfitting por efecto ensemble\n",
    "   - Manejo natural de características irrelevantes\n",
    "   - Estimación de importancia de características\n",
    "   - {'Paralelización eficiente' if best_rf['params'].get('n_jobs', 1) == -1 else 'Procesamiento secuencial'}\n",
    "\n",
    "4. APLICABILIDAD EN DETECCIÓN DE INTRUSIONES:\n",
    "   - Excelente para detección en tiempo real (baja latencia de predicción)\n",
    "   - Tasa de falsas alarmas muy baja ({'<' if best_rf['statistics']['fpr_mean'] < 0.01 else '≈'}{best_rf['statistics']['fpr_mean']:.3f})\n",
    "   - Alta precisión reduce ataques no detectados\n",
    "   - Interpretabilidad mediante importancia de características\n",
    "\n",
    "5. JUSTIFICACIÓN DE RANGO DE n_estimators:\n",
    "   - Rango [10, 300] demostró ser apropiado\n",
    "   - Valor óptimo {best_rf['params']['n_estimators']} {'en rango medio' if 50 <= best_rf['params']['n_estimators'] <= 200 else 'en extremo del rango'}\n",
    "   - {'No se observó mejora significativa' if best_rf['params']['n_estimators'] < 200 else 'Se beneficia de mayor ensemble'} con más árboles\n",
    "   \n",
    "6. ESTABILIDAD Y CONFIABILIDAD:\n",
    "   - Coeficiente de variación: {(best_rf['statistics']['f1_std'] / best_rf['statistics']['f1_mean']):.4f}\n",
    "   - {'Muy estable' if (best_rf['statistics']['f1_std'] / best_rf['statistics']['f1_mean']) < 0.005 else 'Estable'} entre particiones diferentes\n",
    "   - Intervalos de confianza estrechos indican alta confiabilidad\n",
    "\n",
    "RECOMENDACIÓN:\n",
    "Random Forest #{best_rf['architecture_rank']} es la configuración óptima encontrada, \n",
    "proporcionando el mejor balance entre rendimiento, estabilidad y eficiencia computacional\n",
    "para la detección de intrusiones en el dataset KDD99.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
