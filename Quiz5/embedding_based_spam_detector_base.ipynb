{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quiz 5\n",
        "\n",
        "Estudiantes:\n",
        "\n",
        "- Alonso Araya\n",
        "- Pedro Soto\n",
        "- Sofia Oviedo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReWfV74511t2"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4fc5M6fk1Fr2",
        "outputId": "f2e45044-76d1-49f0-d7f0-5bf79301e548"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#data frame\n",
        "sms_df = pd.read_table('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "display(sms_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTBT3VS2f40"
      },
      "source": [
        "# Install BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1RNWfVS2hcD",
        "outputId": "4bcdfd49-14a1-4f67-85c2-2adbac2d209a"
      },
      "outputs": [],
      "source": [
        "%pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Login to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN1iGmH82uyi"
      },
      "source": [
        "# Load BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "7b509bfa1b7b490e9b309b1c5fa16d24",
            "f3cbfeec510b45d38833a76e3dc55e59",
            "f910b17d2b064d49b355e374c0e083d2",
            "d52b5e9d4936490393f3891f287f1f44",
            "e3ae2431135e40aaab919e74e7a6114b",
            "ff126133fe504edeb7ec56322d1bcf1d",
            "d6c69ae7aaed4d0da141d5c4a0e916fa",
            "b7f5fe36693444c4876b4c328d9b8d0e",
            "e42945c82469453580312e6c8e7c2069",
            "ccb4e2fd540b4166ab34a95c9a937282",
            "ca5c6f84d25743fab39557dff236644a",
            "91e5f18228724e2b881249d8b89d5f71",
            "32cc2727c4e24596b7dd6c3a02aa5664",
            "3d7cf2567cc44dcf85240a6d59ca50e3",
            "45349080c9a9406b83f687c4c950616b",
            "f56b9e7fc1ab4656a8b9b49b5b4e6cf7",
            "bf2c248c780a42fb9186f69005c4dd4a",
            "daac5f340a46431eb56d6eb7db39a5e6",
            "8e677c250b0d4375b203c5863021bcf8",
            "e67a614ca7af4b80a38b5721131798c9",
            "da6e2d4730f4489a9fefba569017aa8d",
            "32cec703c6594823b66986150f332797",
            "bab02883fc2c4a609dee477cf44d25bd",
            "7a2d72af86964dd69c084040eba0d72b",
            "7ff203f173004583b8acb2e7586689fe",
            "331260d2d9a0425888f11360b4dd837c",
            "aa4d54529d664bcbbb414ec6c216389d",
            "524edb35fe3f42a78c0b5629134ce9c7",
            "80f0fc10f20b40bcb4c3087afb4f7a8f",
            "a051b67f5cbe4672be421abe0558c287",
            "f86d1ca1dec1433fa6cb5223e2a5fd10",
            "f6135e3dfe744412a00032d947c786bb",
            "e232f1bc5a07418084e7f7fca277ea54",
            "24bf8ee8e2aa433b8eeb73565036fed3",
            "d8824c6c9c3f4dafab94905f2dde0117",
            "ff5254e7c6b14468aafeeb333acdfd09",
            "4f45c6eae349457db086d6445a5e43a0",
            "325babe84ee84cd58fdb9d3e91b1c918",
            "103d097056d744b7ba3b4c41b8016038",
            "a1b39397b75f4503a577b385453ee575",
            "ae20dda8aff84a1f975214847ddec1ce",
            "00942d554e7547a58c947f401076ffc8",
            "8d26a04225c14e51b306500f88392c1a",
            "5999b8d1dd364e2dad94012b2bdffe84",
            "998f83401b4b4b388f9dc1c2796b07e2",
            "34afe2fbc7bc4067be98712600a1415c",
            "f1ca477f371542fb81a3d07142e9e855",
            "5de57bac529e4adcb668adf87ff6140a",
            "b003be8eb821462dafc32997c123cdff",
            "6ab03c76bcd342a7aeffcb89c108fff5",
            "fab74cf88464444ca1fd48f91f929f93",
            "561cebef895d4f02b8a16a36727c0234",
            "85610ec32a8e46d68e5661274eb7dc5d",
            "d2c3367c168b47539489b5d4f8855b85",
            "559d8b5889fc4ca0ae7ad4809013298c"
          ]
        },
        "id": "j3AtNCOK2w67",
        "outputId": "46138f83-9bfd-4433-a14f-59e75143ea8e"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "picVFTUY39H0",
        "outputId": "89a0d08b-f041-4185-a3d5-87c74a5da923"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device used: \", device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrV7G2g44Y3d"
      },
      "source": [
        "# Compute embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0tKGLxui5Emh",
        "outputId": "fe2c29c4-e0b6-4d54-eea2-0aef3daf4c73"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def compute_bert_embeddings(text_snippets, model):\n",
        "    encoded_input = tokenizer(text_snippets, padding=True, truncation=True, return_tensors='pt')\n",
        "    # Move encoded_input to the same device as the model\n",
        "    encoded_input = {key: value.to(model.device) for key, value in encoded_input.items()}\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    # Use the embeddings of the [CLS] token\n",
        "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
        "    #print(\"embeddings shape \\n\", embeddings.shape)\n",
        "    return embeddings\n",
        "\n",
        "sms_df['embeddings'] = sms_df['message'].apply(lambda x: compute_bert_embeddings([x], model).squeeze())\n",
        "\n",
        "display(sms_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57WWuUbe6aAp",
        "outputId": "5c32efdb-7aae-458a-8ad6-ab635ec95af0"
      },
      "outputs": [],
      "source": [
        "#store the embedding in a tensor\n",
        "embeddings_tensor = torch.stack(sms_df['embeddings'].tolist())\n",
        "print(\"Embeddings tensor shape: \", embeddings_tensor.shape)\n",
        "#convert the labels to numerical values in a tensor\n",
        "label_mapping = {'ham': 0, 'spam': 1}\n",
        "numerical_labels = sms_df['label'].map(label_mapping)\n",
        "labels_tensor = torch.tensor(numerical_labels.tolist())\n",
        "print(\"labels 0 \", (labels_tensor == 0).sum())\n",
        "print(\"labels 1 \", (labels_tensor == 1).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSc9Yl1w7mef"
      },
      "source": [
        "# Split the dataset (70% Train, 15% Validation, 15% Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "KwNryZvn7pIR",
        "outputId": "0e9df011-8450-4135-cd86-0168fb1b3716"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Primero dividir en 70% entrenamiento y 30% restante\n",
        "X_train_full, X_temp, y_train_full, y_temp = train_test_split(\n",
        "    embeddings_tensor, labels_tensor, test_size=0.30, random_state=42, stratify=labels_tensor\n",
        ")\n",
        "\n",
        "# Luego dividir el 30% restante en 15% validación y 15% prueba\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Entrenamiento: {X_train_full.shape[0]} muestras ({X_train_full.shape[0]/len(embeddings_tensor)*100:.1f}%)\")\n",
        "print(f\"Validación: {X_val.shape[0]} muestras ({X_val.shape[0]/len(embeddings_tensor)*100:.1f}%)\")\n",
        "print(f\"Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(embeddings_tensor)*100:.1f}%)\")\n",
        "\n",
        "# Verificar distribución de clases\n",
        "print(\"\\nDistribución de clases:\")\n",
        "print(f\"Entrenamiento - Ham: {(y_train_full == 0).sum()}, Spam: {(y_train_full == 1).sum()}\")\n",
        "print(f\"Validación - Ham: {(y_val == 0).sum()}, Spam: {(y_val == 1).sum()}\")\n",
        "print(f\"Prueba - Ham: {(y_test == 0).sum()}, Spam: {(y_test == 1).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. (30 puntos) Para el dataset SMS_dataset disponible en implemente los siguientes modelos de clasificacion\n",
        "\n",
        "```\n",
        "a) Entrene una red neuronal con 2 variantes de arquitectura a definir por usted. Justifique las 2 variantes\n",
        "y entrene ambos modelos, muestre sus curvas de aprendizaje para una particion de datos de\n",
        "entrenamiento (70%) y validacion (15%) luego de la calibracion de los principales hiperparametros.\n",
        "Evalue el error con una particion aleatoria de prueba (15%) Comente los resultados.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelos propuestos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpamClassifierV1(nn.Module):\n",
        "    def __init__(self, input_dim=768):\n",
        "        super(SpamClassifierV1, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class SpamClassifierV2(nn.Module):\n",
        "    def __init__(self, input_dim=768):\n",
        "        super(SpamClassifierV2, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            \n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Primer Modelo\n",
        "\n",
        "Para el primer modelo propuesto \"SpamClassifierV1\", este representa el modelo mas profundo. Este busca reducir la dimensionalidad escalonada desde la entrada de 768 pasando por 512, 256, 128 y 1, esto puede ayudar a que el modelo aprenda representaciones mas complejas y reducir la dimensionalidad gradualmente puede evitar perde informacion abruptamente.\n",
        "\n",
        "En el caso del dropout se utiliza valores de 0,5 y 0,3, esto permite regularizar correctamente el modelo y prevenir el overfitting, se reduce al final con 0,3 para poder preservar la mayor cantidad de informacion antes de realizar la clasificacion.\n",
        "\n",
        "Esta utiliza cuatro capas y es capaz de poder capturar mejor los patrones complejos del set de datos y valores no lineales de los embeddings producidos por BERT. Sin embargo puede utilizar un poco mas de recursos al ser mas profundo.\n",
        "\n",
        "### Segundo Modelo\n",
        "\n",
        "En el segundo que se llama \"SpamClassifierV2\" tiene un perfil mas pequeño pero mas agresivo, ya que va desde la dimension de entrada de 768 hasta 256 y 64. Tiene tambien menos capas comparado con el modelo anterior y este va siendo un modelo mas pequeño con el fin de que sea mas eficiente a nivel computacional y comparar si existe un verdadero beneficio al usar algo mas escalonado y profundo como lo es el modelo V1, en contra de este modelo que es un poco mas pequeño y agresivo.\n",
        "\n",
        "Ademas de eso tiene parametros como el dropout en 0,3 siendo un poco mas balanceado para perder menos informacion y consistente, esto puede ayudar a prevenir overfitting y preservar la mayor cantidad de informacion durante todo el modelo. Al ser mas pequeño tambien podria ayudar a que no se sobreajuste y puda generalizar con buenos resultados a un menor costo y complejidad, sin embargo al no ser tan profundo tiene el riesgo de caer en underfitting.\n",
        "\n",
        "En general ambos utilizan la funcion de activacion ReLu conocida por ser mas eficiente a nivel computacional por su naturaleza linear y que puede ayudara mitigar el problema de gradientes desvanecientes, ayudando a que se puedan crear redes mas profundas, tambien tiene la ventaja dada sus propiedades no lineares ayuda a capturar estos mismos patrones en los datos.\n",
        "\n",
        "Tambien se utiliza la función sigmoidal dado a que es una clasificacion binaria y nos puede dar resultados de 0 y 1, ademas en las proximas celdas de entrenamiento se utiliza BCELoss, que necesita de este formato para operar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logica de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=50, lr=0.001, weight_decay=0.01):\n",
        "    \"\"\"\n",
        "    Entrena un modelo de clasificación binaria para detección de spam con validación cruzada.\n",
        "    \n",
        "    Parameteros\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        Red neuronal a entrenar (SpamClassifierV1, SpamClassifierV2, etc.).\n",
        "        Debe tener salida sigmoid para clasificación binaria.\n",
        "        \n",
        "    train_loader : torch.utils.data.DataLoader\n",
        "        DataLoader con datos de entrenamiento. Debe retornar (features, labels)\n",
        "        donde features son embeddings BERT (shape: [batch_size, 768]) y \n",
        "        labels son enteros {0: ham, 1: spam}.\n",
        "        \n",
        "    val_loader : torch.utils.data.DataLoader\n",
        "        DataLoader con datos de validación. Mismo formato que train_loader.\n",
        "        Usado para evaluación independiente y selección del mejor modelo.\n",
        "        \n",
        "    epochs : int, optional, default=50\n",
        "        Número máximo de épocas de entrenamiento. El entrenamiento puede\n",
        "        detenerse antes si se detectan problemas de convergencia.\n",
        "        \n",
        "    lr : float, optional, default=0.001\n",
        "        Learning rate para el optimizador Adam. Controla el tamaño de paso\n",
        "        en la actualización de parámetros. Valores típicos: [0.0001, 0.01].\n",
        "        \n",
        "    weight_decay : float, optional, default=0.01\n",
        "        Coeficiente de regularización L2 aplicado por el optimizador Adam.\n",
        "        Previene sobreajuste penalizando parámetros grandes. Valores típicos: [0.0, 0.1].\n",
        "    \n",
        "    Retorna\n",
        "    -------\n",
        "    dict\n",
        "        Diccionario con el historial completo de entrenamiento:\n",
        "        \n",
        "        - 'train_losses' : List[float]\n",
        "            Pérdida BCE promedio por época en conjunto de entrenamiento.\n",
        "        - 'val_losses' : List[float] \n",
        "            Pérdida BCE promedio por época en conjunto de validación.\n",
        "        - 'train_accuracies' : List[float]\n",
        "            Accuracy (exactitud) por época en conjunto de entrenamiento.\n",
        "        - 'val_accuracies' : List[float]\n",
        "            Accuracy por época en conjunto de validación.\n",
        "        - 'train_f1_scores' : List[float]\n",
        "            F1-score por época en conjunto de entrenamiento.\n",
        "        - 'val_f1_scores' : List[float]\n",
        "            F1-score por época en conjunto de validación.\n",
        "        - 'best_val_f1' : float\n",
        "            Mejor F1-score alcanzado en validación durante todo el entrenamiento.\n",
        "\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    \n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    train_f1_scores, val_f1_scores = [], []\n",
        "    \n",
        "    best_val_f1 = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Entrenamiento\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_predictions = []\n",
        "        train_targets = []\n",
        "        \n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device).float().unsqueeze(1)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            predictions = (outputs > 0.5).float()\n",
        "            train_predictions.extend(predictions.cpu().numpy().flatten())\n",
        "            train_targets.extend(batch_y.cpu().numpy().flatten())\n",
        "        \n",
        "        # Validación\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device).float().unsqueeze(1)\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                predictions = (outputs > 0.5).float()\n",
        "                val_predictions.extend(predictions.cpu().numpy().flatten())\n",
        "                val_targets.extend(batch_y.cpu().numpy().flatten())\n",
        "        \n",
        "        # Calcular métricas\n",
        "        train_acc = np.mean(np.array(train_predictions) == np.array(train_targets))\n",
        "        val_acc = np.mean(np.array(val_predictions) == np.array(val_targets))\n",
        "        \n",
        "        train_f1 = f1_score(train_targets, train_predictions)\n",
        "        val_f1 = f1_score(val_targets, val_predictions)\n",
        "        \n",
        "        # Guardar métricas\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "        train_f1_scores.append(train_f1)\n",
        "        val_f1_scores.append(val_f1)\n",
        "        \n",
        "        # Guardar mejor modelo\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Época {epoch+1}/{epochs}: '\n",
        "                  f'Train Loss: {train_losses[-1]:.4f}, '\n",
        "                  f'Val Loss: {val_losses[-1]:.4f}, '\n",
        "                  f'Val F1: {val_f1:.4f}')\n",
        "    \n",
        "    # Restaurar mejor modelo\n",
        "    model.load_state_dict(best_model_state)\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_accuracies': val_accuracies,\n",
        "        'train_f1_scores': train_f1_scores,\n",
        "        'val_f1_scores': val_f1_scores,\n",
        "        'best_val_f1': best_val_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La funcion train_model permite entrenar un modelo, esta recibe como parametros el modelo, datos de entrenamiento y validacion, asi como el learning rate y regularizacion l2.\n",
        "\n",
        "Esta funcion transfiere el modelo al device utilizado para asegurar que esta utilizando GPU. Se utiliza la funcion BCELoss que es una de las mas adecuadas para clasificacion binaria ya que mide la divergencia entre probabilidades que salieron del modelo y se ajusta perfectamente al modelo utilizar la funcion sigmoidal y utilizar etiquetas de 0 y 1.\n",
        "\n",
        "Se utiliza Adam como optimizardor ya que puede ajustar adaptativamente parametros como el learning rate para los parametros, combina caracteristicas de RMSProp y SGS con momentum, asi como una convergencia mas estable y rapida en los modelos profundos, por lo que es una buena funcion segura y inicial por utilizar.\n",
        "\n",
        "Tambien la funcion almacena todos las metricas como las perdidas de entrenamiento y validacion, asi como su accuracy y F1 Score para las particiones, finalmente evaluando cual fue el mejor modelo que obtuvo el mejor F1.\n",
        "\n",
        "La funcion entrena el modelo en varios pasos, primero se limpian los gradientes con `zero_grad`, despues se hace un forward pass del modelo, se calculan las perdidas, se hace el calculo de gradiente y finalmente se optimizan los parametros. En general tambien se toma en cuenta pasar los valores al dispositivo ajustado para ser usado en el GPU o en caso de usar librerias como numpy se pasa a CPU. Durante el entrenamiento se convierten las probabilidades en predicciones binarias y se guardan para generar las metricas.\n",
        "\n",
        "En cuanto a la fase de validacion se activa el modo por medio de `model.eval`, esta funcion desactiva el dropout y se ajusta para ser mas deterministico para la validacion. Por medio de `torch.grad` se desactiva el calculo de gradientes y solo se realiza forward pass, estos pasos ayudan a reducir el uso de recursos ya que no se necesitan nuevamente algunos pasos. Estas predicciones de las validaciones se van a guardar para el calculo de las metricas.\n",
        "\n",
        "Se utilizo ademas un sistema de checkpointing el cual utiliza el F1 Score de la validacion y se guarda una copia del mejor modelo por el momento.\n",
        "\n",
        "En el paso final se generan las metricas por medio de las predicciones y targets guardados en los pasos de entrenamiento y validacion, se generaron metricas de promedio de las perdidas, accuracy y F1 Score para el entrenamiento y validacion, todas estas son retornadas al final de la funcion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Procesamiento en lotes y Gestion de Datos\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = TensorDataset(X_train_full, y_train_full)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizacion de Parametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir hiperparámetros a probar\n",
        "learning_rates = [0.001, 0.0005]\n",
        "weight_decays = [0.01, 0.001]\n",
        "epochs = 50\n",
        "\n",
        "best_params_v1 = {'lr': None, 'wd': None, 'score': 0}\n",
        "best_params_v2 = {'lr': None, 'wd': None, 'score': 0}\n",
        "\n",
        "print(\"\\nCalibrando Modelo V1 (Red Profunda)...\")\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        print(f\"  Probando lr={lr}, weight_decay={wd}\")\n",
        "        model_v1 = SpamClassifierV1()\n",
        "        history = train_model(model_v1, train_loader, val_loader, \n",
        "                             epochs=30, lr=lr, weight_decay=wd)  # Menos épocas para calibración\n",
        "        \n",
        "        if history['best_val_f1'] > best_params_v1['score']:\n",
        "            best_params_v1 = {'lr': lr, 'wd': wd, 'score': history['best_val_f1']}\n",
        "\n",
        "print(f\"  Mejor V1: lr={best_params_v1['lr']}, wd={best_params_v1['wd']}, F1={best_params_v1['score']:.4f}\")\n",
        "\n",
        "print(\"\\nCalibrando Modelo V2 (Red Simple)...\")\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        print(f\"  Probando lr={lr}, weight_decay={wd}\")\n",
        "        model_v2 = SpamClassifierV2()\n",
        "        history = train_model(model_v2, train_loader, val_loader, \n",
        "                             epochs=30, lr=lr, weight_decay=wd)\n",
        "        \n",
        "        if history['best_val_f1'] > best_params_v2['score']:\n",
        "            best_params_v2 = {'lr': lr, 'wd': wd, 'score': history['best_val_f1']}\n",
        "\n",
        "print(f\"  Mejor V2: lr={best_params_v2['lr']}, wd={best_params_v2['wd']}, F1={best_params_v2['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la logica de arriba se realiza el entrenamiento en si de los modelos, para este efecto tambien se adecuo el codigo para poder realizar una optimizacion de parametros pequeña, se genero un rango de learning rates y weight decays a utilizar. \n",
        "\n",
        "Se escogio ese rango de learning rate debido a que o.001 es estandar a utolizar con Adam y proporciona un buen balance, el valor de 0.0005 es un poco mas conservador y evita mucho mas el sobreajuste, estos dos valores proporcionan un punto medio para optimizar el modelo. \n",
        "\n",
        "En cuanto al weight decay, el valor de 0.01 presenta una regularizacion un poco mas alta lo que podria evitar de gran manera el overfitting, en cuanto a 0.001 da mas flecibilidad al modelo y entre estos valores se puede variar el ajuste y la capacidad de generalizacion del modelo.\n",
        "\n",
        "Esta funcion finalmente tiene como salida los mejores parametros por cada arquitectura, asi como sus F1 Score, tambien se puede observar las perdidas y F1 por cada grupo de epochs que paso, observando la evolucion del entrenamiento. Se guardan los mejores parametros para relizar el entrenamiento final con estas combinaciones por arquitectura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar Modelo V1 con mejores parámetros\n",
        "print(\"Entrenando Modelo V1 con mejores hiperparámetros...\")\n",
        "model_v1_final = SpamClassifierV1()\n",
        "history_v1 = train_model(model_v1_final, train_loader, val_loader, \n",
        "                        epochs=epochs, \n",
        "                        lr=best_params_v1['lr'], \n",
        "                        weight_decay=best_params_v1['wd'])\n",
        "\n",
        "# Entrenar Modelo V2 con mejores parámetros\n",
        "print(\"\\nEntrenando Modelo V2 con mejores hiperparámetros...\")\n",
        "model_v2_final = SpamClassifierV2()\n",
        "history_v2 = train_model(model_v2_final, train_loader, val_loader, \n",
        "                        epochs=epochs, \n",
        "                        lr=best_params_v2['lr'], \n",
        "                        weight_decay=best_params_v2['wd'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Curvas de pérdida\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(history_v1['train_losses'], label='V1 Train', color='blue', alpha=0.7)\n",
        "plt.plot(history_v1['val_losses'], label='V1 Val', color='blue', linestyle='--')\n",
        "plt.plot(history_v2['train_losses'], label='V2 Train', color='red', alpha=0.7)\n",
        "plt.plot(history_v2['val_losses'], label='V2 Val', color='red', linestyle='--')\n",
        "plt.title('Curvas de Pérdida')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('BCE Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Curvas de precisión\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(history_v1['train_accuracies'], label='V1 Train', color='blue', alpha=0.7)\n",
        "plt.plot(history_v1['val_accuracies'], label='V1 Val', color='blue', linestyle='--')\n",
        "plt.plot(history_v2['train_accuracies'], label='V2 Train', color='red', alpha=0.7)\n",
        "plt.plot(history_v2['val_accuracies'], label='V2 Val', color='red', linestyle='--')\n",
        "plt.title('Curvas de Precisión')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Curvas de F1-Score\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(history_v1['train_f1_scores'], label='V1 Train', color='blue', alpha=0.7)\n",
        "plt.plot(history_v1['val_f1_scores'], label='V1 Val', color='blue', linestyle='--')\n",
        "plt.plot(history_v2['train_f1_scores'], label='V2 Train', color='red', alpha=0.7)\n",
        "plt.plot(history_v2['val_f1_scores'], label='V2 Val', color='red', linestyle='--')\n",
        "plt.title('Curvas de F1-Score')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Comparación final de métricas\n",
        "plt.subplot(2, 3, 4)\n",
        "metrics = ['Train Acc', 'Val Acc', 'Train F1', 'Val F1']\n",
        "v1_final_metrics = [history_v1['train_accuracies'][-1], history_v1['val_accuracies'][-1],\n",
        "                   history_v1['train_f1_scores'][-1], history_v1['val_f1_scores'][-1]]\n",
        "v2_final_metrics = [history_v2['train_accuracies'][-1], history_v2['val_accuracies'][-1],\n",
        "                   history_v2['train_f1_scores'][-1], history_v2['val_f1_scores'][-1]]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "plt.bar(x - width/2, v1_final_metrics, width, label='Modelo V1', alpha=0.8)\n",
        "plt.bar(x + width/2, v2_final_metrics, width, label='Modelo V2', alpha=0.8)\n",
        "plt.title('Comparación Final de Métricas')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(x, metrics, rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En base al entrenamiento hecho con los parametros optimizados se generan las curvas de perdida, precision, F1 Score y comparacion entre ambos modelos. Esto nos permite ver de una mejor manera la evolucion de cada modelo durante los epochs.\n",
        "\n",
        "En cuanto a la curva de perdida se puede observar como con pocos epochs rapidamente se reduce la perdida, ademas se observa una cierta estabilidad despues del epoch 10, las cuales se mantienen entre 0,2 y 0,4. Al ver las curvas de validacion estas tienden a estar cerca de las de entrenamiento, lo que dice que podrian tener un sobreajuste minimo y se obtiene una perdida baja lo que indica que el modelo tiene un buen ajuste.\n",
        "\n",
        "En el accuracy se observa como antes de la mitad de los epochs realizados ya se obtiene resultados de 99%, el modelo V1 tiene una ligera ventaja sobre el V2 y se observa como despues del epoch 15 se estabilizan, asi como la diferencia entre entrenamiento y validacion es minima.\n",
        "\n",
        "En los graficos de F1 Score, se muestra una historia similar, la cual alrededor del epoch 10-15 se empieza a estabilizar la puntuacio y rapidamente llegan a un punto alto de alrededor de 0,95, la arquitectura v1 sigue siendo la mejor pero con una ventaja minima y el valor del F1 Score es alto indicando un balance bueno entre precisión y recall. Al este ser alto y el accuracy tambien, se puede decir que el modelo maneja un buen rendimiento.\n",
        "\n",
        "Al comparar ambas arquitecturas estras muestan varianzas minimas entre ellas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluacion con el conjunto de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"Evalúa el modelo en el conjunto de prueba\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            \n",
        "            probabilities = outputs.cpu().numpy().flatten()\n",
        "            predictions = (outputs > 0.5).float().cpu().numpy().flatten()\n",
        "            targets = batch_y.cpu().numpy().flatten()\n",
        "            \n",
        "            all_predictions.extend(predictions)\n",
        "            all_targets.extend(targets)\n",
        "            all_probabilities.extend(probabilities)\n",
        "    \n",
        "    return np.array(all_predictions), np.array(all_targets), np.array(all_probabilities)\n",
        "\n",
        "print(\"=== EVALUACIÓN EN CONJUNTO DE PRUEBA ===\")\n",
        "\n",
        "# Evaluar ambos modelos\n",
        "pred_v1, true_labels, prob_v1 = evaluate_model(model_v1_final, test_loader)\n",
        "pred_v2, _, prob_v2 = evaluate_model(model_v2_final, test_loader)\n",
        "\n",
        "# Calcular métricas\n",
        "f1_v1 = f1_score(true_labels, pred_v1)\n",
        "f1_v2 = f1_score(true_labels, pred_v2)\n",
        "\n",
        "accuracy_v1 = np.mean(pred_v1 == true_labels)\n",
        "accuracy_v2 = np.mean(pred_v2 == true_labels)\n",
        "\n",
        "print(f\"\\nRESULTADOS EN CONJUNTO DE PRUEBA:\")\n",
        "print(f\"{'Modelo':<15} {'Accuracy':<10} {'F1-Score':<10}\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"{'V1 (Profunda)':<15} {accuracy_v1:<10.4f} {f1_v1:<10.4f}\")\n",
        "print(f\"{'V2 (Simple)':<15} {accuracy_v2:<10.4f} {f1_v2:<10.4f}\")\n",
        "\n",
        "# Reportes detallados\n",
        "print(f\"\\n=== REPORTE DETALLADO - MODELO V1 ===\")\n",
        "print(classification_report(true_labels, pred_v1, target_names=['Ham', 'Spam']))\n",
        "\n",
        "print(f\"\\n=== REPORTE DETALLADO - MODELO V2 ===\")\n",
        "print(classification_report(true_labels, pred_v2, target_names=['Ham', 'Spam']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la validacion final con los datos de prueba que fueron apartados anteriormente se desarrollo la funcion de evaluacion que se encarga de activar el modo de evaluacion en el modelo y se realizan las predicciones correspondientes sin el uso de gradientes, de esta iteracion se extraen todas las prediciones, probabilidades y los targets para la generacion de las metricas.\n",
        "\n",
        "Se utiliza la funcion `evaluate_model` para evaluar los difeentes modelos por separados y realizar una comparacion correcta, se realiza el calculo del F1 Score y el accuracy asi como un `classification_report` para observar las diferentes metricas de precision, recall, f1, support y los mometos estadisticos como promedio macro, con peso y los calculos por cada clase.\n",
        "\n",
        "En cuanto a los resultados finales, podemos observar como en general las dos arquitecturas tiene un rendimiento muy similar y la arquitectura profunda tiene una ventaja minima. Ambos tiene un accuracy de mas del 98% asi como un F1 Score mayor a 0.94.\n",
        "\n",
        "Si nos vamos mas a fondo por cada clase, la etiqueta ham tiene unas metricas casi perfectas, esta clase es la que mas contenia datos para realizar pruebas. Estas metricas indican que los modelos pueden detectar todos los mensajes de ham, casi que nula clasificacion de spam como ham y cero diferencia en metricas. Estos resultados dicen que no existen falsos negativos para ham.\n",
        "\n",
        "En cuanto a la clase de spam, esta contenia muchos menos datos para probar. En cuanto a esta clase si se observa un poco mas de diferencia en cuanto a las arquitecturas. En el modelo V1 se obtiene un mejor recall y menor precision, por lo que detecta mas spam pero genera un poco mas de falsos positivos, la red V2 genera menos falsos positivos al tener mejor precision y detecta menos spam real al tener un menor recall por lo que ambos tiene compromisos el uno al otro en esta prueba.\n",
        "\n",
        "En general se obtiene que se detecta correctamente entre 90-95% de spam en ambos modelos, asi como un 5-10% de spam que se puede no detectar. En cuanto a mensajes clasificados como spam se obtiene un 97-99% de mensajes detectados como spam que realmente es spam y muy pocos falsos positivos 1-3%, esto para ambas arquitecturas.\n",
        "\n",
        "En general para un tiempo de entrenamiento reducido y una red neuronal pequeñas se demuestra como se obtienen buenos valores, que no perfectos pero con compromisos minimos, lo que demuesta la capacidad de BERT para generar los embeddings, asi como arquitecturas simples que manejan de una manera satisfactoria el problema a un buen nivel, manejando un desbalance de clases correctamente, especialmente en un dataset tan desbalanceado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Matriz de confusión V1\n",
        "plt.subplot(1, 2, 1)\n",
        "cm_v1 = confusion_matrix(true_labels, pred_v1)\n",
        "sns.heatmap(cm_v1, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title(f'Modelo V1 (Profunda)\\nF1-Score: {f1_v1:.4f}')\n",
        "plt.ylabel('Etiqueta Real')\n",
        "plt.xlabel('Predicción')\n",
        "\n",
        "# Matriz de confusión V2\n",
        "plt.subplot(1, 2, 2)\n",
        "cm_v2 = confusion_matrix(true_labels, pred_v2)\n",
        "sns.heatmap(cm_v2, annot=True, fmt='d', cmap='Reds',\n",
        "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.title(f'Modelo V2 (Simple)\\nF1-Score: {f1_v2:.4f}')\n",
        "plt.ylabel('Etiqueta Real')\n",
        "plt.xlabel('Predicción')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En estas matrices de confusion de apoyan los resultados en los cuales denota como ambas arquitecturas tienen muy pocos errores en comparacion con su exito por lo que es una buena solucion para el problema de deteccion de spam y aun con mucho mas espacio de mejora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (b) Evalue ambas arquitecturas previamente entrenadas, en 10 particiones aleatorias de entrenamiento y prueba, y reporte el F1-score promedio para ambas y su desviacion estandar. Comente los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento con particiones diferentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"                    PARTE 1B: VALIDACIÓN CON 10 PARTICIONES ALEATORIAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def train_and_evaluate_on_split(model_class, X, y, test_size=0.3, random_state=42, \n",
        "                               best_params=None, epochs=40, verbose=False):\n",
        "    \"\"\"\n",
        "    Entrena y evalúa un modelo de red neuronal en una partición específica.\n",
        "    \n",
        "    Parámetros\n",
        "    ----------\n",
        "    model_class : class\n",
        "        Clase del modelo de PyTorch a instanciar (ej: SpamClassifierV1, SpamClassifierV2).\n",
        "        Debe heredar de nn.Module y tener una salida con activación sigmoid para clasificación binaria.\n",
        "        \n",
        "    X : torch.Tensor\n",
        "        Tensor de características de entrada con shape [n_samples, n_features].\n",
        "        Típicamente embeddings BERT con dimensionalidad de 768.\n",
        "        \n",
        "    y : torch.Tensor  \n",
        "        Tensor de etiquetas binarias con shape [n_samples].\n",
        "        Valores: 0 (ham) o 1 (spam) como enteros.\n",
        "        \n",
        "    test_size : float, opcional, default=0.3\n",
        "        Proporción del dataset a usar para prueba. El resto se usa para entrenamiento.\n",
        "        Debe estar entre 0.0 y 1.0.\n",
        "        \n",
        "    random_state : int, opcional, default=42\n",
        "        Semilla para la división aleatoria de datos. Permite reproducibilidad de resultados\n",
        "        entre diferentes ejecuciones con los mismos parámetros.\n",
        "        \n",
        "    best_params : dict, opcional, default=None\n",
        "        Diccionario con los hiperparámetros optimizados del modelo:\n",
        "        - 'lr' : float - Learning rate para el optimizador Adam\n",
        "        - 'wd' : float - Coeficiente de weight decay (regularización L2)\n",
        "        Si es None, se deben proporcionar valores por defecto.\n",
        "        \n",
        "    epochs : int, opcional, default=40\n",
        "        Número de épocas de entrenamiento. Controla cuántas pasadas completas\n",
        "        sobre los datos de entrenamiento realizará el modelo.\n",
        "        \n",
        "    verbose : bool, opcional, default=False\n",
        "        Si True, imprime información de pérdida cada 10 épocas durante el entrenamiento.\n",
        "        Útil para monitoreo del progreso en entrenamientos largos.\n",
        "    \n",
        "    Retorna\n",
        "    -------\n",
        "    tuple[float, float]\n",
        "        Una tupla con las métricas de evaluación en el conjunto de prueba:\n",
        "        - f1 : float\n",
        "            F1-score del modelo en el conjunto de prueba. Métrica balanceada que \n",
        "            combina precisión y recall, especialmente útil para datos desbalanceados.\n",
        "        - accuracy : float  \n",
        "            Exactitud (accuracy) del modelo en el conjunto de prueba.\n",
        "            Proporción de predicciones correctas sobre el total de muestras.\n",
        "    \"\"\"\n",
        "    # Dividir datos\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Crear datasets y loaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    \n",
        "    # Crear y entrenar modelo\n",
        "    model = model_class()\n",
        "    model = model.to(device)\n",
        "    \n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), \n",
        "                          lr=best_params['lr'], \n",
        "                          weight_decay=best_params['wd'])\n",
        "    \n",
        "    # Entrenamiento\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device).float().unsqueeze(1)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            print(f'    Época {epoch+1}/{epochs}: Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "    \n",
        "    # Evaluación\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            \n",
        "            predictions = (outputs > 0.5).float().cpu().numpy().flatten()\n",
        "            targets = batch_y.cpu().numpy().flatten()\n",
        "            \n",
        "            all_predictions.extend(predictions)\n",
        "            all_targets.extend(targets)\n",
        "    \n",
        "    # Calcular métricas\n",
        "    f1 = f1_score(all_targets, all_predictions)\n",
        "    accuracy = np.mean(np.array(all_predictions) == np.array(all_targets))\n",
        "    \n",
        "    return f1, accuracy\n",
        "\n",
        "# Configuración para las 10 particiones\n",
        "n_splits = 10\n",
        "test_size = 0.3  # 70% entrenamiento, 30% prueba\n",
        "epochs_per_split = 40 \n",
        "\n",
        "# Almacenar resultados\n",
        "results_v1 = {'f1_scores': [], 'accuracies': [], 'times': []}\n",
        "results_v2 = {'f1_scores': [], 'accuracies': [], 'times': []}\n",
        "\n",
        "print(f\"Evaluando ambos modelos en {n_splits} particiones aleatorias...\")\n",
        "print(f\"Configuración: {100*(1-test_size):.0f}% entrenamiento, {100*test_size:.0f}% prueba, {epochs_per_split} épocas por partición\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Usar los mismos datos originales (embeddings_tensor, labels_tensor)\n",
        "total_start_time = time.time()\n",
        "\n",
        "for split in range(n_splits):\n",
        "    print(f\"\\nPARTICIÓN {split + 1}/{n_splits}\")\n",
        "    \n",
        "    # Usar diferentes seeds para cada partición\n",
        "    random_seed = 42 + split * 10\n",
        "    \n",
        "    # Evaluar Modelo V1\n",
        "    print(\"  Entrenando Modelo V1 (Red Profunda)...\")\n",
        "    start_time = time.time()\n",
        "    f1_v1, acc_v1 = train_and_evaluate_on_split(\n",
        "        SpamClassifierV1, embeddings_tensor, labels_tensor,\n",
        "        test_size=test_size, random_state=random_seed,\n",
        "        best_params=best_params_v1, epochs=epochs_per_split,\n",
        "        verbose=False\n",
        "    )\n",
        "    time_v1 = time.time() - start_time\n",
        "    \n",
        "    results_v1['f1_scores'].append(f1_v1)\n",
        "    results_v1['accuracies'].append(acc_v1)\n",
        "    results_v1['times'].append(time_v1)\n",
        "    \n",
        "    # Evaluar Modelo V2\n",
        "    print(\"  Entrenando Modelo V2 (Red Simple)...\")\n",
        "    start_time = time.time()\n",
        "    f1_v2, acc_v2 = train_and_evaluate_on_split(\n",
        "        SpamClassifierV2, embeddings_tensor, labels_tensor,\n",
        "        test_size=test_size, random_state=random_seed,\n",
        "        best_params=best_params_v2, epochs=epochs_per_split,\n",
        "        verbose=False\n",
        "    )\n",
        "    time_v2 = time.time() - start_time\n",
        "    \n",
        "    results_v2['f1_scores'].append(f1_v2)\n",
        "    results_v2['accuracies'].append(acc_v2)\n",
        "    results_v2['times'].append(time_v2)\n",
        "    \n",
        "    # Mostrar resultados de esta partición\n",
        "    print(f\"  Resultados Partición {split + 1}:\")\n",
        "    print(f\"    V1: F1={f1_v1:.4f}, Acc={acc_v1:.4f}, Tiempo={time_v1:.1f}s\")\n",
        "    print(f\"    V2: F1={f1_v2:.4f}, Acc={acc_v2:.4f}, Tiempo={time_v2:.1f}s\")\n",
        "\n",
        "total_time = time.time() - total_start_time\n",
        "print(f\"\\nEvaluación completada en {total_time:.1f} segundos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para esta seccion del entrenamiento con particiones se genero la funcion `train_and_evaluate_on_split` la cual recibe el modelo, la division de un 70/30% para el entrenamiento y pruebas, los epochs y si se genera output verbose. Esta funcion contiene logica para partir el dataset con estratificacion para mantener ambas clases y con el uso de semillas para poder repetir los resultados en multiples corridas. No se utiliza el conjunto de validacion ya que en este nos vamos a fijar en nada mas el rendimiento del modelo y utilizar el esquema original no es necesario ya que seria mas costoso y esto es mas para comparacion y referencia.\n",
        "\n",
        "Esta seccion utiliza la misma lgica anterior la cual se crean los TensorDataset, se carga el modelo al GPU, se alista el BCELoss y el optimizador Adam, asi como el mismo proceso de entrenamiento con el paso de `zero_grad`, model, criterion, loss.backward y optimizer.step. La unica diferencia es que en este se corren todos los epochs y no hay early stopping.\n",
        "\n",
        "Finalmente se almacenan las metricas de F1 Score, accuracy y tiempo de entrenamiento para cada una, esto con el fin de realizar un analisis comparativo mas adelante.\n",
        "\n",
        "Se ejecutan el entrenamiento por arquitectura para las diez particiones necesarias y se van guardando las metricas de cada una."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resultados del entrenamiento con particiones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"                 ESTADÍSTICAS DE LAS 10 PARTICIONES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convertir a arrays numpy para facilitar cálculos\n",
        "f1_scores_v1 = np.array(results_v1['f1_scores'])\n",
        "f1_scores_v2 = np.array(results_v2['f1_scores'])\n",
        "accuracies_v1 = np.array(results_v1['accuracies'])\n",
        "accuracies_v2 = np.array(results_v2['accuracies'])\n",
        "\n",
        "# Calcular estadísticas\n",
        "stats_v1 = {\n",
        "    'f1_mean': np.mean(f1_scores_v1),\n",
        "    'f1_std': np.std(f1_scores_v1),\n",
        "    'f1_min': np.min(f1_scores_v1),\n",
        "    'f1_max': np.max(f1_scores_v1),\n",
        "    'acc_mean': np.mean(accuracies_v1),\n",
        "    'acc_std': np.std(accuracies_v1),\n",
        "    'time_mean': np.mean(results_v1['times'])\n",
        "}\n",
        "\n",
        "stats_v2 = {\n",
        "    'f1_mean': np.mean(f1_scores_v2),\n",
        "    'f1_std': np.std(f1_scores_v2),\n",
        "    'f1_min': np.min(f1_scores_v2),\n",
        "    'f1_max': np.max(f1_scores_v2),\n",
        "    'acc_mean': np.mean(accuracies_v2),\n",
        "    'acc_std': np.std(accuracies_v2),\n",
        "    'time_mean': np.mean(results_v2['times'])\n",
        "}\n",
        "\n",
        "# Mostrar resultados en tabla\n",
        "print(f\"\\n{'Métrica':<20} {'Modelo V1':<15} {'Modelo V2':<15} {'Diferencia':<12}\")\n",
        "print(\"-\" * 65)\n",
        "print(f\"{'F1-Score (μ ± σ)':<20} {stats_v1['f1_mean']:.4f} ± {stats_v1['f1_std']:.4f}  {stats_v2['f1_mean']:.4f} ± {stats_v2['f1_std']:.4f}  {abs(stats_v1['f1_mean'] - stats_v2['f1_mean']):.4f}\")\n",
        "print(f\"{'F1-Score (min/max)':<20} {stats_v1['f1_min']:.4f} / {stats_v1['f1_max']:.4f}  {stats_v2['f1_min']:.4f} / {stats_v2['f1_max']:.4f}  -\")\n",
        "print(f\"{'Accuracy (μ ± σ)':<20} {stats_v1['acc_mean']:.4f} ± {stats_v1['acc_std']:.4f}  {stats_v2['acc_mean']:.4f} ± {stats_v2['acc_std']:.4f}  {abs(stats_v1['acc_mean'] - stats_v2['acc_mean']):.4f}\")\n",
        "print(f\"{'Tiempo promedio':<20} {stats_v1['time_mean']:.1f}s        {stats_v2['time_mean']:.1f}s        {abs(stats_v1['time_mean'] - stats_v2['time_mean']):.1f}s\")\n",
        "\n",
        "# Crear DataFrame para mejor visualización\n",
        "results_df = pd.DataFrame({\n",
        "    'Partición': range(1, n_splits + 1),\n",
        "    'V1_F1': f1_scores_v1,\n",
        "    'V2_F1': f1_scores_v2,\n",
        "    'V1_Acc': accuracies_v1,\n",
        "    'V2_Acc': accuracies_v2,\n",
        "    'Diferencia_F1': f1_scores_v1 - f1_scores_v2\n",
        "})\n",
        "\n",
        "print(f\"\\nDETALLE POR PARTICIÓN:\")\n",
        "print(results_df.round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se observan de los resultados mostrados por la celda anterior sobre las corridas de las diferentes particiones en ambas arquitecturas se muestran distintos puntos.\n",
        "\n",
        "El F1 Score en la arquitectura V1 sigue siendo superior al llegar a mas de 0,96 en promedio y tener valores consistenes y muy similares, su accuracy tiene una diferencia menor en comparacion con la parte V2, ademas de individualmente presentar una mejora con los resultados iniciales. Al ver tambien los maximos y minimos la arquitectura V2 tiene un valor mas bajo en comparacion y la arquitectura V1 siempre es mas estable nunca bajando de 0,95.\n",
        "\n",
        "En cuanto a la desviacion estandar del F1 Score se muestra que la arquitectura V1 es mucho mas estable y la V2 es casi dos veces mas dispersa, demostrando que la arquitectura V1 es mas estable.\n",
        "\n",
        "En el accuracy podemos ver en su desviacion estandar y resultados por particion, se puede ver como los resultados son muy similares y no existe una diferencia destacable entre las dos, ambas presentan valores estables y su desviacion indica que los resultados no estaban muy dispersos, asimismos sus resultados de accuracy son de mas de 97% y no hay cambios drasticos entre corridas ni entre arquitecturas.\n",
        "\n",
        "En cuanto al entrenamiento V2 es mas rapida por ser un poco mas simple pero tiene un costo de ser mas inestable y tener un poco de peores resultados, V1 sigue demostrando ser inclusive mejor que los resultados iniciales y los \"peores\" casos son de igual manera buenos por lo que es un buen candidato para la tarea y tiene una mejor capacidad de representacion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Boxplot de F1-Scores\n",
        "ax1 = axes[0, 0]\n",
        "box_data = [f1_scores_v1, f1_scores_v2]\n",
        "box_labels = ['Modelo V1\\n(Red Profunda)', 'Modelo V2\\n(Red Simple)']\n",
        "bp = ax1.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "bp['boxes'][0].set_facecolor('lightblue')\n",
        "bp['boxes'][1].set_facecolor('lightcoral')\n",
        "ax1.set_title('Distribución de F1-Scores\\n(10 Particiones Aleatorias)')\n",
        "ax1.set_ylabel('F1-Score')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Agregar puntos individuales\n",
        "for i, data in enumerate(box_data, 1):\n",
        "    y = data\n",
        "    x = np.random.normal(i, 0.04, size=len(y))\n",
        "    ax1.scatter(x, y, alpha=0.6, s=20)\n",
        "\n",
        "# 2. F1-Scores por partición\n",
        "ax2 = axes[0, 1]\n",
        "partitions = range(1, n_splits + 1)\n",
        "ax2.plot(partitions, f1_scores_v1, 'o-', label='Modelo V1', color='blue', alpha=0.7)\n",
        "ax2.plot(partitions, f1_scores_v2, 'o-', label='Modelo V2', color='red', alpha=0.7)\n",
        "ax2.set_title('F1-Score por Partición')\n",
        "ax2.set_xlabel('Número de Partición')\n",
        "ax2.set_ylabel('F1-Score')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Histograma de diferencias\n",
        "ax3 = axes[1, 0]\n",
        "differences = f1_scores_v1 - f1_scores_v2\n",
        "ax3.hist(differences, bins=6, alpha=0.7, color='green', edgecolor='black')\n",
        "ax3.axvline(np.mean(differences), color='red', linestyle='--', \n",
        "            label=f'Media: {np.mean(differences):.4f}')\n",
        "ax3.set_title('Distribución de Diferencias\\n(V1 - V2) en F1-Score')\n",
        "ax3.set_xlabel('Diferencia en F1-Score')\n",
        "ax3.set_ylabel('Frecuencia')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Comparación de medias con intervalos de confianza\n",
        "ax4 = axes[1, 1]\n",
        "models = ['Modelo V1', 'Modelo V2']\n",
        "means = [stats_v1['f1_mean'], stats_v2['f1_mean']]\n",
        "stds = [stats_v1['f1_std'], stats_v2['f1_std']]\n",
        "colors = ['blue', 'red']\n",
        "\n",
        "bars = ax4.bar(models, means, yerr=stds, capsize=5, \n",
        "               color=colors, alpha=0.7, edgecolor='black')\n",
        "ax4.set_title('F1-Score Promedio ± Desviación Estándar')\n",
        "ax4.set_ylabel('F1-Score')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Agregar valores en las barras\n",
        "for bar, mean, std in zip(bars, means, stds):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height + std + 0.005,\n",
        "             f'{mean:.4f}±{std:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En estas graficas se pueden visualizar mejor los resultados explicados anteriormente, se muestra la distribucion de F1 Score, por particion, promedio y desviacion estandar del mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. (20 puntos) Escoja al menos 2 modelos grandes del lenguaje, y uselos por medio del API de huggingface para hacer la clasificacion de los mensajes del dataset SMS_dataset, usando al menos una de las particiones de prueba anteriores. Reporte el prompt usado. Reporte el F1-score promedio para ambas y su desviacion estandar. Comente los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers accelerate bitsandbytes --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generacion de Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_spam_classification_prompt(message):\n",
        "    \"\"\"\n",
        "    Crea un prompt estructurado para clasificación de spam usando el conocimiento del dominio.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    message : str\n",
        "        Mensaje SMS a clasificar\n",
        "        \n",
        "    Retorna:\n",
        "    --------\n",
        "    str : Prompt formateado para el LLM\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Task: Classify the following SMS message as either \"spam\" or \"ham\" (legitimate).\n",
        "\n",
        "Guidelines:\n",
        "- SPAM indicators: promotional content, prizes, urgent calls to action, suspicious links, money requests, lottery/contests\n",
        "- HAM indicators: personal conversations, legitimate business communications, informational messages\n",
        "\n",
        "SMS Message: \"{message}\"\n",
        "\n",
        "Classification: \"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "def extract_classification_from_response(response):\n",
        "    \"\"\"\n",
        "    Extrae la clasificación de la respuesta del LLM.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    response : str\n",
        "        Respuesta completa del modelo\n",
        "        \n",
        "    Retorna:\n",
        "    --------\n",
        "    str : 'spam' o 'ham'\n",
        "    \"\"\"\n",
        "    response_clean = response.lower().strip()\n",
        "    \n",
        "    # Buscar patrones de clasificación\n",
        "    if 'spam' in response_clean and 'ham' not in response_clean:\n",
        "        return 'spam'\n",
        "    elif 'ham' in response_clean and 'spam' not in response_clean:\n",
        "        return 'ham'\n",
        "    elif response_clean.startswith('spam'):\n",
        "        return 'spam'  \n",
        "    elif response_clean.startswith('ham'):\n",
        "        return 'ham'\n",
        "    else:\n",
        "        # Clasificación por defecto en casos ambiguos\n",
        "        return 'ham'\n",
        "\n",
        "# Mostrar ejemplo del prompt utilizado\n",
        "sample_msg = \"URGENT! You've won £1000! Call 09061701461 to claim now!\"\n",
        "print(\"PROMPT UTILIZADO EN LA EVALUACIÓN:\")\n",
        "print(\"=\"*50)\n",
        "print(create_spam_classification_prompt(sample_msg))\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logica de los Modelos y Funciones de Clasificacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LLMSpamClassifier:\n",
        "    \"\"\"\n",
        "    Clasificador de spam usando modelos de lenguaje.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, use_quantization=True):\n",
        "        self.model_name = model_name\n",
        "        self.use_quantization = use_quantization\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.device = device \n",
        "        \n",
        "    def load_model(self):\n",
        "        \"\"\"Carga el modelo y tokenizer.\"\"\"\n",
        "        try:\n",
        "            print(f\"Cargando {self.model_name}...\")\n",
        "            \n",
        "            # Configurar cuantización para eficiencia de memoria\n",
        "            quantization_config = None\n",
        "            if self.use_quantization and torch.cuda.is_available():\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_compute_dtype=torch.float16,\n",
        "                    bnb_4bit_quant_type=\"nf4\"\n",
        "                )\n",
        "            \n",
        "            # Cargar tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_name,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "            # Cargar modelo\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            print(f\"{self.model_name} cargado exitosamente\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error cargando {self.model_name}: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def classify_single_message(self, message, max_length=400):\n",
        "        \"\"\"\n",
        "        Clasifica un mensaje individual.\n",
        "        \n",
        "        Parámetros:\n",
        "        -----------\n",
        "        message : str\n",
        "            Mensaje a clasificar\n",
        "        max_length : int\n",
        "            Longitud máxima del prompt\n",
        "            \n",
        "        Retorna:\n",
        "        --------\n",
        "        str : 'spam' o 'ham'\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Modelo no cargado. Ejecuta load_model() primero.\")\n",
        "        \n",
        "        prompt = create_spam_classification_prompt(message)\n",
        "        \n",
        "        # Tokenizar entrada\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        )\n",
        "        \n",
        "        # Mover tensores al dispositivo del modelo\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        # Generar respuesta\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=8,\n",
        "                do_sample=False,\n",
        "                temperature=0.1,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decodificar solo la parte nueva de la respuesta\n",
        "        response = self.tokenizer.decode(\n",
        "            outputs[0][inputs['input_ids'].shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        \n",
        "        return extract_classification_from_response(response)\n",
        "    \n",
        "    def classify_messages_batch(self, messages, batch_size=4, show_progress=True):\n",
        "        \"\"\"\n",
        "        Clasifica un lote de mensajes.\n",
        "        \n",
        "        Parámetros:\n",
        "        -----------\n",
        "        messages : list\n",
        "            Lista de mensajes a clasificar\n",
        "        batch_size : int  \n",
        "            Tamaño del lote (reducido para LLMs)\n",
        "        show_progress : bool\n",
        "            Mostrar barra de progreso\n",
        "            \n",
        "        Retorna:\n",
        "        --------\n",
        "        list : Lista de clasificaciones\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        iterator = tqdm(range(0, len(messages), batch_size), \n",
        "                       desc=f\"Clasificando con {self.model_name.split('/')[-1]}\") if show_progress else range(0, len(messages), batch_size)\n",
        "        \n",
        "        for i in iterator:\n",
        "            batch = messages[i:i + batch_size]\n",
        "            \n",
        "            for message in batch:\n",
        "                try:\n",
        "                    pred = self.classify_single_message(message)\n",
        "                    predictions.append(pred)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error en mensaje: {str(e)}\")\n",
        "                    predictions.append('ham')  # Clasificación segura por defecto\n",
        "            \n",
        "            # Pausa breve para gestión de memoria\n",
        "            time.sleep(0.3)\n",
        "        \n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuracion de Modelos de Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración de los modelos LLM a evaluar\n",
        "\n",
        "# llm_models_config = {\n",
        "#     \"GPT2-Base\": {\n",
        "#         \"model_name\": \"gpt2\",\n",
        "#         \"use_quantization\": False,\n",
        "#         \"description\": \"GPT-2 base model - Lightweight generative model\"\n",
        "#     },\n",
        "#     \"DialoGPT\": {\n",
        "#         \"model_name\": \"microsoft/DialoGPT-small\", \n",
        "#         \"use_quantization\": False,\n",
        "#         \"description\": \"DialoGPT - Conversational AI model\"\n",
        "#     }\n",
        "# }\n",
        "\n",
        "llm_models_config = {\n",
        "    \"Llama-3-1B\": {\n",
        "        \"model_name\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        \"use_quantization\": False,\n",
        "        \"description\": \"Llama-3.2-1B-Instruct\"\n",
        "    },\n",
        "    \"Qwen-2-5-3B\": {\n",
        "        \"model_name\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        \"use_quantization\": False,\n",
        "        \"description\": \"Qwen2.5-3B-Instruct\"\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"MODELOS LLM CONFIGURADOS PARA EVALUACIÓN:\")\n",
        "print(\"-\" * 50)\n",
        "for name, config in llm_models_config.items():\n",
        "    print(f\"• {name}: {config['model_name']}\")\n",
        "    print(f\"  └─ {config['description']}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generación de Particiones y Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_test_partition(split_index, max_test_samples=60):\n",
        "    \"\"\"\n",
        "    Obtiene la misma partición de test usada en las evaluaciones de RN.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    split_index : int\n",
        "        Índice de la partición (0-9)\n",
        "    max_test_samples : int\n",
        "        Máximo de muestras para eficiencia con LLMs\n",
        "        \n",
        "    Retorna:\n",
        "    --------\n",
        "    tuple : (test_messages, test_labels_str, test_labels_numeric)\n",
        "    \"\"\"\n",
        "    # Usar la misma lógica que train_and_evaluate_on_split\n",
        "    random_seed = 42 + split_index * 10\n",
        "    test_size = 0.3\n",
        "    \n",
        "    # Dividir datos con la misma estrategia\n",
        "    original_indices = list(range(len(sms_df)))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        original_indices,\n",
        "        test_size=test_size,\n",
        "        random_state=random_seed,\n",
        "        stratify=labels_tensor.numpy()\n",
        "    )\n",
        "    \n",
        "    # Extraer mensajes y etiquetas del conjunto de prueba\n",
        "    test_messages = [sms_df.iloc[idx]['message'] for idx in test_idx]\n",
        "    test_labels_str = [sms_df.iloc[idx]['label'] for idx in test_idx]\n",
        "    test_labels_numeric = [1 if label == 'spam' else 0 for label in test_labels_str]\n",
        "    \n",
        "    # Muestreo estratificado para eficiencia con LLMs\n",
        "    if len(test_messages) > max_test_samples:\n",
        "        # Mantener proporción de clases en la muestra\n",
        "        stratified_sample_idx, _ = train_test_split(\n",
        "            range(len(test_messages)),\n",
        "            train_size=max_test_samples,\n",
        "            random_state=42,\n",
        "            stratify=test_labels_numeric\n",
        "        )\n",
        "        \n",
        "        test_messages = [test_messages[i] for i in stratified_sample_idx]\n",
        "        test_labels_str = [test_labels_str[i] for i in stratified_sample_idx]\n",
        "        test_labels_numeric = [test_labels_numeric[i] for i in stratified_sample_idx]\n",
        "    \n",
        "    return test_messages, test_labels_str, test_labels_numeric\n",
        "\n",
        "# Probar la función con una partición de ejemplo\n",
        "test_msgs, test_lbls_str, test_lbls_num = get_test_partition(0)\n",
        "print(f\"Partición de prueba extraída:\")\n",
        "print(f\"   • Total mensajes: {len(test_msgs)}\")\n",
        "print(f\"   • Ham: {test_lbls_str.count('ham')} ({test_lbls_str.count('ham')/len(test_msgs)*100:.1f}%)\")  \n",
        "print(f\"   • Spam: {test_lbls_str.count('spam')} ({test_lbls_str.count('spam')/len(test_msgs)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nEjemplos de mensajes:\")\n",
        "for i in range(min(3, len(test_msgs))):\n",
        "    print(f\"   [{test_lbls_str[i]}] {test_msgs[i][:70]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_llm_multiple_partitions(classifier, n_splits=5, max_samples_per_partition=60):\n",
        "    \"\"\"\n",
        "    Evalúa un clasificador LLM usando el mismo esquema de particiones que las redes neuronales.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    classifier : LLMSpamClassifier\n",
        "        Clasificador LLM inicializado y cargado\n",
        "    n_splits : int\n",
        "        Número de particiones (usar menos que RN por eficiencia)\n",
        "    max_samples_per_partition : int\n",
        "        Máximo de mensajes por partición\n",
        "        \n",
        "    Retorna:\n",
        "    --------\n",
        "    dict : Resultados con F1-scores y accuracies\n",
        "    \"\"\"\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "    \n",
        "    print(f\"\\nEvaluando {classifier.model_name.split('/')[-1]} en {n_splits} particiones\")\n",
        "    print(f\"Usando máximo {max_samples_per_partition} mensajes por partición\")\n",
        "    \n",
        "    for split in range(n_splits):\n",
        "        print(f\"\\n--- Partición {split + 1}/{n_splits} ---\")\n",
        "        \n",
        "        # Obtener datos de prueba para esta partición\n",
        "        test_messages, test_labels_str, test_labels_numeric = get_test_partition(\n",
        "            split, max_samples_per_partition\n",
        "        )\n",
        "        \n",
        "        print(f\"Evaluando {len(test_messages)} mensajes...\")\n",
        "        \n",
        "        try:\n",
        "            # Clasificar usando el LLM\n",
        "            predictions_str = classifier.classify_messages_batch(test_messages, batch_size=3)\n",
        "            \n",
        "            # Convertir predicciones a formato numérico\n",
        "            predictions_numeric = [1 if pred == 'spam' else 0 for pred in predictions_str]\n",
        "            \n",
        "            # Calcular métricas\n",
        "            f1 = f1_score(test_labels_numeric, predictions_numeric, zero_division=0.0)\n",
        "            accuracy = np.mean(np.array(predictions_numeric) == np.array(test_labels_numeric))\n",
        "            \n",
        "            f1_scores.append(f1)\n",
        "            accuracies.append(accuracy)\n",
        "            \n",
        "            print(f\"Resultados: F1={f1:.4f}, Accuracy={accuracy:.4f}\")\n",
        "            \n",
        "            # Mostrar ejemplos de clasificación\n",
        "            print(\"Ejemplos de clasificación:\")\n",
        "            correct_count = 0\n",
        "            for i in range(min(4, len(predictions_str))):\n",
        "                is_correct = predictions_str[i] == test_labels_str[i]\n",
        "                if is_correct:\n",
        "                    correct_count += 1\n",
        "                status = \"✅\" if is_correct else \"❌\"\n",
        "                print(f\"   {status} Real: {test_labels_str[i]:4} | Pred: {predictions_str[i]:4} | {test_messages[i][:55]}...\")\n",
        "            \n",
        "            print(f\"   Precisión en ejemplos: {correct_count}/{min(4, len(predictions_str))}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error en partición {split + 1}: {str(e)}\")\n",
        "            f1_scores.append(0.0)\n",
        "            accuracies.append(0.5)  # Random baseline\n",
        "    \n",
        "    # Calcular estadísticas finales\n",
        "    stats = {\n",
        "        'f1_scores': f1_scores,\n",
        "        'accuracies': accuracies,\n",
        "        'f1_mean': np.mean(f1_scores),\n",
        "        'f1_std': np.std(f1_scores),\n",
        "        'acc_mean': np.mean(accuracies),\n",
        "        'acc_std': np.std(accuracies)\n",
        "    }\n",
        "    \n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EVALUACIÓN PRINCIPAL DE LOS MODELOS LLM\n",
        "print(\"=\"*80)\n",
        "print(\"INICIANDO EVALUACIÓN DE MODELOS LLM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuración de la evaluación\n",
        "n_evaluation_splits = 4  # Reducido para eficiencia con LLMs\n",
        "llm_results = {}\n",
        "\n",
        "# Evaluar cada modelo LLM\n",
        "for model_name, config in llm_models_config.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUANDO: {model_name}\")\n",
        "    print(f\"Modelo: {config['model_name']}\")\n",
        "    print(f\"Descripción: {config['description']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Inicializar clasificador\n",
        "    classifier = LLMSpamClassifier(\n",
        "        config[\"model_name\"], \n",
        "        config[\"use_quantization\"]\n",
        "    )\n",
        "    \n",
        "    # Cargar modelo\n",
        "    model_loaded = classifier.load_model()\n",
        "    \n",
        "    if model_loaded:\n",
        "        try:\n",
        "            # Evaluar en múltiples particiones\n",
        "            results = evaluate_llm_multiple_partitions(\n",
        "                classifier, \n",
        "                n_evaluation_splits\n",
        "            )\n",
        "            \n",
        "            llm_results[model_name] = results\n",
        "            \n",
        "            print(f\"\\nRESULTADOS FINALES - {model_name}:\")\n",
        "            print(f\"   F1-Score: {results['f1_mean']:.4f} ± {results['f1_std']:.4f}\")\n",
        "            print(f\"   Accuracy: {results['acc_mean']:.4f} ± {results['acc_std']:.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error durante evaluación: {str(e)}\")\n",
        "            # Resultados por defecto en caso de error\n",
        "            llm_results[model_name] = {\n",
        "                'f1_scores': [0.0] * n_evaluation_splits,\n",
        "                'accuracies': [0.5] * n_evaluation_splits,\n",
        "                'f1_mean': 0.0,\n",
        "                'f1_std': 0.0,\n",
        "                'acc_mean': 0.5,\n",
        "                'acc_std': 0.0\n",
        "            }\n",
        "        \n",
        "        # Limpieza de memoria GPU\n",
        "        try:\n",
        "            del classifier.model\n",
        "            del classifier.tokenizer\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"Memoria GPU limpiada\")\n",
        "        except:\n",
        "            pass\n",
        "            \n",
        "    else:\n",
        "        print(f\"No se pudo cargar el modelo {model_name}\")\n",
        "        # Resultados por defecto\n",
        "        llm_results[model_name] = {\n",
        "            'f1_scores': [0.0] * n_evaluation_splits,\n",
        "            'accuracies': [0.5] * n_evaluation_splits,\n",
        "            'f1_mean': 0.0,\n",
        "            'f1_std': 0.0,\n",
        "            'acc_mean': 0.5,\n",
        "            'acc_std': 0.0\n",
        "        }\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"EVALUACIÓN COMPLETADA\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resultados Finales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANÁLISIS Y COMPARACIÓN DE RESULTADOS\n",
        "print(\"=\"*80)\n",
        "print(\"ANÁLISIS COMPARATIVO: LLMs vs REDES NEURONALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear tabla de resultados de LLMs\n",
        "print(\"\\nRESULTADOS DE MODELOS LLM:\")\n",
        "llm_results_df = pd.DataFrame({\n",
        "    'Modelo LLM': list(llm_results.keys()),\n",
        "    'F1-Score (μ)': [results['f1_mean'] for results in llm_results.values()],\n",
        "    'F1-Score (σ)': [results['f1_std'] for results in llm_results.values()],\n",
        "    'Accuracy (μ)': [results['acc_mean'] for results in llm_results.values()],\n",
        "    'Accuracy (σ)': [results['acc_std'] for results in llm_results.values()]\n",
        "})\n",
        "print(llm_results_df.round(4).to_string(index=False))\n",
        "\n",
        "# Comparación completa incluyendo redes neuronales\n",
        "print(f\"\\nCOMPARACIÓN COMPLETA (LLMs vs Redes Neuronales):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Crear datos para comparación\n",
        "all_models_data = []\n",
        "\n",
        "# Agregar resultados de redes neuronales (ya existentes)\n",
        "all_models_data.append({\n",
        "    'Modelo': 'Red Neuronal V1 (Profunda)',\n",
        "    'Tipo': 'Red Neuronal + BERT',\n",
        "    'F1-Score (μ)': stats_v1['f1_mean'],\n",
        "    'F1-Score (σ)': stats_v1['f1_std'],\n",
        "    'Accuracy (μ)': stats_v1['acc_mean'],\n",
        "    'Accuracy (σ)': stats_v1['acc_std'],\n",
        "    'Particiones': 10\n",
        "})\n",
        "\n",
        "all_models_data.append({\n",
        "    'Modelo': 'Red Neuronal V2 (Simple)',\n",
        "    'Tipo': 'Red Neuronal + BERT',\n",
        "    'F1-Score (μ)': stats_v2['f1_mean'],\n",
        "    'F1-Score (σ)': stats_v2['f1_std'], \n",
        "    'Accuracy (μ)': stats_v2['acc_mean'],\n",
        "    'Accuracy (σ)': stats_v2['acc_std'],\n",
        "    'Particiones': 10\n",
        "})\n",
        "\n",
        "# Agregar resultados de LLMs\n",
        "for model_name, results in llm_results.items():\n",
        "    all_models_data.append({\n",
        "        'Modelo': f'{model_name} (LLM)',\n",
        "        'Tipo': 'LLM',\n",
        "        'F1-Score (μ)': results['f1_mean'],\n",
        "        'F1-Score (σ)': results['f1_std'],\n",
        "        'Accuracy (μ)': results['acc_mean'],\n",
        "        'Accuracy (σ)': results['acc_std'],\n",
        "        'Particiones': n_evaluation_splits\n",
        "    })\n",
        "\n",
        "# Crear DataFrame completo\n",
        "comparison_df = pd.DataFrame(all_models_data)\n",
        "comparison_df_sorted = comparison_df.sort_values('F1-Score (μ)', ascending=False)\n",
        "\n",
        "print(comparison_df_sorted.round(4).to_string(index=False))\n",
        "\n",
        "# Identificar el mejor modelo\n",
        "best_model = comparison_df_sorted.iloc[0]\n",
        "print(f\"\\nMEJOR MODELO GENERAL:\")\n",
        "print(f\"   {best_model['Modelo']}\")\n",
        "print(f\"   F1-Score: {best_model['F1-Score (μ)']:.4f} ± {best_model['F1-Score (σ)']:.4f}\")\n",
        "\n",
        "# Análisis por tipo de modelo\n",
        "print(f\"\\nANÁLISIS POR TIPO:\")\n",
        "type_analysis = comparison_df.groupby('Tipo').agg({\n",
        "    'F1-Score (μ)': ['mean', 'max'],\n",
        "    'Accuracy (μ)': ['mean', 'max']\n",
        "}).round(4)\n",
        "type_analysis.columns = ['F1_Mean_Avg', 'F1_Best', 'Acc_Mean_Avg', 'Acc_Best']\n",
        "print(type_analysis)\n",
        "\n",
        "print(f\"\\nRANKING FINAL POR F1-SCORE:\")\n",
        "for i, (_, row) in enumerate(comparison_df_sorted.iterrows(), 1):\n",
        "    medal = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else f\"{i}.\"\n",
        "    print(f\"   {medal} {row['Modelo']}: {row['F1-Score (μ)']:.4f} ± {row['F1-Score (σ)']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configurar estilo de gráficos\n",
        "plt.style.use('default')\n",
        "colors = ['#3498DB', '#E74C3C', '#2ECC71', '#F39C12', '#9B59B6', '#1ABC9C']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Comparación F1-Score de todos los modelos\n",
        "ax1 = axes[0, 0]\n",
        "models_short = [name.split()[0] + (' RN' if 'Red' in name else ' LLM' if 'LLM' in name else '') \n",
        "                for name in comparison_df_sorted['Modelo']]\n",
        "f1_means = comparison_df_sorted['F1-Score (μ)']\n",
        "f1_stds = comparison_df_sorted['F1-Score (σ)']\n",
        "\n",
        "bars1 = ax1.bar(range(len(models_short)), f1_means, yerr=f1_stds, \n",
        "                capsize=5, color=colors[:len(models_short)], \n",
        "                alpha=0.8, edgecolor='black', linewidth=1)\n",
        "\n",
        "ax1.set_title('F1-Score: Comparación Todos los Modelos', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('F1-Score')\n",
        "ax1.set_xticks(range(len(models_short)))\n",
        "ax1.set_xticklabels(models_short, rotation=45, ha='right')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Agregar valores en las barras\n",
        "for i, (bar, mean, std) in enumerate(zip(bars1, f1_means, f1_stds)):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
        "             f'{mean:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "\n",
        "# 2. Comparación por tipo de modelo\n",
        "ax2 = axes[0, 1]\n",
        "type_comparison = comparison_df.groupby('Tipo')['F1-Score (μ)'].agg(['mean', 'std']).reset_index()\n",
        "type_colors = ['#3498DB', '#E74C3C']\n",
        "\n",
        "bars2 = ax2.bar(type_comparison['Tipo'], type_comparison['mean'], \n",
        "                yerr=type_comparison['std'], capsize=5,\n",
        "                color=type_colors, alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax2.set_title('F1-Score Promedio por Tipo', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('F1-Score Promedio')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "for bar, mean, std in zip(bars2, type_comparison['mean'], type_comparison['std']):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
        "             f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3. Scatter plot: Accuracy vs F1-Score\n",
        "ax3 = axes[1, 0]\n",
        "colors_scatter = ['blue' if 'Red' in modelo else 'red' for modelo in comparison_df['Modelo']]\n",
        "scatter = ax3.scatter(comparison_df['Accuracy (μ)'], comparison_df['F1-Score (μ)'], \n",
        "                     c=colors_scatter, alpha=0.7, s=150, edgecolors='black', linewidth=2)\n",
        "\n",
        "for i, modelo in enumerate(comparison_df['Modelo']):\n",
        "    label = modelo.split()[0] + (' RN' if 'Red' in modelo else ' LLM' if 'LLM' in modelo else '')\n",
        "    ax3.annotate(label, \n",
        "                (comparison_df.iloc[i]['Accuracy (μ)'], comparison_df.iloc[i]['F1-Score (μ)']),\n",
        "                xytext=(8, 8), textcoords='offset points', fontsize=9, fontweight='bold')\n",
        "\n",
        "ax3.set_xlabel('Accuracy')\n",
        "ax3.set_ylabel('F1-Score') \n",
        "ax3.set_title('Accuracy vs F1-Score', fontsize=14, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Leyenda personalizada\n",
        "from matplotlib.lines import Line2D\n",
        "legend_elements = [\n",
        "    Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=12, label='Red Neuronal + BERT'),\n",
        "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=12, label='LLM')\n",
        "]\n",
        "ax3.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "# 4. Distribución de F1-Scores (si hay datos suficientes)\n",
        "ax4 = axes[1, 1]\n",
        "if len(llm_results) > 0:\n",
        "    # Datos para boxplot\n",
        "    plot_data = []\n",
        "    plot_labels = []\n",
        "    \n",
        "    # Agregar datos de RNs\n",
        "    plot_data.extend([stats_v1['f1_mean']] * 10)  # Simular distribución\n",
        "    plot_labels.extend(['RN V1'] * 10)\n",
        "    plot_data.extend([stats_v2['f1_mean']] * 10)\n",
        "    plot_labels.extend(['RN V2'] * 10)\n",
        "    \n",
        "    # Agregar datos de LLMs\n",
        "    for model_name, results in llm_results.items():\n",
        "        if len(results['f1_scores']) > 0:\n",
        "            plot_data.extend(results['f1_scores'])\n",
        "            plot_labels.extend([model_name] * len(results['f1_scores']))\n",
        "    \n",
        "    if plot_data:\n",
        "        box_df = pd.DataFrame({'Modelo': plot_labels, 'F1_Score': plot_data})\n",
        "        sns.boxplot(data=box_df, x='Modelo', y='F1_Score', ax=ax4)\n",
        "        ax4.set_title('Distribución F1-Scores', fontsize=14, fontweight='bold')\n",
        "        ax4.tick_params(axis='x', rotation=45)\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax4.text(0.5, 0.5, 'Datos insuficientes\\npara distribución', \n",
        "             ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
        "    ax4.set_title('Distribución F1-Scores', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ANÁLISIS DETALLADO Y COMENTARIOS FINALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Determinar mejores modelos\n",
        "best_rn_f1 = max(stats_v1['f1_mean'], stats_v2['f1_mean'])\n",
        "best_llm_f1 = max([results['f1_mean'] for results in llm_results.values()]) if llm_results else 0.0\n",
        "\n",
        "print(f\"\"\"\n",
        "### RESULTADOS OBTENIDOS:\n",
        "\n",
        "**Modelos de Redes Neuronales (10 particiones):**\n",
        "• Red V1 (Profunda): F1 = {stats_v1['f1_mean']:.4f} ± {stats_v1['f1_std']:.4f}\n",
        "• Red V2 (Simple):   F1 = {stats_v2['f1_mean']:.4f} ± {stats_v2['f1_std']:.4f}\n",
        "\n",
        "**Modelos LLM ({n_evaluation_splits} particiones):**\"\"\")\n",
        "\n",
        "for model_name, results in llm_results.items():\n",
        "    print(f\"• {model_name}: F1 = {results['f1_mean']:.4f} ± {results['f1_std']:.4f}\")\n",
        "\n",
        "winner = \"Redes Neuronales\" if best_rn_f1 > best_llm_f1 else \"LLMs\"\n",
        "performance_gap = abs(best_rn_f1 - best_llm_f1)\n",
        "\n",
        "print(f\"\"\"\n",
        "### ANÁLISIS COMPARATIVO:\n",
        "\n",
        "**Mejor Rendimiento:** {winner} (diferencia: {performance_gap:.4f} F1-Score)\n",
        "**Modelo Ganador:** {comparison_df_sorted.iloc[0]['Modelo']}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. (40 puntos) Compare los resultados entre los 4 modelos entrenados, y argumente ventajas y desventajas de cada uno con respecto a los resultados logrados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*100)\n",
        "print(\"PUNTO 3: ANÁLISIS COMPARATIVO COMPLETO DE LOS 4 MODELOS ENTRENADOS\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "# Recopilar todos los resultados\n",
        "all_models_comparison = []\n",
        "\n",
        "# Agregar resultados de redes neuronales\n",
        "all_models_comparison.append({\n",
        "    'Modelo': 'Red Neuronal V1 (Profunda)',\n",
        "    'Tipo': 'Red Neuronal',\n",
        "    'Arquitectura': '4 capas (768→512→256→128→1)',\n",
        "    'F1_Mean': stats_v1['f1_mean'],\n",
        "    'F1_Std': stats_v1['f1_std'],\n",
        "    'Acc_Mean': stats_v1['acc_mean'],\n",
        "    'Acc_Std': stats_v1['acc_std'],\n",
        "    'Tiempo_Entrenamiento': stats_v1['time_mean'],\n",
        "    'Particiones': 10,\n",
        "    'Parametros_Aprox': '~2M parámetros de clasificador + BERT embeddings',\n",
        "    'Complejidad': 'Alta',\n",
        "    'Estabilidad': stats_v1['f1_std']\n",
        "})\n",
        "\n",
        "all_models_comparison.append({\n",
        "    'Modelo': 'Red Neuronal V2 (Simple)',\n",
        "    'Tipo': 'Red Neuronal', \n",
        "    'Arquitectura': '3 capas (768→256→64→1)',\n",
        "    'F1_Mean': stats_v2['f1_mean'],\n",
        "    'F1_Std': stats_v2['f1_std'],\n",
        "    'Acc_Mean': stats_v2['acc_mean'],\n",
        "    'Acc_Std': stats_v2['acc_std'],\n",
        "    'Tiempo_Entrenamiento': stats_v2['time_mean'],\n",
        "    'Particiones': 10,\n",
        "    'Parametros_Aprox': '~500K parámetros de clasificador + BERT embeddings',\n",
        "    'Complejidad': 'Media',\n",
        "    'Estabilidad': stats_v2['f1_std']\n",
        "})\n",
        "\n",
        "# Agregar resultados de LLMs\n",
        "for model_name, results in llm_results.items():\n",
        "    all_models_comparison.append({\n",
        "        'Modelo': f'{model_name}',\n",
        "        'Tipo': 'LLM',\n",
        "        'Arquitectura': 'Transformer autoregresivo',\n",
        "        'F1_Mean': results['f1_mean'],\n",
        "        'F1_Std': results['f1_std'],\n",
        "        'Acc_Mean': results['acc_mean'],\n",
        "        'Acc_Std': results['acc_std'],\n",
        "        'Tiempo_Entrenamiento': 'N/A (pre-entrenado)',\n",
        "        'Particiones': n_evaluation_splits,\n",
        "        'Parametros_Aprox': '3B+' if 'Qwen' in model_name else 'Variable',\n",
        "        'Complejidad': 'Muy Alta',\n",
        "        'Estabilidad': results['f1_std']\n",
        "    })\n",
        "\n",
        "# Crear DataFrame para análisis\n",
        "comparison_df = pd.DataFrame(all_models_comparison)\n",
        "comparison_df_sorted = comparison_df.sort_values('F1_Mean', ascending=False)\n",
        "\n",
        "print(\"\\nTABLA COMPARATIVA COMPLETA:\")\n",
        "print(\"-\"*120)\n",
        "display_cols = ['Modelo', 'Tipo', 'F1_Mean', 'F1_Std', 'Acc_Mean', 'Estabilidad', 'Complejidad']\n",
        "print(comparison_df_sorted[display_cols].round(4).to_string(index=False))\n",
        "\n",
        "print(f\"\\nRANKING DE RENDIMIENTO:\")\n",
        "for i, (_, row) in enumerate(comparison_df_sorted.iterrows(), 1):\n",
        "    medal = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else f\"{i}.\"\n",
        "    print(f\"   {medal} {row['Modelo']}: F1={row['F1_Mean']:.4f}±{row['F1_Std']:.4f}\")\n",
        "\n",
        "# Análisis por categorías\n",
        "print(f\"\\nANÁLISIS POR CATEGORÍAS:\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Mejor rendimiento general\n",
        "best_model = comparison_df_sorted.iloc[0]\n",
        "print(f\"MEJOR RENDIMIENTO GENERAL: {best_model['Modelo']}\")\n",
        "print(f\"   • F1-Score: {best_model['F1_Mean']:.4f} ± {best_model['F1_Std']:.4f}\")\n",
        "print(f\"   • Accuracy: {best_model['Acc_Mean']:.4f}\")\n",
        "\n",
        "# Más estable (menor desviación estándar)\n",
        "most_stable = comparison_df.loc[comparison_df['Estabilidad'].idxmin()]\n",
        "print(f\"\\nMÁS ESTABLE: {most_stable['Modelo']}\")\n",
        "print(f\"   • Desviación F1: {most_stable['Estabilidad']:.4f}\")\n",
        "print(f\"   • F1 promedio: {most_stable['F1_Mean']:.4f}\")\n",
        "\n",
        "# Análisis por tipo de modelo\n",
        "type_analysis = comparison_df.groupby('Tipo').agg({\n",
        "    'F1_Mean': ['mean', 'max', 'min'],\n",
        "    'Acc_Mean': ['mean', 'max', 'min'],\n",
        "    'Estabilidad': 'mean'\n",
        "}).round(4)\n",
        "\n",
        "print(f\"\\nANÁLISIS POR TIPO DE MODELO:\")\n",
        "print(type_analysis)\n",
        "\n",
        "# Crear visualizaciones comparativas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Rendimiento F1 por modelo\n",
        "ax1 = axes[0, 0]\n",
        "models_short = [name.split()[0] + (' RN' if 'Red' in name else '') for name in comparison_df_sorted['Modelo']]\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
        "bars = ax1.bar(models_short, comparison_df_sorted['F1_Mean'], \n",
        "               yerr=comparison_df_sorted['F1_Std'], capsize=5,\n",
        "               color=colors[:len(models_short)], alpha=0.8, edgecolor='black')\n",
        "ax1.set_title('F1-Score por Modelo', fontweight='bold')\n",
        "ax1.set_ylabel('F1-Score')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Agregar valores en las barras\n",
        "for bar, mean in zip(bars, comparison_df_sorted['F1_Mean']):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. Estabilidad vs Rendimiento\n",
        "ax2 = axes[0, 1]\n",
        "scatter = ax2.scatter(comparison_df['Estabilidad'], comparison_df['F1_Mean'], \n",
        "                     s=200, alpha=0.7, \n",
        "                     c=['blue' if 'Red' in x else 'red' for x in comparison_df['Modelo']],\n",
        "                     edgecolors='black', linewidth=2)\n",
        "ax2.set_xlabel('Desviación Estándar F1 (menor = más estable)')\n",
        "ax2.set_ylabel('F1-Score Promedio')\n",
        "ax2.set_title('Estabilidad vs Rendimiento', fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Agregar etiquetas\n",
        "for i, modelo in enumerate(comparison_df['Modelo']):\n",
        "    label = modelo.split()[0] + (' RN' if 'Red' in modelo else '')\n",
        "    ax2.annotate(label, \n",
        "                (comparison_df.iloc[i]['Estabilidad'], comparison_df.iloc[i]['F1_Mean']),\n",
        "                xytext=(8, 8), textcoords='offset points', fontweight='bold')\n",
        "\n",
        "# 3. Accuracy vs F1-Score\n",
        "ax3 = axes[0, 2]\n",
        "ax3.scatter(comparison_df['Acc_Mean'], comparison_df['F1_Mean'], \n",
        "           s=200, alpha=0.7,\n",
        "           c=['blue' if 'Red' in x else 'red' for x in comparison_df['Modelo']],\n",
        "           edgecolors='black', linewidth=2)\n",
        "ax3.set_xlabel('Accuracy Promedio')\n",
        "ax3.set_ylabel('F1-Score Promedio')\n",
        "ax3.set_title('Accuracy vs F1-Score', fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Comparación por tipo de modelo\n",
        "ax4 = axes[1, 0]\n",
        "type_means = comparison_df.groupby('Tipo')['F1_Mean'].mean()\n",
        "type_stds = comparison_df.groupby('Tipo')['F1_Mean'].std().fillna(0)\n",
        "bars = ax4.bar(type_means.index, type_means.values, \n",
        "               yerr=type_stds.values, capsize=5,\n",
        "               color=['#2E86AB', '#A23B72'], alpha=0.8)\n",
        "ax4.set_title('F1-Score Promedio por Tipo', fontweight='bold')\n",
        "ax4.set_ylabel('F1-Score Promedio')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Distribución de F1-Scores\n",
        "ax5 = axes[1, 1]\n",
        "rn_f1_scores = [stats_v1['f1_mean'], stats_v2['f1_mean']]\n",
        "llm_f1_scores = [results['f1_mean'] for results in llm_results.values()]\n",
        "ax5.boxplot([rn_f1_scores, llm_f1_scores], labels=['Redes Neuronales', 'LLMs'])\n",
        "ax5.set_title('Distribución F1-Scores por Tipo', fontweight='bold')\n",
        "ax5.set_ylabel('F1-Score')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Mapa de calor de métricas normalizadas\n",
        "ax6 = axes[1, 2]\n",
        "metrics_for_heatmap = comparison_df[['F1_Mean', 'Acc_Mean']].copy()\n",
        "# Normalizar métricas (invertir estabilidad para que mayor sea mejor)\n",
        "metrics_for_heatmap['Estabilidad_Inv'] = 1 / (comparison_df['Estabilidad'] + 0.001)\n",
        "metrics_normalized = (metrics_for_heatmap - metrics_for_heatmap.min()) / (metrics_for_heatmap.max() - metrics_for_heatmap.min())\n",
        "\n",
        "im = ax6.imshow(metrics_normalized.T, cmap='RdYlGn', aspect='auto')\n",
        "ax6.set_xticks(range(len(comparison_df)))\n",
        "ax6.set_xticklabels([name.split()[0] for name in comparison_df['Modelo']], rotation=45)\n",
        "ax6.set_yticks(range(len(metrics_normalized.columns)))\n",
        "ax6.set_yticklabels(['F1-Score', 'Accuracy', 'Estabilidad'])\n",
        "ax6.set_title('Mapa de Calor de Métricas\\n(Verde=Mejor, Rojo=Peor)', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANÁLISIS DETALLADO DE VENTAJAS Y DESVENTAJAS POR MODELO\n",
        "\n",
        "models_analysis = {\n",
        "    'Red Neuronal V1 (Profunda)': {\n",
        "        'ventajas': [\n",
        "            f\"MEJOR RENDIMIENTO: F1-Score más alto ({stats_v1['f1_mean']:.4f})\",\n",
        "            f\"ALTA ESTABILIDAD: Desviación estándar baja ({stats_v1['f1_std']:.4f})\",\n",
        "            \"EFICIENCIA COMPUTACIONAL: Rápido en inferencia una vez entrenado\",\n",
        "            \"ESPECIALIZACIÓN: Optimizado específicamente para detección de spam\",\n",
        "            \"MEMORIA EFICIENTE: Solo requiere almacenar embeddings BERT + clasificador\",\n",
        "            \"INTERPRETABILIDAD: Arquitectura simple y comprensible\",\n",
        "            f\"TIEMPO DE ENTRENAMIENTO RAZONABLE: {stats_v1['time_mean']:.1f}s promedio\"\n",
        "        ],\n",
        "        'desventajas': [\n",
        "            \"📚 DEPENDENCIA DE EMBEDDINGS: Requiere BERT pre-entrenado para funcionar\",\n",
        "            \"🔒 DOMINIO ESPECÍFICO: Limitado a tareas de clasificación de texto\",\n",
        "            \"📊 DATOS DE ENTRENAMIENTO: Necesita datos etiquetados para entrenamiento\",\n",
        "            \"🎨 FLEXIBILIDAD LIMITADA: No puede generar texto o realizar otras tareas NLP\",\n",
        "            \"🔄 REENTRENAMIENTO: Requiere reentrenamiento completo para nuevos dominios\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    'Red Neuronal V2 (Simple)': {\n",
        "        'ventajas': [\n",
        "            f\"MUY EFICIENTE: Menor complejidad computacional y memoria\",\n",
        "            f\"ENTRENAMIENTO RÁPIDO: {stats_v2['time_mean']:.1f}s promedio\",\n",
        "            \"SIMPLICIDAD: Arquitectura más simple, menos propenso a overfitting\",\n",
        "            f\"BUEN RENDIMIENTO: F1-Score competitivo ({stats_v2['f1_mean']:.4f})\",\n",
        "            \"FÁCIL IMPLEMENTACIÓN: Menos parámetros para ajustar\",\n",
        "            \"COSTO-EFECTIVO: Menor requerimiento de recursos computacionales\"\n",
        "        ],\n",
        "        'desventajas': [\n",
        "            f\"RENDIMIENTO INFERIOR: F1-Score menor que V1\",\n",
        "            f\"MENOS ESTABLE: Mayor variabilidad ({stats_v2['f1_std']:.4f})\",\n",
        "            \"MENOR CAPACIDAD DE REPRESENTACIÓN: Puede perder patrones complejos\",\n",
        "            \"MISMAS LIMITACIONES que V1: Dependencia de BERT, dominio específico\",\n",
        "            \"UNDERFITTING POTENCIAL: Podría no capturar toda la complejidad de los datos\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Agregar análisis de LLMs\n",
        "for model_name, results in llm_results.items():\n",
        "    models_analysis[model_name] = {\n",
        "        'ventajas': [\n",
        "            \"VERSATILIDAD EXTREMA: Puede realizar múltiples tareas NLP sin reentrenamiento\",\n",
        "            \"CAPACIDAD GENERATIVA: Puede generar texto y explicaciones\",\n",
        "            \"CONOCIMIENTO PREVIO: Incorpora conocimiento del mundo pre-entrenado\",\n",
        "            \"ZERO-SHOT LEARNING: Funciona sin ejemplos específicos de entrenamiento\",\n",
        "            \"PROMPT ENGINEERING: Ajustable mediante instrucciones en lenguaje natural\",\n",
        "            \"MULTILINGÜE: Capacidad para trabajar en múltiples idiomas\",\n",
        "            f\"F1-Score: {results['f1_mean']:.4f} sin entrenamiento específico\"\n",
        "        ],\n",
        "        'desventajas': [\n",
        "            f\"COSTO COMPUTACIONAL EXTREMO: Requiere recursos GPU/CPU masivos\",\n",
        "            f\"LATENCIA ALTA: Inferencia más lenta que redes neuronales especializadas\",\n",
        "            f\"VARIABILIDAD: Desviación estándar {results['f1_std']:.4f}\",\n",
        "            \"FALTA DE ESPECIALIZACIÓN: No optimizado específicamente para spam\",\n",
        "            \"MEMORIA MASIVA: Requiere GB de memoria para cargar el modelo\",\n",
        "            \"INCONSISTENCIA: Respuestas pueden variar entre ejecuciones\",\n",
        "            \"PROMPT DEPENDENCY: Rendimiento muy sensible al diseño del prompt\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Imprimir análisis detallado\n",
        "for modelo, analysis in models_analysis.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ANÁLISIS DETALLADO: {modelo}\")\n",
        "    print('='*80)\n",
        "    \n",
        "    # Obtener métricas del modelo\n",
        "    model_data = comparison_df[comparison_df['Modelo'].str.contains(modelo.split()[0])]\n",
        "    if not model_data.empty:\n",
        "        row = model_data.iloc[0]\n",
        "        print(f\"MÉTRICAS CLAVE:\")\n",
        "        print(f\"   • F1-Score: {row['F1_Mean']:.4f} ± {row['F1_Std']:.4f}\")\n",
        "        print(f\"   • Accuracy: {row['Acc_Mean']:.4f}\")\n",
        "        print(f\"   • Tipo: {row['Tipo']}\")\n",
        "        print(f\"   • Complejidad: {row['Complejidad']}\")\n",
        "    \n",
        "    print(f\"\\nVENTAJAS:\")\n",
        "    for i, ventaja in enumerate(analysis['ventajas'], 1):\n",
        "        print(f\"   {i}. {ventaja}\")\n",
        "    \n",
        "    print(f\"\\nDESVENTAJAS:\")\n",
        "    for i, desventaja in enumerate(analysis['desventajas'], 1):\n",
        "        print(f\"   {i}. {desventaja}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00942d554e7547a58c947f401076ffc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "103d097056d744b7ba3b4c41b8016038": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24bf8ee8e2aa433b8eeb73565036fed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8824c6c9c3f4dafab94905f2dde0117",
              "IPY_MODEL_ff5254e7c6b14468aafeeb333acdfd09",
              "IPY_MODEL_4f45c6eae349457db086d6445a5e43a0"
            ],
            "layout": "IPY_MODEL_325babe84ee84cd58fdb9d3e91b1c918"
          }
        },
        "325babe84ee84cd58fdb9d3e91b1c918": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32cc2727c4e24596b7dd6c3a02aa5664": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf2c248c780a42fb9186f69005c4dd4a",
            "placeholder": "​",
            "style": "IPY_MODEL_daac5f340a46431eb56d6eb7db39a5e6",
            "value": "vocab.txt: 100%"
          }
        },
        "32cec703c6594823b66986150f332797": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "331260d2d9a0425888f11360b4dd837c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6135e3dfe744412a00032d947c786bb",
            "placeholder": "​",
            "style": "IPY_MODEL_e232f1bc5a07418084e7f7fca277ea54",
            "value": " 466k/466k [00:00&lt;00:00, 23.0MB/s]"
          }
        },
        "34afe2fbc7bc4067be98712600a1415c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab03c76bcd342a7aeffcb89c108fff5",
            "placeholder": "​",
            "style": "IPY_MODEL_fab74cf88464444ca1fd48f91f929f93",
            "value": "model.safetensors: 100%"
          }
        },
        "3d7cf2567cc44dcf85240a6d59ca50e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e677c250b0d4375b203c5863021bcf8",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e67a614ca7af4b80a38b5721131798c9",
            "value": 231508
          }
        },
        "45349080c9a9406b83f687c4c950616b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da6e2d4730f4489a9fefba569017aa8d",
            "placeholder": "​",
            "style": "IPY_MODEL_32cec703c6594823b66986150f332797",
            "value": " 232k/232k [00:00&lt;00:00, 2.74MB/s]"
          }
        },
        "4f45c6eae349457db086d6445a5e43a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d26a04225c14e51b306500f88392c1a",
            "placeholder": "​",
            "style": "IPY_MODEL_5999b8d1dd364e2dad94012b2bdffe84",
            "value": " 570/570 [00:00&lt;00:00, 23.1kB/s]"
          }
        },
        "524edb35fe3f42a78c0b5629134ce9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "559d8b5889fc4ca0ae7ad4809013298c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "561cebef895d4f02b8a16a36727c0234": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5999b8d1dd364e2dad94012b2bdffe84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5de57bac529e4adcb668adf87ff6140a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c3367c168b47539489b5d4f8855b85",
            "placeholder": "​",
            "style": "IPY_MODEL_559d8b5889fc4ca0ae7ad4809013298c",
            "value": " 440M/440M [00:04&lt;00:00, 125MB/s]"
          }
        },
        "6ab03c76bcd342a7aeffcb89c108fff5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2d72af86964dd69c084040eba0d72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_524edb35fe3f42a78c0b5629134ce9c7",
            "placeholder": "​",
            "style": "IPY_MODEL_80f0fc10f20b40bcb4c3087afb4f7a8f",
            "value": "tokenizer.json: 100%"
          }
        },
        "7b509bfa1b7b490e9b309b1c5fa16d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3cbfeec510b45d38833a76e3dc55e59",
              "IPY_MODEL_f910b17d2b064d49b355e374c0e083d2",
              "IPY_MODEL_d52b5e9d4936490393f3891f287f1f44"
            ],
            "layout": "IPY_MODEL_e3ae2431135e40aaab919e74e7a6114b"
          }
        },
        "7ff203f173004583b8acb2e7586689fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a051b67f5cbe4672be421abe0558c287",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f86d1ca1dec1433fa6cb5223e2a5fd10",
            "value": 466062
          }
        },
        "80f0fc10f20b40bcb4c3087afb4f7a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85610ec32a8e46d68e5661274eb7dc5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d26a04225c14e51b306500f88392c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e677c250b0d4375b203c5863021bcf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e5f18228724e2b881249d8b89d5f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32cc2727c4e24596b7dd6c3a02aa5664",
              "IPY_MODEL_3d7cf2567cc44dcf85240a6d59ca50e3",
              "IPY_MODEL_45349080c9a9406b83f687c4c950616b"
            ],
            "layout": "IPY_MODEL_f56b9e7fc1ab4656a8b9b49b5b4e6cf7"
          }
        },
        "998f83401b4b4b388f9dc1c2796b07e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34afe2fbc7bc4067be98712600a1415c",
              "IPY_MODEL_f1ca477f371542fb81a3d07142e9e855",
              "IPY_MODEL_5de57bac529e4adcb668adf87ff6140a"
            ],
            "layout": "IPY_MODEL_b003be8eb821462dafc32997c123cdff"
          }
        },
        "a051b67f5cbe4672be421abe0558c287": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1b39397b75f4503a577b385453ee575": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa4d54529d664bcbbb414ec6c216389d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae20dda8aff84a1f975214847ddec1ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b003be8eb821462dafc32997c123cdff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f5fe36693444c4876b4c328d9b8d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bab02883fc2c4a609dee477cf44d25bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a2d72af86964dd69c084040eba0d72b",
              "IPY_MODEL_7ff203f173004583b8acb2e7586689fe",
              "IPY_MODEL_331260d2d9a0425888f11360b4dd837c"
            ],
            "layout": "IPY_MODEL_aa4d54529d664bcbbb414ec6c216389d"
          }
        },
        "bf2c248c780a42fb9186f69005c4dd4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5c6f84d25743fab39557dff236644a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccb4e2fd540b4166ab34a95c9a937282": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c3367c168b47539489b5d4f8855b85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52b5e9d4936490393f3891f287f1f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccb4e2fd540b4166ab34a95c9a937282",
            "placeholder": "​",
            "style": "IPY_MODEL_ca5c6f84d25743fab39557dff236644a",
            "value": " 48.0/48.0 [00:00&lt;00:00, 892B/s]"
          }
        },
        "d6c69ae7aaed4d0da141d5c4a0e916fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8824c6c9c3f4dafab94905f2dde0117": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_103d097056d744b7ba3b4c41b8016038",
            "placeholder": "​",
            "style": "IPY_MODEL_a1b39397b75f4503a577b385453ee575",
            "value": "config.json: 100%"
          }
        },
        "da6e2d4730f4489a9fefba569017aa8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daac5f340a46431eb56d6eb7db39a5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e232f1bc5a07418084e7f7fca277ea54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3ae2431135e40aaab919e74e7a6114b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e42945c82469453580312e6c8e7c2069": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e67a614ca7af4b80a38b5721131798c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1ca477f371542fb81a3d07142e9e855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_561cebef895d4f02b8a16a36727c0234",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85610ec32a8e46d68e5661274eb7dc5d",
            "value": 440449768
          }
        },
        "f3cbfeec510b45d38833a76e3dc55e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff126133fe504edeb7ec56322d1bcf1d",
            "placeholder": "​",
            "style": "IPY_MODEL_d6c69ae7aaed4d0da141d5c4a0e916fa",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f56b9e7fc1ab4656a8b9b49b5b4e6cf7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6135e3dfe744412a00032d947c786bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f86d1ca1dec1433fa6cb5223e2a5fd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f910b17d2b064d49b355e374c0e083d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7f5fe36693444c4876b4c328d9b8d0e",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e42945c82469453580312e6c8e7c2069",
            "value": 48
          }
        },
        "fab74cf88464444ca1fd48f91f929f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff126133fe504edeb7ec56322d1bcf1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5254e7c6b14468aafeeb333acdfd09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae20dda8aff84a1f975214847ddec1ce",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00942d554e7547a58c947f401076ffc8",
            "value": 570
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
