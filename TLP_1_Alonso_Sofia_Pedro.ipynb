{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBIp6DT8si_1"
   },
   "source": [
    "# Trabajo Pr√°ctico 1\n",
    "\n",
    "Estudiantes:\n",
    "\n",
    "- Alonso Araya Calvo\n",
    "- Pedro Soto\n",
    "- Sofia Oviedo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLfMPjjz6Tqf"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:32:56.720644Z",
     "start_time": "2025-08-18T22:32:56.718567Z"
    },
    "id": "7lh6x4GjDbzO"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TqpxBWGYveZ"
   },
   "source": [
    "# Cargando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.627208Z",
     "start_time": "2025-08-18T22:32:56.739590Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pn_P8kZ6a4G",
    "outputId": "5e66e87d-e21b-467a-d7b5-7469ca29a80f"
   },
   "outputs": [],
   "source": [
    "#tomado de https://www.kaggle.com/code/wailinnoo/intrusion-detection-system-using-kdd99-dataset\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "try:\n",
    "    path = get_file('kddcup.data_10_percent.gz',\n",
    "                    origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
    "except:\n",
    "    print('Error downloading')\n",
    "    raise\n",
    "\n",
    "print(path)\n",
    "\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "# Download from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "pd_data_frame = pd.read_csv(path, header=None)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "pd_data_frame.columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'outcome'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyxCEYNuY0Dc"
   },
   "source": [
    "# Limpieza del dataset y generaci√≥n de subset del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.883181Z",
     "start_time": "2025-08-18T22:33:01.641539Z"
    },
    "id": "Gc6X0vQv5ad6"
   },
   "outputs": [],
   "source": [
    "# For now, just drop NA's (rows with missing values), in case there are\n",
    "pd_data_frame.dropna(inplace=True, axis=1)\n",
    "\n",
    "# Checking for DUPLICATE values\n",
    "pd_data_frame.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.903294Z",
     "start_time": "2025-08-18T22:33:01.887249Z"
    },
    "id": "G_m74GC69apZ"
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep only 'normal.' and 'back.' outcomes\n",
    "filtered_df = pd_data_frame[pd_data_frame['outcome'].isin(['normal.', 'back.'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.936522Z",
     "start_time": "2025-08-18T22:33:01.908309Z"
    },
    "id": "v8NO_C3d8ox2"
   },
   "outputs": [],
   "source": [
    "list_nominal_features = [\"flag\", \"protocol_type\", \"service\"]\n",
    "\n",
    "# Apply one-hot encoding to the nominal features\n",
    "df_encoded = pd.get_dummies(filtered_df, columns=list_nominal_features)\n",
    "\n",
    "# Convert boolean columns (from one-hot encoding) to integers (0 or 1) in df_encoded\n",
    "for col in df_encoded.columns:\n",
    "    if df_encoded[col].dtype == 'bool':\n",
    "        df_encoded[col] = df_encoded[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.975131Z",
     "start_time": "2025-08-18T22:33:01.939965Z"
    },
    "id": "DUfTputwfB4Z"
   },
   "outputs": [],
   "source": [
    "df_attacks = df_encoded[df_encoded['outcome'] == 'back.'].copy()\n",
    "df_no_attacks = df_encoded[df_encoded['outcome'] == 'normal.'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt8L5f-a9YbX"
   },
   "source": [
    "# Parte 1 An√°lisis Descriptivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a An√°lisis de momentos estad√≠sticos\n",
    "\n",
    "- Media\n",
    "- Desviaci√≥n est√°ndar\n",
    "- Inclinaci√≥n\n",
    "- Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.994762Z",
     "start_time": "2025-08-18T22:33:01.978965Z"
    },
    "id": "WtrwKuOi9fXs"
   },
   "outputs": [],
   "source": [
    "df_attacks_without_outcome = df_attacks.drop('outcome', axis=1)\n",
    "df_normal_without_outcome = df_no_attacks.drop('outcome', axis=1)\n",
    "\n",
    "attack_without_outcomes_column_names = df_attacks_without_outcome.columns\n",
    "attack_tensor = torch.tensor(df_attacks_without_outcome.values, dtype=torch.float32)\n",
    "\n",
    "normal_without_outcomes_column_names = df_normal_without_outcome.columns\n",
    "no_attack_tensor = torch.tensor(df_normal_without_outcome.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.001535Z",
     "start_time": "2025-08-18T22:33:01.999162Z"
    },
    "id": "qeZhNvCnaSAh"
   },
   "outputs": [],
   "source": [
    "def calculate_moments(dataset_tensor, feature_names):\n",
    "    means = torch.mean(dataset_tensor, dim=0)\n",
    "    stds = torch.std(dataset_tensor, dim=0)\n",
    "\n",
    "    z = (dataset_tensor - means) / stds\n",
    "    z = torch.where(torch.isfinite(z), z, torch.zeros_like(z))\n",
    "\n",
    "    skews = torch.mean(z ** 3, dim=0)\n",
    "    kurtosis = torch.mean(z ** 4, dim=0) - 3\n",
    "\n",
    "    stats_df = pd.DataFrame({\n",
    "        \"Media\": means.numpy(),\n",
    "        \"Desviaci√≥n Est√°ndar\": stds.numpy(),\n",
    "        \"Inclinaci√≥n\": skews.numpy(),\n",
    "        \"Kurtosis\": kurtosis.numpy()\n",
    "    }, index=feature_names)\n",
    "\n",
    "    display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br59lN0g-12I"
   },
   "source": [
    "### Momentos Estad√≠sticos para Datos de Ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.022339Z",
     "start_time": "2025-08-18T22:33:02.008223Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpaW2mTppxL3",
    "outputId": "9b7d9c90-d95b-4a48-9db7-45dcb427d772"
   },
   "outputs": [],
   "source": [
    "calculate_moments(attack_tensor, attack_without_outcomes_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.048588Z",
     "start_time": "2025-08-18T22:33:02.039497Z"
    }
   },
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(index=df_attacks_without_outcome.columns)\n",
    "stats_df[\"Mean\"] = df_attacks_without_outcome.mean()\n",
    "stats_df[\"Std\"] = df_attacks_without_outcome.std()\n",
    "stats_df[\"Skewness\"] = df_attacks_without_outcome.skew()\n",
    "stats_df[\"Kurtosis\"] = df_attacks_without_outcome.kurt()\n",
    "\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChrLjL87-7qG"
   },
   "source": [
    "### Momentos Estad√≠sticos para Paquetes Normales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.135247Z",
     "start_time": "2025-08-18T22:33:02.100276Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nss70vsjpzTl",
    "outputId": "d656773b-816a-4dd7-ed63-437211710d39"
   },
   "outputs": [],
   "source": [
    "calculate_moments(no_attack_tensor, normal_without_outcomes_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.236445Z",
     "start_time": "2025-08-18T22:33:02.173405Z"
    },
    "id": "5UgR0YsvtOPD"
   },
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(index=df_normal_without_outcome.columns)\n",
    "stats_df[\"Mean\"] = df_normal_without_outcome.mean()\n",
    "stats_df[\"Std\"] = df_normal_without_outcome.std()\n",
    "stats_df[\"Skewness\"] = df_normal_without_outcome.skew()\n",
    "stats_df[\"Kurtosis\"] = df_normal_without_outcome.kurt()\n",
    "\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Histogramas y Distancia Jensen Shannon\n",
    "\n",
    "### Histogramas para datos de ataque backdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.273989Z",
     "start_time": "2025-08-18T22:33:02.269760Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_feature_histogram_and_calculate_jensen_shannon(df_normal, df_backdoor, feature_names, bins=30):\n",
    "    js_distances = []\n",
    "\n",
    "    for feat in feature_names:\n",
    "        normal_df_values = df_normal[feat].values\n",
    "        backdoor_df_values = df_backdoor[feat].values\n",
    "\n",
    "        limits_histogram = (min(normal_df_values.min(), backdoor_df_values.min()),\n",
    "                            max(normal_df_values.max(), backdoor_df_values.max()))\n",
    "\n",
    "        hist_normal, _ = np.histogram(normal_df_values, bins=bins, range=limits_histogram)\n",
    "        hist_backdoor, _ = np.histogram(backdoor_df_values, bins=bins, range=limits_histogram)\n",
    "\n",
    "        hist_normal = hist_normal / hist_normal.sum()\n",
    "        hist_backdoor = hist_backdoor / hist_backdoor.sum()\n",
    "\n",
    "        jsd = jensenshannon(hist_normal, hist_backdoor)\n",
    "        js_distances.append(jsd)\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(normal_df_values, bins=bins, range=limits_histogram, alpha=0.5, label=\"Normal\", color='blue',\n",
    "                 density=True)\n",
    "        plt.hist(backdoor_df_values, bins=bins, range=limits_histogram, alpha=0.5, label=\"Backdoor\", color='red',\n",
    "                 density=True)\n",
    "        plt.title(f\"Histograma de {feat} (Distancia JS: {jsd:.4f})\")\n",
    "        plt.xlabel(\"Valor\")\n",
    "        plt.ylabel(\"Densidad\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    js_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"DistanciaJS\": js_distances\n",
    "    }).sort_values(by=\"DistanciaJS\", ascending=False)\n",
    "\n",
    "    display(js_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.656762Z",
     "start_time": "2025-08-18T22:33:02.318103Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_feature_histogram_and_calculate_jensen_shannon(df_normal_without_outcome, df_attacks_without_outcome,\n",
    "                                                        attack_without_outcomes_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 Implementaci√≥n de la clasificaci√≥n multi-clase con √°rboles de decisi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaci√≥n final de las clases NodeCart y Cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.727541Z",
     "start_time": "2025-08-18T22:33:07.718528Z"
    }
   },
   "outputs": [],
   "source": [
    "class NodeCart:\n",
    "    def __init__(self, num_classes=2, ref_cart=None, current_depth=0):\n",
    "        \"\"\"\n",
    "        Create the node attributes\n",
    "        param num_classes: K number of classes to classify\n",
    "        param ref_cart: reference to the tree containing the node\n",
    "        param current_depth: current depth of the node in the tree\n",
    "        \"\"\"\n",
    "        self.ref_cart = ref_cart\n",
    "        self.threshold_value = 0\n",
    "        self.feature_num = 0\n",
    "        self.node_right = None\n",
    "        self.node_left = None\n",
    "        self.data_torch_partition = None\n",
    "        self.gini = 0\n",
    "        self.dominant_class = None\n",
    "        self.accuracy_dominant_class = None\n",
    "        self.num_classes = num_classes\n",
    "        self.current_depth = current_depth\n",
    "\n",
    "    def to_xml(self, current_str=\"\"):\n",
    "        \"\"\"\n",
    "        Recursive function to write the node content to a xml formatted string\n",
    "        param current_str : the xml content so far in the whole tree\n",
    "        return the string with the node content\n",
    "        \"\"\"\n",
    "        str_node = \"<node><thresh>\" + str(self.threshold_value) + \"</thresh>\" + \"<feature>\" + str(\n",
    "            self.feature_num) + \"</feature><depth>\" + str(self.current_depth) + \"</depth>\"\n",
    "        str_node += \"<gini>\" + str(self.gini) + \"</gini>\"\n",
    "        if self.node_right is not None:\n",
    "            str_left = self.node_right.to_xml(current_str)\n",
    "            str_node += str_left\n",
    "        if self.node_left is not None:\n",
    "            str_right = self.node_left.to_xml(current_str)\n",
    "            str_node += str_right\n",
    "\n",
    "        if self.is_leaf():\n",
    "            str_node += \"<dominant_class>\" + str(self.dominant_class) + \"</dominant_class><acc_dominant_class>\" + str(\n",
    "                self.accuracy_dominant_class) + \"</acc_dominant_class>\"\n",
    "        str_node += \"</node>\"\n",
    "        return str_node\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Checks whether the node is a leaf\n",
    "        \"\"\"\n",
    "        return self.node_left is None and self.node_right is None\n",
    "\n",
    "    def create_with_children(self, data_torch, current_depth, min_gini=0.000001):\n",
    "        \"\"\"\n",
    "        Creates a node by selecting the best feature and threshold, and if needed, creating its children\n",
    "        param data_torch: dataset with the current partition to deal with in the node\n",
    "        param current_depth: depth counter for the node\n",
    "        param min_gini: hyperparameter selected by the user defining the minimum tolerated Gini coefficient for a  node\n",
    "        return the list of selected features so far\n",
    "        \"\"\"\n",
    "        labels = data_torch[:, -1].long()\n",
    "\n",
    "        self.dominant_class = torch.mode(labels)[0].item()\n",
    "        self.gini = self.calculate_gini(labels, self.num_classes)\n",
    "\n",
    "        list_selected_features = []\n",
    "\n",
    "        if (current_depth >= self.ref_cart.get_max_depth() or\n",
    "                data_torch.shape[0] <= self.ref_cart.get_min_observations() or\n",
    "                self.gini <= min_gini):\n",
    "            return list_selected_features\n",
    "\n",
    "        threshold, feature_idx, min_gini_split = self.select_best_feature_and_thresh(data_torch, self.num_classes)\n",
    "\n",
    "        if feature_idx is None or min_gini_split >= self.gini:\n",
    "            return list_selected_features\n",
    "\n",
    "        self.feature_num = feature_idx\n",
    "        self.threshold_value = threshold\n",
    "\n",
    "        list_selected_features.append(feature_idx)\n",
    "\n",
    "        features = data_torch[:, :-1]\n",
    "        feature_values = features[:, feature_idx]\n",
    "\n",
    "        left_mask = feature_values < threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_data = data_torch[left_mask]\n",
    "        right_data = data_torch[right_mask]\n",
    "\n",
    "        if left_data.shape[0] > 0:\n",
    "            self.node_left = NodeCart(self.num_classes, self.ref_cart, current_depth + 1)\n",
    "            left_features = self.node_left.create_with_children(left_data, current_depth + 1, min_gini)\n",
    "            list_selected_features.extend(left_features)\n",
    "\n",
    "        if right_data.shape[0] > 0:\n",
    "            self.node_right = NodeCart(self.num_classes, self.ref_cart, current_depth + 1)\n",
    "            right_features = self.node_right.create_with_children(right_data, current_depth + 1, min_gini)\n",
    "            list_selected_features.extend(right_features)\n",
    "\n",
    "        return list_selected_features\n",
    "\n",
    "    def select_best_feature_and_thresh(self, data_torch, num_classes=2):\n",
    "        \"\"\"\n",
    "        Selects the best feature and threshold that minimizes the Gini coefficient\n",
    "        param data_torch: dataset partition to analyze\n",
    "        param num_classes: number of K classes to discriminate from\n",
    "        return min_thresh, min_feature, min_gini found for the dataset partition when\n",
    "        selecting the found feature and threshold\n",
    "        \"\"\"\n",
    "        features = data_torch[:, :-1]\n",
    "        labels = data_torch[:, -1].long()\n",
    "\n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "\n",
    "        for feature_idx in range(features.shape[1]):\n",
    "            feature_values = features[:, feature_idx]\n",
    "            unique_values = torch.unique(feature_values, sorted=True)\n",
    "\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "\n",
    "                left_mask = feature_values < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                left_labels = labels[left_mask]\n",
    "                right_labels = labels[right_mask]\n",
    "\n",
    "                weighted_gini = self.weighted_gini(left_labels, right_labels, num_classes)\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_feature = feature_idx\n",
    "                    best_thresh = threshold.item()\n",
    "\n",
    "        return best_thresh, best_feature, best_gini\n",
    "\n",
    "    def calculate_gini(self, data_partition_torch, num_classes=2):\n",
    "        \"\"\"\n",
    "        Calculates the Gini coefficient for a given partition with the given number of classes\n",
    "        param data_partition_torch: current dataset partition as a tensor\n",
    "        param num_classes: K number of classes to discriminate from\n",
    "        returns the calculated Gini coefficient\n",
    "        \"\"\"\n",
    "        if data_partition_torch.numel() == 0:\n",
    "            return 0.0\n",
    "\n",
    "        class_counts = torch.bincount(data_partition_torch, minlength=num_classes).float()\n",
    "        proportions = class_counts / class_counts.sum()\n",
    "        gini_score = 1.0 - torch.sum(proportions ** 2)\n",
    "        return gini_score.item()\n",
    "\n",
    "    def weighted_gini(self, left_side, right_side, num_classes=2):\n",
    "        n = left_side.numel() + right_side.numel()\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        gini_left = self.calculate_gini(left_side, num_classes)\n",
    "        gini_right = self.calculate_gini(right_side, num_classes)\n",
    "        return (left_side.numel() / n) * gini_left + (right_side.numel() / n) * gini_right\n",
    "\n",
    "    def evaluate_node(self, input_torch):\n",
    "        \"\"\"\n",
    "        Evaluates an input observation within the node.\n",
    "        If is not a leaf node, send it to the corresponding node\n",
    "        return predicted label\n",
    "        \"\"\"\n",
    "        feature_val_input = input_torch[self.feature_num]\n",
    "        if self.is_leaf():\n",
    "            return self.dominant_class\n",
    "        else:\n",
    "            if feature_val_input < self.threshold_value:\n",
    "                return self.node_left.evaluate_node(input_torch)\n",
    "            else:\n",
    "                return self.node_right.evaluate_node(input_torch)\n",
    "\n",
    "\n",
    "class CART:\n",
    "    def __init__(self, dataset_torch, max_cart_depth, min_observations=2):\n",
    "        \"\"\"\n",
    "        CART has only one root node\n",
    "        \"\"\"\n",
    "        #min observations per node\n",
    "        self.min_observations = min_observations\n",
    "        self.root = NodeCart(num_classes=2, ref_cart=self, current_depth=0)\n",
    "        self.max_cart_depth = max_cart_depth\n",
    "        self.list_selected_features = []\n",
    "\n",
    "    def get_root(self):\n",
    "        \"\"\"\n",
    "        Gets tree root\n",
    "        \"\"\"\n",
    "        return self.root\n",
    "\n",
    "    def get_min_observations(self):\n",
    "        \"\"\"\n",
    "        return min observations per node\n",
    "        \"\"\"\n",
    "        return self.min_observations\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        \"\"\"\n",
    "        Gets the selected max depth of the tree\n",
    "        \"\"\"\n",
    "        return self.max_cart_depth\n",
    "\n",
    "    def build_cart(self, data_torch):\n",
    "        \"\"\"\n",
    "        Build CART from root\n",
    "        \"\"\"\n",
    "        self.list_selected_features = self.root.create_with_children(data_torch, current_depth=0)\n",
    "\n",
    "    def to_xml(self, xml_file_name):\n",
    "        \"\"\"\n",
    "        write Xml file with tree content\n",
    "        \"\"\"\n",
    "        str_nodes = self.root.to_xml()\n",
    "        file = open(xml_file_name, \"w+\")\n",
    "        file.write(str_nodes)\n",
    "        file.close()\n",
    "        return str_nodes\n",
    "\n",
    "    def evaluate_input(self, input_torch):\n",
    "        \"\"\"\n",
    "        Evaluate a specific input in the tree and get the predicted class\n",
    "        \"\"\"\n",
    "        return self.root.evaluate_node(input_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Implementaci√≥n de calculo de Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a Pruebas unitarias para el calculo de Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.738082Z",
     "start_time": "2025-08-18T22:33:07.735111Z"
    }
   },
   "outputs": [],
   "source": [
    "node = NodeCart(num_classes=2)\n",
    "ones_tensor = torch.tensor([1, 1, 1, 1])\n",
    "gini = node.calculate_gini(ones_tensor, num_classes=2)\n",
    "assert gini == 0.0, f\"Expected 0.0, got {gini}\"\n",
    "print(\"Test 1 Gini ‚úÖ\")\n",
    "\n",
    "variable_tensor = torch.tensor([0, 1, 2, 3])\n",
    "gini = node.calculate_gini(variable_tensor, num_classes=4)\n",
    "assert gini == 0.75, f\"Expected 0.75, got {gini}\"\n",
    "print(\"Test 2 Gini ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Pruebas unitarias de select_best_feature_and_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.750224Z",
     "start_time": "2025-08-18T22:33:07.748962Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_select_best_feature_two_classes():\n",
    "    data = torch.tensor([\n",
    "        [1.0, 5.0, 0],\n",
    "        [2.0, 3.0, 0],\n",
    "        [3.0, 1.0, 1],\n",
    "        [4.0, 2.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    node = NodeCart(num_classes=2)\n",
    "    thresh, feature_idx, gini_score = node.select_best_feature_and_thresh(data, num_classes=2)\n",
    "\n",
    "    assert thresh is not None, \"Threshold inv√°lido\"\n",
    "    assert feature_idx is not None, \"Feature inv√°lido\"\n",
    "    assert 0 <= feature_idx < 2, f\"Feature debe estar entre 0-1, obtuvo {feature_idx}\"\n",
    "    assert 1.0 <= thresh <= 4.0, f\"Threshold debe estar dentro del rango esperado se obtuvo {thresh}\"\n",
    "    assert gini_score >= 0.0, f\"Gini debe ser >= 0 se obtuvo {gini_score}\"\n",
    "\n",
    "    print(f\"‚úÖ Test 1: Feature={feature_idx}, Threshold={thresh:.3f}, Gini={gini_score:.3f}\")\n",
    "\n",
    "\n",
    "def test_select_best_feature_single_class():\n",
    "    single_class_data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 3.0, 0],\n",
    "        [3.0, 1.0, 0],\n",
    "        [4.0, 4.0, 0]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    node = NodeCart(num_classes=2)\n",
    "    thresh, feature_idx, gini_score = node.select_best_feature_and_thresh(single_class_data, num_classes=2)\n",
    "\n",
    "    if thresh is not None and feature_idx is not None:\n",
    "        assert 0 <= feature_idx < 2, f\"Feature inv√°lido: {feature_idx}\"\n",
    "        assert gini_score >= 0.0, f\"Gini debe ser >= 0 se obtuvo {gini_score}\"\n",
    "        print(f\"‚úÖ Test 2: Feature={feature_idx}, Threshold={thresh:.3f}, Gini={gini_score:.3f}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Test 2: No se pudo encontrar un split\")\n",
    "\n",
    "\n",
    "print(\"Pruebas de select_best_feature_and_thresh:\")\n",
    "test_select_best_feature_two_classes()\n",
    "test_select_best_feature_single_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Pruebas unitarias de create_with_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_create_with_children_normal_splitting():\n",
    "    data = torch.tensor([\n",
    "        [1.0, 10.0, 0],\n",
    "        [1.5, 12.0, 0],\n",
    "        [5.0, 2.0, 1],\n",
    "        [6.0, 1.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=data, max_cart_depth=2, min_observations=1)\n",
    "    root_node = tree.get_root()\n",
    "\n",
    "    selected_features = root_node.create_with_children(data, current_depth=0)\n",
    "\n",
    "    assert isinstance(selected_features, list), \"Deber√≠a devolver una lista\"\n",
    "    assert len(selected_features) > 0, \"Deber√≠a contener al menos una caracter√≠stica\"\n",
    "    assert not root_node.is_leaf(), \"Root node no deber√≠a ser un leaf\"\n",
    "    assert root_node.feature_num is not None, \"Feature number no deber√≠a estar vaci√≥\"\n",
    "    assert root_node.threshold_value is not None, \"Threshold value no deber√≠a estar vaci√≥\"\n",
    "    assert root_node.feature_num >= 0, f\"Feature number deber√≠a ser mayor o igual a cero, se obtuvo {root_node.feature_num}\"\n",
    "\n",
    "    has_left_child = root_node.node_left is not None\n",
    "    has_right_child = root_node.node_right is not None\n",
    "    assert has_left_child or has_right_child, \"Deber√≠a tener al menos un hijo\"\n",
    "\n",
    "    for feature_idx in selected_features:\n",
    "        assert 0 <= feature_idx < 2, f\"Feature index {feature_idx} deber√≠a estar entre 0 y 1\"\n",
    "\n",
    "    print(f\"‚úÖ Test 1: Normal splitting - Selected features: {selected_features}, \"\n",
    "          f\"Root feature: {root_node.feature_num}, Threshold: {root_node.threshold_value:.3f} \"\n",
    "          f\"Root feature: {root_node.feature_num}, Threshold: {root_node.threshold_value:.3f}\")\n",
    "\n",
    "\n",
    "def test_create_with_children_min_gini_condition():\n",
    "    data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [1.1, 2.1, 0],\n",
    "        [1.2, 2.2, 0],\n",
    "        [1.3, 2.3, 0],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=data, max_cart_depth=3, min_observations=1)\n",
    "    root_node = tree.get_root()\n",
    "\n",
    "    selected_features = root_node.create_with_children(data, current_depth=0, min_gini=0.1)\n",
    "\n",
    "    assert isinstance(selected_features, list), \"Deber√≠a devolver una lista\"\n",
    "    assert len(selected_features) == 0, \"Deber√≠a devolver una lista vac√≠a\"\n",
    "    assert root_node.is_leaf(), \"Deber√≠a ser  una hoja\"\n",
    "    assert root_node.gini < 0.1, f\"Gibi deber√≠a ser menor a 0.1, se obtuvo {root_node.gini:.3f}\"\n",
    "    assert root_node.dominant_class == 0, \"Deber√≠a ser la clase dominante 0\"\n",
    "\n",
    "    print(f\"‚úÖ Test 2: Min Gini - Gini: {root_node.gini:.3f}, Dominant class: {root_node.dominant_class}\")\n",
    "\n",
    "\n",
    "print(\"Pruebas de create_with_children:\")\n",
    "test_create_with_children_normal_splitting()\n",
    "test_create_with_children_min_gini_condition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Implementaci√≥n de TestCart y unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cart(tree, testset_torch):\n",
    "    \"\"\"\n",
    "    Test a previously built CART\n",
    "    \"\"\"\n",
    "    if testset_torch.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    test_features = testset_torch[:, :-1]\n",
    "    true_labels = testset_torch[:, -1].long()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = testset_torch.shape[0]\n",
    "\n",
    "    for i in range(total_predictions):\n",
    "        current_observation = test_features[i]\n",
    "        predicted_label = tree.evaluate_input(current_observation)\n",
    "        true_label = true_labels[i].item()\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cart_perfect_predictions():\n",
    "    train_data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 3.0, 0],\n",
    "        [3.0, 1.0, 1],\n",
    "        [4.0, 2.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    test_data = torch.tensor([\n",
    "        [1.5, 2.5, 0],\n",
    "        [3.5, 1.5, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=train_data, max_cart_depth=2, min_observations=1)\n",
    "    tree.build_cart(train_data)\n",
    "\n",
    "    accuracy = test_cart(tree, test_data)\n",
    "\n",
    "    assert 0.0 <= accuracy <= 1.0, f\"Accuracy debe estar entre 0 y 1 se obtuvo {accuracy}\"\n",
    "    print(f\"‚úÖ Test 1: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "\n",
    "def test_cart_invalid_dataset():\n",
    "    train_data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 3.0, 1],\n",
    "        [3.0, 1.0, 0],\n",
    "        [4.0, 2.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=train_data, max_cart_depth=1, min_observations=1)\n",
    "    tree.build_cart(train_data)\n",
    "\n",
    "    empty_test = torch.empty((0, 3), dtype=torch.float32)\n",
    "    accuracy_empty = test_cart(tree, empty_test)\n",
    "    assert accuracy_empty == 0.0, f\"Deber√≠a dar accuracy 0.0 se obtuvo {accuracy_empty}\"\n",
    "\n",
    "    single_test = torch.tensor([[2.5, 1.5, 0]], dtype=torch.float32)\n",
    "    accuracy_single = test_cart(tree, single_test)\n",
    "    assert 0.0 <= accuracy_single <= 1.0, f\"Accuracy inv√°lida: {accuracy_single}\"\n",
    "    assert accuracy_single in [0.0, 1.0], \"Con una observaci√≥n, accuracy deber√≠a ser 0.0 o 1.0\"\n",
    "\n",
    "    print(f\"‚úÖ Test 2: Dataset vac√≠o = {accuracy_empty}, Una observaci√≥n = {accuracy_single}\")\n",
    "\n",
    "\n",
    "test_cart_perfect_predictions()\n",
    "test_cart_invalid_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluaci√≥n del CART\n",
    "\n",
    "## Implementaci√≥n de funciones generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_kd99_dataset_tensor():\n",
    "    complete_dataset = pd.concat([df_no_attacks, df_attacks], ignore_index=True)\n",
    "\n",
    "    class_mapping = {'normal.': 0, 'back.': 1}\n",
    "    complete_dataset['outcome'] = complete_dataset['outcome'].map(class_mapping)\n",
    "\n",
    "    dataset_tensor = torch.tensor(complete_dataset.values, dtype=torch.float32)\n",
    "\n",
    "    print(f\"Dataset completo creado:\")\n",
    "    print(f\"- Total de observaciones: {dataset_tensor.shape[0]}\")\n",
    "    print(f\"- N√∫mero de caracter√≠sticas: {dataset_tensor.shape[1] - 1}\")  # -1 por la etiqueta\n",
    "    print(f\"- Observaciones normales: {len(df_no_attacks)}\")\n",
    "    print(f\"- Observaciones de backdoor: {len(df_attacks)}\")\n",
    "    print(f\"- Distribuci√≥n de clases: {complete_dataset['outcome'].value_counts().to_dict()}\")\n",
    "\n",
    "    return dataset_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Tasa de aciertos y F1-score promedio de todas las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(y_true, y_pred, num_classes=2):\n",
    "    f1_scores = []\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        tp = torch.sum((y_true == class_id) & (y_pred == class_id)).float()\n",
    "        fp = torch.sum((y_true != class_id) & (y_pred == class_id)).float()\n",
    "        fn = torch.sum((y_true == class_id) & (y_pred != class_id)).float()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else torch.tensor(0.0)\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else torch.tensor(0.0)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else torch.tensor(0.0)\n",
    "        f1_scores.append(f1.item())\n",
    "\n",
    "        class_name = \"Normal\" if class_id == 0 else \"Backdoor\"\n",
    "        print(f\"üìä {class_name}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cart_complete(dataset_tensor, max_depth, min_observations=2):\n",
    "    print(f\"\\nüå≥ Evaluando CART con profundidad m√°xima = {max_depth}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    cart = CART(dataset_torch=dataset_tensor,\n",
    "                max_cart_depth=max_depth,\n",
    "                min_observations=min_observations)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    cart.build_cart(dataset_tensor)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    features = dataset_tensor[:, :-1]\n",
    "    true_labels = dataset_tensor[:, -1].long()\n",
    "\n",
    "    predicted_labels = []\n",
    "    for i in range(features.shape[0]):\n",
    "        prediction = cart.evaluate_input(features[i])\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    predicted_labels = torch.tensor(predicted_labels)\n",
    "    evaluation_time = time.time() - start_time\n",
    "\n",
    "    accuracy = test_cart(cart, dataset_tensor)\n",
    "\n",
    "    print(f\"üìä Detalle por clase:\")\n",
    "    f1_score = calculate_f1_score(true_labels, predicted_labels, num_classes=2)\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nüìä Resultados generales:\")\n",
    "    print(f\"   ‚Ä¢ Tasa de aciertos (Accuracy): {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ F1-Score promedio: {f1_score:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Tiempo de entrenamiento: {training_time:.4f} segundos\")\n",
    "    print(f\"   ‚Ä¢ Tiempo de evaluaci√≥n: {evaluation_time:.4f} segundos\")\n",
    "    print(f\"   ‚Ä¢ Caracter√≠sticas seleccionadas: {len(cart.list_selected_features)}\")\n",
    "    print(f\"   ‚Ä¢ Features utilizadas: {cart.list_selected_features}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1_score,\n",
    "        'training_time': training_time,\n",
    "        'evaluation_time': evaluation_time,\n",
    "        'selected_features': cart.list_selected_features,\n",
    "        'cart': cart\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARTE 3 - PUNTO 1: EVALUACI√ìN DEL CART\")\n",
    "print(\"Usando el mismo conjunto de datos para entrenamiento y prueba\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã Informaci√≥n de los dataframes originales:\")\n",
    "print(f\"   ‚Ä¢ df_no_attacks (normal): {len(df_no_attacks)} observaciones\")\n",
    "print(f\"   ‚Ä¢ df_attacks (backdoor): {len(df_attacks)} observaciones\")\n",
    "print(f\"   ‚Ä¢ Clases en df_no_attacks: {df_no_attacks['outcome'].unique()}\")\n",
    "print(f\"   ‚Ä¢ Clases en df_attacks: {df_attacks['outcome'].unique()}\")\n",
    "\n",
    "complete_dataset = get_cart_kd99_dataset_tensor()\n",
    "\n",
    "print(f\"\\nüìã Configuraci√≥n de evaluaci√≥n:\")\n",
    "print(f\"   ‚Ä¢ M√≠nimo 2 observaciones por hoja\")\n",
    "print(f\"   ‚Ä¢ Evaluaci√≥n con profundidades m√°ximas: 3 y 4\")\n",
    "\n",
    "# Evaluar con profundidad m√°xima = 3\n",
    "results_depth_3 = evaluate_cart_complete(complete_dataset, max_depth=3, min_observations=2)\n",
    "\n",
    "# Evaluar con profundidad m√°xima = 4  \n",
    "results_depth_4 = evaluate_cart_complete(complete_dataset, max_depth=4, min_observations=2)\n",
    "\n",
    "# Resumen comparativo\n",
    "print(f\"\\nüìà RESUMEN COMPARATIVO:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'M√©trica':<25} {'Profundidad 3':<15} {'Profundidad 4':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Accuracy':<25} {results_depth_3['accuracy']:<15.4f} {results_depth_4['accuracy']:<15.4f}\")\n",
    "print(f\"{'F1-Score':<25} {results_depth_3['f1_score']:<15.4f} {results_depth_4['f1_score']:<15.4f}\")\n",
    "print(\n",
    "    f\"{'Tiempo Entrenamiento (s)':<25} {results_depth_3['training_time']:<15.4f} {results_depth_4['training_time']:<15.4f}\")\n",
    "print(\n",
    "    f\"{'Tiempo Evaluaci√≥n (s)':<25} {results_depth_3['evaluation_time']:<15.4f} {results_depth_4['evaluation_time']:<15.4f}\")\n",
    "print(\n",
    "    f\"{'Features Seleccionadas':<25} {len(results_depth_3['selected_features']):<15} {len(results_depth_4['selected_features']):<15}\")\n",
    "\n",
    "print(f\"\\nüéØ CONCLUSIONES:\")\n",
    "mejor_accuracy = \"Profundidad 3\" if results_depth_3['accuracy'] > results_depth_4['accuracy'] else \"Profundidad 4\"\n",
    "mejor_f1 = \"Profundidad 3\" if results_depth_3['f1_score'] > results_depth_4['f1_score'] else \"Profundidad 4\"\n",
    "print(f\"   ‚Ä¢ Mejor Accuracy: {mejor_accuracy}\")\n",
    "print(f\"   ‚Ä¢ Mejor F1-Score: {mejor_f1}\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ El √°rbol con profundidad 4 utiliza {len(results_depth_4['selected_features']) - len(results_depth_3['selected_features'])} caracter√≠sticas adicionales\")\n",
    "\n",
    "# Guardar los √°rboles para an√°lisis posterior\n",
    "best_cart_depth_3 = results_depth_3['cart']\n",
    "best_cart_depth_4 = results_depth_4['cart']\n",
    "\n",
    "print(\"\\n‚úÖ Evaluaci√≥n del punto 1 completada exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Evaluaci√≥n del CART compleja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run_evaluation(dataset_tensor, max_depth, min_observations=2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        random.seed(random_state)\n",
    "\n",
    "    data_np = dataset_tensor.numpy()\n",
    "    X = data_np[:, :-1]\n",
    "    y = data_np[:, -1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    train_tensor = torch.tensor(np.column_stack([X_train, y_train]), dtype=torch.float32)\n",
    "    test_tensor = torch.tensor(np.column_stack([X_test, y_test]), dtype=torch.float32)\n",
    "\n",
    "    cart = CART(dataset_torch=train_tensor,\n",
    "                max_cart_depth=max_depth,\n",
    "                min_observations=min_observations)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    cart.build_cart(train_tensor)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    accuracy = test_cart(cart, test_tensor)\n",
    "\n",
    "    test_features = test_tensor[:, :-1]\n",
    "    true_labels = test_tensor[:, -1].long()\n",
    "\n",
    "    predicted_labels = []\n",
    "    for i in range(test_features.shape[0]):\n",
    "        prediction = cart.evaluate_input(test_features[i])\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    predicted_labels = torch.tensor(predicted_labels)\n",
    "    evaluation_time = time.time() - start_time\n",
    "\n",
    "    f1_score = calculate_f1_score_silent(true_labels, predicted_labels, num_classes=2)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1_score,\n",
    "        'training_time': training_time,\n",
    "        'evaluation_time': evaluation_time,\n",
    "        'cart': cart,\n",
    "        'train_size': train_tensor.shape[0],\n",
    "        'test_size': test_tensor.shape[0]\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_f1_score_silent(y_true, y_pred, num_classes=2):\n",
    "    f1_scores = []\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        tp = torch.sum((y_true == class_id) & (y_pred == class_id)).float()\n",
    "        fp = torch.sum((y_true != class_id) & (y_pred == class_id)).float()\n",
    "        fn = torch.sum((y_true == class_id) & (y_pred != class_id)).float()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else torch.tensor(0.0)\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else torch.tensor(0.0)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else torch.tensor(0.0)\n",
    "        f1_scores.append(f1.item())\n",
    "\n",
    "    return sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "\n",
    "def evaluate_multiple_runs(dataset_tensor, max_depth, n_runs=10, min_observations=2):\n",
    "    print(f\"\\nüîÑ Ejecutando {n_runs} corridas con profundidad m√°xima = {max_depth}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    results = []\n",
    "    best_f1_idx = 0\n",
    "    best_f1_score = -1\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"   Corrida {run + 1}/{n_runs}...\", end=\" \")\n",
    "\n",
    "        result = single_run_evaluation(\n",
    "            dataset_tensor,\n",
    "            max_depth,\n",
    "            min_observations,\n",
    "            random_state=42 + run\n",
    "        )\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        if result['f1_score'] > best_f1_score:\n",
    "            best_f1_score = result['f1_score']\n",
    "            best_f1_idx = run\n",
    "\n",
    "        print(f\"‚úÖ Acc: {result['accuracy']:.3f}, F1: {result['f1_score']:.3f}\")\n",
    "\n",
    "    accuracies = [r['accuracy'] for r in results]\n",
    "    f1_scores = [r['f1_score'] for r in results]\n",
    "    train_times = [r['training_time'] for r in results]\n",
    "    eval_times = [r['evaluation_time'] for r in results]\n",
    "\n",
    "    stats = {\n",
    "        'accuracy_mean': np.mean(accuracies),\n",
    "        'accuracy_std': np.std(accuracies),\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores),\n",
    "        'train_time_mean': np.mean(train_times),\n",
    "        'train_time_std': np.std(train_times),\n",
    "        'eval_time_mean': np.mean(eval_times),\n",
    "        'eval_time_std': np.std(eval_times),\n",
    "        'best_run_idx': best_f1_idx,\n",
    "        'best_cart': results[best_f1_idx]['cart'],\n",
    "        'all_results': results\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def display_results_table(stats_depth_2, stats_depth_3):\n",
    "    print(f\"\\nüìä TABLA DE RESULTADOS (Promedio ¬± Desviaci√≥n Est√°ndar)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'M√©trica':<25} {'Profundidad 2':<25} {'Profundidad 3':<25}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"{'Accuracy':<25} \"\n",
    "          f\"{stats_depth_2['accuracy_mean']:.4f} ¬± {stats_depth_2['accuracy_std']:.4f} \"\n",
    "          f\"{stats_depth_3['accuracy_mean']:.4f} ¬± {stats_depth_3['accuracy_std']:.4f}\")\n",
    "\n",
    "    print(f\"{'F1-Score':<25} \"\n",
    "          f\"{stats_depth_2['f1_mean']:.4f} ¬± {stats_depth_2['f1_std']:.4f:} \"\n",
    "          f\"{stats_depth_3['f1_mean']:.4f} ¬± {stats_depth_3['f1_std']:.4f}\")\n",
    "\n",
    "    print(f\"{'Tiempo Entren. (s)':<25} \"\n",
    "          f\"{stats_depth_2['train_time_mean']:.4f} ¬± {stats_depth_2['train_time_std']:.4f} \"\n",
    "          f\"{stats_depth_3['train_time_mean']:.4f} ¬± {stats_depth_3['train_time_std']:.4f}\")\n",
    "\n",
    "    print(f\"{'Tiempo Eval. (s)':<25} \"\n",
    "          f\"{stats_depth_2['eval_time_mean']:.4f} ¬± {stats_depth_2['eval_time_std']:.4f:} \"\n",
    "          f\"{stats_depth_3['eval_time_mean']:.4f} ¬± {stats_depth_3['eval_time_std']:.4f}\")\n",
    "\n",
    "\n",
    "def generate_tree_visualization(cart, filename):\n",
    "    xml_content = cart.to_xml(filename)\n",
    "    print(f\"   üìÑ √Årbol guardado en: {filename}\")\n",
    "    print(f\"   üåê Para visualizar: abrir {filename} en navegador web\")\n",
    "    return xml_content\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARTE 3 - PUNTO 2: EVALUACI√ìN CON 10 PARTICIONES ALEATORIAS (70%-30%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìã Configuraci√≥n:\")\n",
    "print(f\"   ‚Ä¢ Dataset: {complete_dataset.shape[0]} observaciones\")\n",
    "print(f\"   ‚Ä¢ Particiones: 10 corridas aleatorias\")\n",
    "print(f\"   ‚Ä¢ Divisi√≥n: 70% entrenamiento, 30% prueba\")\n",
    "print(f\"   ‚Ä¢ Profundidades a evaluar: 2 y 3\")\n",
    "print(f\"   ‚Ä¢ M√≠nimo 2 observaciones por hoja\")\n",
    "\n",
    "stats_depth_2 = evaluate_multiple_runs(complete_dataset, max_depth=2, n_runs=10, min_observations=2)\n",
    "stats_depth_3 = evaluate_multiple_runs(complete_dataset, max_depth=3, n_runs=10, min_observations=2)\n",
    "\n",
    "display_results_table(stats_depth_2, stats_depth_3)\n",
    "\n",
    "print(f\"\\nüèÜ MEJORES CORRIDAS:\")\n",
    "print(f\"   ‚Ä¢ Profundidad 2: Corrida {stats_depth_2['best_run_idx'] + 1} (F1-Score: {stats_depth_2['f1_mean']:.4f})\")\n",
    "print(f\"   ‚Ä¢ Profundidad 3: Corrida {stats_depth_3['best_run_idx'] + 1} (F1-Score: {stats_depth_3['f1_mean']:.4f})\")\n",
    "\n",
    "best_overall = stats_depth_2 if stats_depth_2['f1_mean'] > stats_depth_3['f1_mean'] else stats_depth_3\n",
    "best_depth = 2 if stats_depth_2['f1_mean'] > stats_depth_3['f1_mean'] else 3\n",
    "\n",
    "print(f\"\\nüå≥ GENERANDO VISUALIZACI√ìN DEL MEJOR √ÅRBOL:\")\n",
    "print(f\"   ‚Ä¢ Mejor profundidad general: {best_depth}\")\n",
    "\n",
    "xml_file_depth_2 = f\"mejor_arbol_profundidad_2.xml\"\n",
    "xml_file_depth_3 = f\"mejor_arbol_profundidad_3.xml\"\n",
    "\n",
    "generate_tree_visualization(stats_depth_2['best_cart'], xml_file_depth_2)\n",
    "generate_tree_visualization(stats_depth_3['best_cart'], xml_file_depth_3)\n",
    "\n",
    "print(f\"\\nüìà AN√ÅLISIS COMPARATIVO:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if stats_depth_3['accuracy_mean'] > stats_depth_2['accuracy_mean']:\n",
    "    print(\n",
    "        f\"   ‚Ä¢ ‚úÖ Profundidad 3 tiene mejor accuracy promedio (+{stats_depth_3['accuracy_mean'] - stats_depth_2['accuracy_mean']:.4f})\")\n",
    "else:\n",
    "    print(\n",
    "        f\"   ‚Ä¢ ‚úÖ Profundidad 2 tiene mejor accuracy promedio (+{stats_depth_2['accuracy_mean'] - stats_depth_3['accuracy_mean']:.4f})\")\n",
    "\n",
    "if stats_depth_3['f1_mean'] > stats_depth_2['f1_mean']:\n",
    "    print(\n",
    "        f\"   ‚Ä¢ ‚úÖ Profundidad 3 tiene mejor F1-Score promedio (+{stats_depth_3['f1_mean'] - stats_depth_2['f1_mean']:.4f})\")\n",
    "else:\n",
    "    print(\n",
    "        f\"   ‚Ä¢ ‚úÖ Profundidad 2 tiene mejor F1-Score promedio (+{stats_depth_2['f1_mean'] - stats_depth_3['f1_mean']:.4f})\")\n",
    "\n",
    "print(\n",
    "    f\"   ‚Ä¢ Variabilidad Accuracy - Prof. 2: {stats_depth_2['accuracy_std']:.4f}, Prof. 3: {stats_depth_3['accuracy_std']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Variabilidad F1-Score - Prof. 2: {stats_depth_2['f1_std']:.4f}, Prof. 3: {stats_depth_3['f1_std']:.4f}\")\n",
    "\n",
    "more_stable_acc = \"Profundidad 2\" if stats_depth_2['accuracy_std'] < stats_depth_3['accuracy_std'] else \"Profundidad 3\"\n",
    "more_stable_f1 = \"Profundidad 2\" if stats_depth_2['f1_std'] < stats_depth_3['f1_std'] else \"Profundidad 3\"\n",
    "\n",
    "print(f\"   ‚Ä¢ M√°s estable en Accuracy: {more_stable_acc}\")\n",
    "print(f\"   ‚Ä¢ M√°s estable en F1-Score: {more_stable_f1}\")\n",
    "\n",
    "print(\n",
    "    f\"   ‚Ä¢ Tiempo entrenamiento - Prof. 2: {stats_depth_2['train_time_mean']:.4f}s, Prof. 3: {stats_depth_3['train_time_mean']:.4f}s\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Tiempo evaluaci√≥n - Prof. 2: {stats_depth_2['eval_time_mean']:.4f}s, Prof. 3: {stats_depth_3['eval_time_mean']:.4f}s\")\n",
    "\n",
    "print(f\"\\nüí° PROPUESTA DE OPTIMIZACI√ìN CON JENSEN-SHANNON:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"   La distancia Jensen-Shannon calculada en la Parte 1 podr√≠a usarse para:\")\n",
    "print(f\"   1. Pre-seleccionar caracter√≠sticas m√°s discriminativas antes del entrenamiento\")\n",
    "print(f\"   2. Reducir el espacio de b√∫squeda en select_best_feature_and_thresh\")\n",
    "print(f\"   3. Priorizar splits en caracter√≠sticas con mayor separabilidad entre clases\")\n",
    "print(f\"   4. Implementar poda temprana basada en distancias JS bajas\")\n",
    "print(f\"   5. Usar JS como criterio alternativo al Gini para splits m√°s informativos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
