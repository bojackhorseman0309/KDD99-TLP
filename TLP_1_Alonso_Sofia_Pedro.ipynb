{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBIp6DT8si_1"
   },
   "source": [
    "# Trabajo Práctico 1\n",
    "\n",
    "Estudiantes:\n",
    "\n",
    "- Alonso Araya Calvo\n",
    "- Pedro Soto\n",
    "- Sofia Oviedo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLfMPjjz6Tqf"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:32:56.720644Z",
     "start_time": "2025-08-18T22:32:56.718567Z"
    },
    "id": "7lh6x4GjDbzO"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TqpxBWGYveZ"
   },
   "source": [
    "# Cargando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.627208Z",
     "start_time": "2025-08-18T22:32:56.739590Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pn_P8kZ6a4G",
    "outputId": "5e66e87d-e21b-467a-d7b5-7469ca29a80f"
   },
   "outputs": [],
   "source": [
    "#tomado de https://www.kaggle.com/code/wailinnoo/intrusion-detection-system-using-kdd99-dataset\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "try:\n",
    "    path = get_file('kddcup.data_10_percent.gz',\n",
    "                    origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
    "except:\n",
    "    print('Error downloading')\n",
    "    raise\n",
    "\n",
    "print(path)\n",
    "\n",
    "# This file is a CSV, just no CSV extension or headers\n",
    "# Download from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "pd_data_frame = pd.read_csv(path, header=None)\n",
    "\n",
    "# The CSV file has no column heads, so add them\n",
    "pd_data_frame.columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'outcome'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyxCEYNuY0Dc"
   },
   "source": [
    "# Limpieza del dataset y generación de subset del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.883181Z",
     "start_time": "2025-08-18T22:33:01.641539Z"
    },
    "id": "Gc6X0vQv5ad6"
   },
   "outputs": [],
   "source": [
    "# For now, just drop NA's (rows with missing values), in case there are\n",
    "pd_data_frame.dropna(inplace=True, axis=1)\n",
    "\n",
    "# Checking for DUPLICATE values\n",
    "pd_data_frame.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.903294Z",
     "start_time": "2025-08-18T22:33:01.887249Z"
    },
    "id": "G_m74GC69apZ"
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep only 'normal.' and 'back.' outcomes\n",
    "filtered_df = pd_data_frame[pd_data_frame['outcome'].isin(['normal.', 'back.'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.936522Z",
     "start_time": "2025-08-18T22:33:01.908309Z"
    },
    "id": "v8NO_C3d8ox2"
   },
   "outputs": [],
   "source": [
    "list_nominal_features = [\"flag\", \"protocol_type\", \"service\"]\n",
    "\n",
    "# Apply one-hot encoding to the nominal features\n",
    "df_encoded = pd.get_dummies(filtered_df, columns=list_nominal_features)\n",
    "\n",
    "# Convert boolean columns (from one-hot encoding) to integers (0 or 1) in df_encoded\n",
    "for col in df_encoded.columns:\n",
    "    if df_encoded[col].dtype == 'bool':\n",
    "        df_encoded[col] = df_encoded[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.975131Z",
     "start_time": "2025-08-18T22:33:01.939965Z"
    },
    "id": "DUfTputwfB4Z"
   },
   "outputs": [],
   "source": [
    "df_attacks = df_encoded[df_encoded['outcome'] == 'back.'].copy()\n",
    "df_no_attacks = df_encoded[df_encoded['outcome'] == 'normal.'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt8L5f-a9YbX"
   },
   "source": [
    "# Parte 1 Análisis Descriptivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Análisis de momentos estadísticos\n",
    "\n",
    "- Media\n",
    "- Desviación estándar\n",
    "- Inclinación\n",
    "- Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:01.994762Z",
     "start_time": "2025-08-18T22:33:01.978965Z"
    },
    "id": "WtrwKuOi9fXs"
   },
   "outputs": [],
   "source": [
    "df_attacks_without_outcome = df_attacks.drop('outcome', axis=1)\n",
    "df_normal_without_outcome = df_no_attacks.drop('outcome', axis=1)\n",
    "\n",
    "attack_without_outcomes_column_names = df_attacks_without_outcome.columns\n",
    "attack_tensor = torch.tensor(df_attacks_without_outcome.values, dtype=torch.float32)\n",
    "\n",
    "normal_without_outcomes_column_names = df_normal_without_outcome.columns\n",
    "no_attack_tensor = torch.tensor(df_normal_without_outcome.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.001535Z",
     "start_time": "2025-08-18T22:33:01.999162Z"
    },
    "id": "qeZhNvCnaSAh"
   },
   "outputs": [],
   "source": [
    "def calculate_moments(dataset_tensor, feature_names):\n",
    "    means = torch.mean(dataset_tensor, dim=0)\n",
    "    stds = torch.std(dataset_tensor, dim=0)\n",
    "\n",
    "    z = (dataset_tensor - means) / stds\n",
    "    z = torch.where(torch.isfinite(z), z, torch.zeros_like(z))\n",
    "\n",
    "    skews = torch.mean(z ** 3, dim=0)\n",
    "    kurtosis = torch.mean(z ** 4, dim=0) - 3\n",
    "\n",
    "    stats_df = pd.DataFrame({\n",
    "        \"Media\": means.numpy(),\n",
    "        \"Desviación Estándar\": stds.numpy(),\n",
    "        \"Inclinación\": skews.numpy(),\n",
    "        \"Kurtosis\": kurtosis.numpy()\n",
    "    }, index=feature_names)\n",
    "\n",
    "    display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br59lN0g-12I"
   },
   "source": [
    "### Momentos Estadísticos para Datos de Ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.022339Z",
     "start_time": "2025-08-18T22:33:02.008223Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpaW2mTppxL3",
    "outputId": "9b7d9c90-d95b-4a48-9db7-45dcb427d772"
   },
   "outputs": [],
   "source": [
    "calculate_moments(attack_tensor, attack_without_outcomes_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.048588Z",
     "start_time": "2025-08-18T22:33:02.039497Z"
    }
   },
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(index=df_attacks_without_outcome.columns)\n",
    "stats_df[\"Mean\"] = df_attacks_without_outcome.mean()\n",
    "stats_df[\"Std\"] = df_attacks_without_outcome.std()\n",
    "stats_df[\"Skewness\"] = df_attacks_without_outcome.skew()\n",
    "stats_df[\"Kurtosis\"] = df_attacks_without_outcome.kurt()\n",
    "\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChrLjL87-7qG"
   },
   "source": [
    "### Momentos Estadísticos para Paquetes Normales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.135247Z",
     "start_time": "2025-08-18T22:33:02.100276Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nss70vsjpzTl",
    "outputId": "d656773b-816a-4dd7-ed63-437211710d39"
   },
   "outputs": [],
   "source": [
    "calculate_moments(no_attack_tensor, normal_without_outcomes_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.236445Z",
     "start_time": "2025-08-18T22:33:02.173405Z"
    },
    "id": "5UgR0YsvtOPD"
   },
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(index=df_normal_without_outcome.columns)\n",
    "stats_df[\"Mean\"] = df_normal_without_outcome.mean()\n",
    "stats_df[\"Std\"] = df_normal_without_outcome.std()\n",
    "stats_df[\"Skewness\"] = df_normal_without_outcome.skew()\n",
    "stats_df[\"Kurtosis\"] = df_normal_without_outcome.kurt()\n",
    "\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Histogramas y Distancia Jensen Shannon\n",
    "\n",
    "### Histogramas para datos de ataque backdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:02.273989Z",
     "start_time": "2025-08-18T22:33:02.269760Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_feature_histogram_and_calculate_jensen_shannon(df_normal, df_backdoor, feature_names, bins=30):\n",
    "    js_distances = []\n",
    "\n",
    "    for feat in feature_names:\n",
    "        normal_df_values = df_normal[feat].values\n",
    "        backdoor_df_values = df_backdoor[feat].values\n",
    "\n",
    "        limits_histogram = (min(normal_df_values.min(), backdoor_df_values.min()),\n",
    "                            max(normal_df_values.max(), backdoor_df_values.max()))\n",
    "\n",
    "        hist_normal, _ = np.histogram(normal_df_values, bins=bins, range=limits_histogram)\n",
    "        hist_backdoor, _ = np.histogram(backdoor_df_values, bins=bins, range=limits_histogram)\n",
    "\n",
    "        hist_normal = hist_normal / hist_normal.sum()\n",
    "        hist_backdoor = hist_backdoor / hist_backdoor.sum()\n",
    "\n",
    "        jsd = jensenshannon(hist_normal, hist_backdoor)\n",
    "        js_distances.append(jsd)\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(normal_df_values, bins=bins, range=limits_histogram, alpha=0.5, label=\"Normal\", color='blue',\n",
    "                 density=True)\n",
    "        plt.hist(backdoor_df_values, bins=bins, range=limits_histogram, alpha=0.5, label=\"Backdoor\", color='red',\n",
    "                 density=True)\n",
    "        plt.title(f\"Histograma de {feat} (Distancia JS: {jsd:.4f})\")\n",
    "        plt.xlabel(\"Valor\")\n",
    "        plt.ylabel(\"Densidad\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    js_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"DistanciaJS\": js_distances\n",
    "    }).sort_values(by=\"DistanciaJS\", ascending=False)\n",
    "\n",
    "    display(js_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.656762Z",
     "start_time": "2025-08-18T22:33:02.318103Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_feature_histogram_and_calculate_jensen_shannon(df_normal_without_outcome, df_attacks_without_outcome,\n",
    "                                                        attack_without_outcomes_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 Implementación de la clasificación multi-clase con árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación final de las clases NodeCart y Cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.727541Z",
     "start_time": "2025-08-18T22:33:07.718528Z"
    }
   },
   "outputs": [],
   "source": [
    "class NodeCart:\n",
    "    def __init__(self, num_classes=2, ref_cart=None, current_depth=0):\n",
    "        \"\"\"\n",
    "        Create the node attributes\n",
    "        param num_classes: K number of classes to classify\n",
    "        param ref_cart: reference to the tree containing the node\n",
    "        param current_depth: current depth of the node in the tree\n",
    "        \"\"\"\n",
    "        self.ref_cart = ref_cart\n",
    "        self.threshold_value = 0\n",
    "        self.feature_num = 0\n",
    "        self.node_right = None\n",
    "        self.node_left = None\n",
    "        self.data_torch_partition = None\n",
    "        self.gini = 0\n",
    "        self.dominant_class = None\n",
    "        self.accuracy_dominant_class = None\n",
    "        self.num_classes = num_classes\n",
    "        self.current_depth = current_depth\n",
    "\n",
    "    def to_xml(self, current_str=\"\"):\n",
    "        \"\"\"\n",
    "        Recursive function to write the node content to a xml formatted string\n",
    "        param current_str : the xml content so far in the whole tree\n",
    "        return the string with the node content\n",
    "        \"\"\"\n",
    "        str_node = \"<node><thresh>\" + str(self.threshold_value) + \"</thresh>\" + \"<feature>\" + str(\n",
    "            self.feature_num) + \"</feature><depth>\" + str(self.current_depth) + \"</depth>\"\n",
    "        str_node += \"<gini>\" + str(self.gini) + \"</gini>\"\n",
    "        if self.node_right is not None:\n",
    "            str_left = self.node_right.to_xml(current_str)\n",
    "            str_node += str_left\n",
    "        if self.node_left is not None:\n",
    "            str_right = self.node_left.to_xml(current_str)\n",
    "            str_node += str_right\n",
    "\n",
    "        if self.is_leaf():\n",
    "            str_node += \"<dominant_class>\" + str(self.dominant_class) + \"</dominant_class><acc_dominant_class>\" + str(\n",
    "                self.accuracy_dominant_class) + \"</acc_dominant_class>\"\n",
    "        str_node += \"</node>\"\n",
    "        return str_node\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Checks whether the node is a leaf\n",
    "        \"\"\"\n",
    "        return self.node_left is None and self.node_right is None\n",
    "\n",
    "    def create_with_children(self, data_torch, current_depth, min_gini=0.000001):\n",
    "        \"\"\"\n",
    "        Creates a node by selecting the best feature and threshold, and if needed, creating its children\n",
    "        param data_torch: dataset with the current partition to deal with in the node\n",
    "        param current_depth: depth counter for the node\n",
    "        param min_gini: hyperparameter selected by the user defining the minimum tolerated Gini coefficient for a  node\n",
    "        return the list of selected features so far\n",
    "        \"\"\"\n",
    "        labels = data_torch[:, -1].long()\n",
    "\n",
    "        self.dominant_class = torch.mode(labels)[0].item()\n",
    "        self.gini = self.calculate_gini(labels, self.num_classes)\n",
    "\n",
    "        list_selected_features = []\n",
    "\n",
    "        if (current_depth >= self.ref_cart.get_max_depth() or\n",
    "                data_torch.shape[0] <= self.ref_cart.get_min_observations() or\n",
    "                self.gini <= min_gini):\n",
    "            return list_selected_features\n",
    "\n",
    "        threshold, feature_idx, min_gini_split = self.select_best_feature_and_thresh(data_torch, self.num_classes)\n",
    "\n",
    "        if feature_idx is None or min_gini_split >= self.gini:\n",
    "            return list_selected_features\n",
    "\n",
    "        self.feature_num = feature_idx\n",
    "        self.threshold_value = threshold\n",
    "\n",
    "        list_selected_features.append(feature_idx)\n",
    "\n",
    "        features = data_torch[:, :-1]\n",
    "        feature_values = features[:, feature_idx]\n",
    "\n",
    "        left_mask = feature_values < threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_data = data_torch[left_mask]\n",
    "        right_data = data_torch[right_mask]\n",
    "\n",
    "        if left_data.shape[0] > 0:\n",
    "            self.node_left = NodeCart(self.num_classes, self.ref_cart, current_depth + 1)\n",
    "            left_features = self.node_left.create_with_children(left_data, current_depth + 1, min_gini)\n",
    "            list_selected_features.extend(left_features)\n",
    "\n",
    "        if right_data.shape[0] > 0:\n",
    "            self.node_right = NodeCart(self.num_classes, self.ref_cart, current_depth + 1)\n",
    "            right_features = self.node_right.create_with_children(right_data, current_depth + 1, min_gini)\n",
    "            list_selected_features.extend(right_features)\n",
    "\n",
    "        return list_selected_features\n",
    "\n",
    "    def select_best_feature_and_thresh(self, data_torch, num_classes=2):\n",
    "        \"\"\"\n",
    "        Selects the best feature and threshold that minimizes the Gini coefficient\n",
    "        param data_torch: dataset partition to analyze\n",
    "        param num_classes: number of K classes to discriminate from\n",
    "        return min_thresh, min_feature, min_gini found for the dataset partition when\n",
    "        selecting the found feature and threshold\n",
    "        \"\"\"\n",
    "        features = data_torch[:, :-1]\n",
    "        labels = data_torch[:, -1].long()\n",
    "\n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "\n",
    "        for feature_idx in range(features.shape[1]):\n",
    "            feature_values = features[:, feature_idx]\n",
    "            unique_values = torch.unique(feature_values, sorted=True)\n",
    "\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "\n",
    "                left_mask = feature_values < threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                left_labels = labels[left_mask]\n",
    "                right_labels = labels[right_mask]\n",
    "\n",
    "                weighted_gini = self.weighted_gini(left_labels, right_labels, num_classes)\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_feature = feature_idx\n",
    "                    best_thresh = threshold.item()\n",
    "\n",
    "        return best_thresh, best_feature, best_gini\n",
    "\n",
    "    def calculate_gini(self, data_partition_torch, num_classes=2):\n",
    "        \"\"\"\n",
    "        Calculates the Gini coefficient for a given partition with the given number of classes\n",
    "        param data_partition_torch: current dataset partition as a tensor\n",
    "        param num_classes: K number of classes to discriminate from\n",
    "        returns the calculated Gini coefficient\n",
    "        \"\"\"\n",
    "        if data_partition_torch.numel() == 0:\n",
    "            return 0.0\n",
    "\n",
    "        class_counts = torch.bincount(data_partition_torch, minlength=num_classes).float()\n",
    "        proportions = class_counts / class_counts.sum()\n",
    "        gini_score = 1.0 - torch.sum(proportions ** 2)\n",
    "        return gini_score.item()\n",
    "\n",
    "    def weighted_gini(self, left_side, right_side, num_classes=2):\n",
    "        n = left_side.numel() + right_side.numel()\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        gini_left = self.calculate_gini(left_side, num_classes)\n",
    "        gini_right = self.calculate_gini(right_side, num_classes)\n",
    "        return (left_side.numel() / n) * gini_left + (right_side.numel() / n) * gini_right\n",
    "\n",
    "    def evaluate_node(self, input_torch):\n",
    "        \"\"\"\n",
    "        Evaluates an input observation within the node.\n",
    "        If is not a leaf node, send it to the corresponding node\n",
    "        return predicted label\n",
    "        \"\"\"\n",
    "        feature_val_input = input_torch[self.feature_num]\n",
    "        if self.is_leaf():\n",
    "            return self.dominant_class\n",
    "        else:\n",
    "            if feature_val_input < self.threshold_value:\n",
    "                return self.node_left.evaluate_node(input_torch)\n",
    "            else:\n",
    "                return self.node_right.evaluate_node(input_torch)\n",
    "\n",
    "\n",
    "class CART:\n",
    "    def __init__(self, dataset_torch, max_cart_depth, min_observations=2):\n",
    "        \"\"\"\n",
    "        CART has only one root node\n",
    "        \"\"\"\n",
    "        #min observations per node\n",
    "        self.min_observations = min_observations\n",
    "        self.root = NodeCart(num_classes=2, ref_cart=self, current_depth=0)\n",
    "        self.max_cart_depth = max_cart_depth\n",
    "        self.list_selected_features = []\n",
    "\n",
    "    def get_root(self):\n",
    "        \"\"\"\n",
    "        Gets tree root\n",
    "        \"\"\"\n",
    "        return self.root\n",
    "\n",
    "    def get_min_observations(self):\n",
    "        \"\"\"\n",
    "        return min observations per node\n",
    "        \"\"\"\n",
    "        return self.min_observations\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        \"\"\"\n",
    "        Gets the selected max depth of the tree\n",
    "        \"\"\"\n",
    "        return self.max_cart_depth\n",
    "\n",
    "    def build_cart(self, data_torch):\n",
    "        \"\"\"\n",
    "        Build CART from root\n",
    "        \"\"\"\n",
    "        self.list_selected_features = self.root.create_with_children(data_torch, current_depth=0)\n",
    "\n",
    "    def to_xml(self, xml_file_name):\n",
    "        \"\"\"\n",
    "        write Xml file with tree content\n",
    "        \"\"\"\n",
    "        str_nodes = self.root.to_xml()\n",
    "        file = open(xml_file_name, \"w+\")\n",
    "        file.write(str_nodes)\n",
    "        file.close()\n",
    "        return str_nodes\n",
    "\n",
    "    def evaluate_input(self, input_torch):\n",
    "        \"\"\"\n",
    "        Evaluate a specific input in the tree and get the predicted class\n",
    "        \"\"\"\n",
    "        return self.root.evaluate_node(input_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Implementación de calculo de Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a Pruebas unitarias para el calculo de Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.738082Z",
     "start_time": "2025-08-18T22:33:07.735111Z"
    }
   },
   "outputs": [],
   "source": [
    "node = NodeCart(num_classes=2)\n",
    "ones_tensor = torch.tensor([1, 1, 1, 1])\n",
    "gini = node.calculate_gini(ones_tensor, num_classes=2)\n",
    "assert gini == 0.0, f\"Expected 0.0, got {gini}\"\n",
    "print(\"Test 1 Gini ✅\")\n",
    "\n",
    "variable_tensor = torch.tensor([0, 1, 2, 3])\n",
    "gini = node.calculate_gini(variable_tensor, num_classes=4)\n",
    "assert gini == 0.75, f\"Expected 0.75, got {gini}\"\n",
    "print(\"Test 2 Gini ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Pruebas unitarias de select_best_feature_and_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T22:33:07.750224Z",
     "start_time": "2025-08-18T22:33:07.748962Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_select_best_feature_two_classes():\n",
    "    data = torch.tensor([\n",
    "        [1.0, 5.0, 0],\n",
    "        [2.0, 3.0, 0],\n",
    "        [3.0, 1.0, 1],\n",
    "        [4.0, 2.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    node = NodeCart(num_classes=2)\n",
    "    thresh, feature_idx, gini_score = node.select_best_feature_and_thresh(data, num_classes=2)\n",
    "\n",
    "    assert thresh is not None, \"Threshold inválido\"\n",
    "    assert feature_idx is not None, \"Feature inválido\"\n",
    "    assert 0 <= feature_idx < 2, f\"Feature debe estar entre 0-1, obtuvo {feature_idx}\"\n",
    "    assert 1.0 <= thresh <= 4.0, f\"Threshold debe estar dentro del rango esperado se obtuvo {thresh}\"\n",
    "    assert gini_score >= 0.0, f\"Gini debe ser >= 0 se obtuvo {gini_score}\"\n",
    "\n",
    "    print(f\"✅ Test 1: Feature={feature_idx}, Threshold={thresh:.3f}, Gini={gini_score:.3f}\")\n",
    "\n",
    "\n",
    "def test_select_best_feature_single_class():\n",
    "    single_class_data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 3.0, 0],\n",
    "        [3.0, 1.0, 0],\n",
    "        [4.0, 4.0, 0]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    node = NodeCart(num_classes=2)\n",
    "    thresh, feature_idx, gini_score = node.select_best_feature_and_thresh(single_class_data, num_classes=2)\n",
    "\n",
    "    if thresh is not None and feature_idx is not None:\n",
    "        assert 0 <= feature_idx < 2, f\"Feature inválido: {feature_idx}\"\n",
    "        assert gini_score >= 0.0, f\"Gini debe ser >= 0 se obtuvo {gini_score}\"\n",
    "        print(f\"✅ Test 2: Feature={feature_idx}, Threshold={thresh:.3f}, Gini={gini_score:.3f}\")\n",
    "    else:\n",
    "        print(\"✅ Test 2: No se pudo encontrar un split\")\n",
    "\n",
    "\n",
    "print(\"Pruebas de select_best_feature_and_thresh:\")\n",
    "test_select_best_feature_two_classes()\n",
    "test_select_best_feature_single_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Pruebas unitarias de create_with_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_create_with_children_normal_splitting():\n",
    "    data = torch.tensor([\n",
    "        [1.0, 10.0, 0],\n",
    "        [1.5, 12.0, 0],\n",
    "        [5.0, 2.0, 1],\n",
    "        [6.0, 1.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=data, max_cart_depth=2, min_observations=1)\n",
    "    root_node = tree.get_root()\n",
    "\n",
    "    selected_features = root_node.create_with_children(data, current_depth=0)\n",
    "\n",
    "    assert isinstance(selected_features, list), \"Debería devolver una lista\"\n",
    "    assert len(selected_features) > 0, \"Debería contener al menos una característica\"\n",
    "    assert not root_node.is_leaf(), \"Root node no debería ser un leaf\"\n",
    "    assert root_node.feature_num is not None, \"Feature number no debería estar vació\"\n",
    "    assert root_node.threshold_value is not None, \"Threshold value no debería estar vació\"\n",
    "    assert root_node.feature_num >= 0, f\"Feature number debería ser mayor o igual a cero, se obtuvo {root_node.feature_num}\"\n",
    "\n",
    "    has_left_child = root_node.node_left is not None\n",
    "    has_right_child = root_node.node_right is not None\n",
    "    assert has_left_child or has_right_child, \"Debería tener al menos un hijo\"\n",
    "\n",
    "    for feature_idx in selected_features:\n",
    "        assert 0 <= feature_idx < 2, f\"Feature index {feature_idx} debería estar entre 0 y 1\"\n",
    "\n",
    "    print(f\"✅ Test 1: Normal splitting - Selected features: {selected_features}, \"\n",
    "          f\"Root feature: {root_node.feature_num}, Threshold: {root_node.threshold_value:.3f} \"\n",
    "          f\"Root feature: {root_node.feature_num}, Threshold: {root_node.threshold_value:.3f}\")\n",
    "\n",
    "\n",
    "def test_create_with_children_min_gini_condition():\n",
    "    data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [1.1, 2.1, 0],\n",
    "        [1.2, 2.2, 0],\n",
    "        [1.3, 2.3, 0],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=data, max_cart_depth=3, min_observations=1)\n",
    "    root_node = tree.get_root()\n",
    "\n",
    "    selected_features = root_node.create_with_children(data, current_depth=0, min_gini=0.1)\n",
    "\n",
    "    assert isinstance(selected_features, list), \"Debería devolver una lista\"\n",
    "    assert len(selected_features) == 0, \"Debería devolver una lista vacía\"\n",
    "    assert root_node.is_leaf(), \"Debería ser  una hoja\"\n",
    "    assert root_node.gini < 0.1, f\"Gibi debería ser menor a 0.1, se obtuvo {root_node.gini:.3f}\"\n",
    "    assert root_node.dominant_class == 0, \"Debería ser la clase dominante 0\"\n",
    "\n",
    "    print(f\"✅ Test 2: Min Gini - Gini: {root_node.gini:.3f}, Dominant class: {root_node.dominant_class}\")\n",
    "\n",
    "\n",
    "print(\"Pruebas de create_with_children:\")\n",
    "test_create_with_children_normal_splitting()\n",
    "test_create_with_children_min_gini_condition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Implementación de TestCart y unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cart(tree, testset_torch):\n",
    "    \"\"\"\n",
    "    Test a previously built CART\n",
    "    \"\"\"\n",
    "    if testset_torch.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    test_features = testset_torch[:, :-1]\n",
    "    true_labels = testset_torch[:, -1].long()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = testset_torch.shape[0]\n",
    "\n",
    "    for i in range(total_predictions):\n",
    "        current_observation = test_features[i]\n",
    "        predicted_label = tree.evaluate_input(current_observation)\n",
    "        true_label = true_labels[i].item()\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cart_perfect_predictions():\n",
    "    train_data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 3.0, 0],\n",
    "        [3.0, 1.0, 1],\n",
    "        [4.0, 2.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    test_data = torch.tensor([\n",
    "        [1.5, 2.5, 0],\n",
    "        [3.5, 1.5, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=train_data, max_cart_depth=2, min_observations=1)\n",
    "    tree.build_cart(train_data)\n",
    "\n",
    "    accuracy = test_cart(tree, test_data)\n",
    "\n",
    "    assert 0.0 <= accuracy <= 1.0, f\"Accuracy debe estar entre 0 y 1 se obtuvo {accuracy}\"\n",
    "    print(f\"✅ Test 1: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "\n",
    "def test_cart_invalid_dataset():\n",
    "    train_data = torch.tensor([\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 3.0, 1],\n",
    "        [3.0, 1.0, 0],\n",
    "        [4.0, 2.0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    tree = CART(dataset_torch=train_data, max_cart_depth=1, min_observations=1)\n",
    "    tree.build_cart(train_data)\n",
    "\n",
    "    empty_test = torch.empty((0, 3), dtype=torch.float32)\n",
    "    accuracy_empty = test_cart(tree, empty_test)\n",
    "    assert accuracy_empty == 0.0, f\"Debería dar accuracy 0.0 se obtuvo {accuracy_empty}\"\n",
    "\n",
    "    single_test = torch.tensor([[2.5, 1.5, 0]], dtype=torch.float32)\n",
    "    accuracy_single = test_cart(tree, single_test)\n",
    "    assert 0.0 <= accuracy_single <= 1.0, f\"Accuracy inválida: {accuracy_single}\"\n",
    "    assert accuracy_single in [0.0, 1.0], \"Con una observación, accuracy debería ser 0.0 o 1.0\"\n",
    "\n",
    "    print(f\"✅ Test 2: Dataset vacío = {accuracy_empty}, Una observación = {accuracy_single}\")\n",
    "\n",
    "\n",
    "test_cart_perfect_predictions()\n",
    "test_cart_invalid_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluación del CART\n",
    "\n",
    "## Implementación de funciones generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_kd99_dataset_tensor():\n",
    "    complete_dataset = pd.concat([df_no_attacks, df_attacks], ignore_index=True)\n",
    "\n",
    "    class_mapping = {'normal.': 0, 'back.': 1}\n",
    "    complete_dataset['outcome'] = complete_dataset['outcome'].map(class_mapping)\n",
    "\n",
    "    dataset_tensor = torch.tensor(complete_dataset.values, dtype=torch.float32)\n",
    "\n",
    "    print(f\"Dataset completo creado:\")\n",
    "    print(f\"- Total de observaciones: {dataset_tensor.shape[0]}\")\n",
    "    print(f\"- Número de características: {dataset_tensor.shape[1] - 1}\")  # -1 por la etiqueta\n",
    "    print(f\"- Observaciones normales: {len(df_no_attacks)}\")\n",
    "    print(f\"- Observaciones de backdoor: {len(df_attacks)}\")\n",
    "    print(f\"- Distribución de clases: {complete_dataset['outcome'].value_counts().to_dict()}\")\n",
    "\n",
    "    return dataset_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Tasa de aciertos y F1-score promedio de todas las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(y_true, y_pred, num_classes=2):\n",
    "    f1_scores = []\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        tp = torch.sum((y_true == class_id) & (y_pred == class_id)).float()\n",
    "        fp = torch.sum((y_true != class_id) & (y_pred == class_id)).float()\n",
    "        fn = torch.sum((y_true == class_id) & (y_pred != class_id)).float()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else torch.tensor(0.0)\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else torch.tensor(0.0)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else torch.tensor(0.0)\n",
    "        f1_scores.append(f1.item())\n",
    "\n",
    "        class_name = \"Normal\" if class_id == 0 else \"Backdoor\"\n",
    "        print(f\"📊 {class_name}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cart_complete(dataset_tensor, max_depth, min_observations=2):\n",
    "    print(f\"\\n🌳 Evaluando CART con profundidad máxima = {max_depth}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    cart = CART(dataset_torch=dataset_tensor,\n",
    "                max_cart_depth=max_depth,\n",
    "                min_observations=min_observations)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    cart.build_cart(dataset_tensor)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    features = dataset_tensor[:, :-1]\n",
    "    true_labels = dataset_tensor[:, -1].long()\n",
    "\n",
    "    predicted_labels = []\n",
    "    for i in range(features.shape[0]):\n",
    "        prediction = cart.evaluate_input(features[i])\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    predicted_labels = torch.tensor(predicted_labels)\n",
    "    evaluation_time = time.time() - start_time\n",
    "\n",
    "    accuracy = test_cart(cart, dataset_tensor)\n",
    "\n",
    "    print(f\"📊 Detalle por clase:\")\n",
    "    f1_score = calculate_f1_score(true_labels, predicted_labels, num_classes=2)\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(f\"\\n📊 Resultados generales:\")\n",
    "    print(f\"   • Tasa de aciertos (Accuracy): {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "    print(f\"   • F1-Score promedio: {f1_score:.4f}\")\n",
    "    print(f\"   • Tiempo de entrenamiento: {training_time:.4f} segundos\")\n",
    "    print(f\"   • Tiempo de evaluación: {evaluation_time:.4f} segundos\")\n",
    "    print(f\"   • Características seleccionadas: {len(cart.list_selected_features)}\")\n",
    "    print(f\"   • Features utilizadas: {cart.list_selected_features}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1_score,\n",
    "        'training_time': training_time,\n",
    "        'evaluation_time': evaluation_time,\n",
    "        'selected_features': cart.list_selected_features,\n",
    "        'cart': cart\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARTE 3 - PUNTO 1: EVALUACIÓN DEL CART\")\n",
    "print(\"Usando el mismo conjunto de datos para entrenamiento y prueba\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📋 Información de los dataframes originales:\")\n",
    "print(f\"   • df_no_attacks (normal): {len(df_no_attacks)} observaciones\")\n",
    "print(f\"   • df_attacks (backdoor): {len(df_attacks)} observaciones\")\n",
    "print(f\"   • Clases en df_no_attacks: {df_no_attacks['outcome'].unique()}\")\n",
    "print(f\"   • Clases en df_attacks: {df_attacks['outcome'].unique()}\")\n",
    "\n",
    "complete_dataset = get_cart_kd99_dataset_tensor()\n",
    "\n",
    "print(f\"\\n📋 Configuración de evaluación:\")\n",
    "print(f\"   • Mínimo 2 observaciones por hoja\")\n",
    "print(f\"   • Evaluación con profundidades máximas: 3 y 4\")\n",
    "\n",
    "# Evaluar con profundidad máxima = 3\n",
    "results_depth_3 = evaluate_cart_complete(complete_dataset, max_depth=3, min_observations=2)\n",
    "\n",
    "# Evaluar con profundidad máxima = 4  \n",
    "results_depth_4 = evaluate_cart_complete(complete_dataset, max_depth=4, min_observations=2)\n",
    "\n",
    "# Resumen comparativo\n",
    "print(f\"\\n📈 RESUMEN COMPARATIVO:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Métrica':<25} {'Profundidad 3':<15} {'Profundidad 4':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Accuracy':<25} {results_depth_3['accuracy']:<15.4f} {results_depth_4['accuracy']:<15.4f}\")\n",
    "print(f\"{'F1-Score':<25} {results_depth_3['f1_score']:<15.4f} {results_depth_4['f1_score']:<15.4f}\")\n",
    "print(\n",
    "    f\"{'Tiempo Entrenamiento (s)':<25} {results_depth_3['training_time']:<15.4f} {results_depth_4['training_time']:<15.4f}\")\n",
    "print(\n",
    "    f\"{'Tiempo Evaluación (s)':<25} {results_depth_3['evaluation_time']:<15.4f} {results_depth_4['evaluation_time']:<15.4f}\")\n",
    "print(\n",
    "    f\"{'Features Seleccionadas':<25} {len(results_depth_3['selected_features']):<15} {len(results_depth_4['selected_features']):<15}\")\n",
    "\n",
    "print(f\"\\n🎯 CONCLUSIONES:\")\n",
    "mejor_accuracy = \"Profundidad 3\" if results_depth_3['accuracy'] > results_depth_4['accuracy'] else \"Profundidad 4\"\n",
    "mejor_f1 = \"Profundidad 3\" if results_depth_3['f1_score'] > results_depth_4['f1_score'] else \"Profundidad 4\"\n",
    "print(f\"   • Mejor Accuracy: {mejor_accuracy}\")\n",
    "print(f\"   • Mejor F1-Score: {mejor_f1}\")\n",
    "print(\n",
    "    f\"   • El árbol con profundidad 4 utiliza {len(results_depth_4['selected_features']) - len(results_depth_3['selected_features'])} características adicionales\")\n",
    "\n",
    "# Guardar los árboles para análisis posterior\n",
    "best_cart_depth_3 = results_depth_3['cart']\n",
    "best_cart_depth_4 = results_depth_4['cart']\n",
    "\n",
    "print(\"\\n✅ Evaluación del punto 1 completada exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Evaluación del CART compleja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_run_evaluation(dataset_tensor, max_depth, min_observations=2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        torch.manual_seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        random.seed(random_state)\n",
    "\n",
    "    data_np = dataset_tensor.numpy()\n",
    "    X = data_np[:, :-1]\n",
    "    y = data_np[:, -1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    train_tensor = torch.tensor(np.column_stack([X_train, y_train]), dtype=torch.float32)\n",
    "    test_tensor = torch.tensor(np.column_stack([X_test, y_test]), dtype=torch.float32)\n",
    "\n",
    "    cart = CART(dataset_torch=train_tensor,\n",
    "                max_cart_depth=max_depth,\n",
    "                min_observations=min_observations)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    cart.build_cart(train_tensor)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    accuracy = test_cart(cart, test_tensor)\n",
    "\n",
    "    test_features = test_tensor[:, :-1]\n",
    "    true_labels = test_tensor[:, -1].long()\n",
    "\n",
    "    predicted_labels = []\n",
    "    for i in range(test_features.shape[0]):\n",
    "        prediction = cart.evaluate_input(test_features[i])\n",
    "        predicted_labels.append(prediction)\n",
    "\n",
    "    predicted_labels = torch.tensor(predicted_labels)\n",
    "    evaluation_time = time.time() - start_time\n",
    "\n",
    "    f1_score = calculate_f1_score_silent(true_labels, predicted_labels, num_classes=2)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1_score,\n",
    "        'training_time': training_time,\n",
    "        'evaluation_time': evaluation_time,\n",
    "        'cart': cart,\n",
    "        'train_size': train_tensor.shape[0],\n",
    "        'test_size': test_tensor.shape[0]\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_f1_score_silent(y_true, y_pred, num_classes=2):\n",
    "    f1_scores = []\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        tp = torch.sum((y_true == class_id) & (y_pred == class_id)).float()\n",
    "        fp = torch.sum((y_true != class_id) & (y_pred == class_id)).float()\n",
    "        fn = torch.sum((y_true == class_id) & (y_pred != class_id)).float()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else torch.tensor(0.0)\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else torch.tensor(0.0)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else torch.tensor(0.0)\n",
    "        f1_scores.append(f1.item())\n",
    "\n",
    "    return sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "\n",
    "def evaluate_multiple_runs(dataset_tensor, max_depth, n_runs=10, min_observations=2):\n",
    "    print(f\"\\n🔄 Ejecutando {n_runs} corridas con profundidad máxima = {max_depth}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    results = []\n",
    "    best_f1_idx = 0\n",
    "    best_f1_score = -1\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"   Corrida {run + 1}/{n_runs}...\", end=\" \")\n",
    "\n",
    "        result = single_run_evaluation(\n",
    "            dataset_tensor,\n",
    "            max_depth,\n",
    "            min_observations,\n",
    "            random_state=42 + run\n",
    "        )\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        if result['f1_score'] > best_f1_score:\n",
    "            best_f1_score = result['f1_score']\n",
    "            best_f1_idx = run\n",
    "\n",
    "        print(f\"✅ Acc: {result['accuracy']:.3f}, F1: {result['f1_score']:.3f}\")\n",
    "\n",
    "    accuracies = [r['accuracy'] for r in results]\n",
    "    f1_scores = [r['f1_score'] for r in results]\n",
    "    train_times = [r['training_time'] for r in results]\n",
    "    eval_times = [r['evaluation_time'] for r in results]\n",
    "\n",
    "    stats = {\n",
    "        'accuracy_mean': np.mean(accuracies),\n",
    "        'accuracy_std': np.std(accuracies),\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores),\n",
    "        'train_time_mean': np.mean(train_times),\n",
    "        'train_time_std': np.std(train_times),\n",
    "        'eval_time_mean': np.mean(eval_times),\n",
    "        'eval_time_std': np.std(eval_times),\n",
    "        'best_run_idx': best_f1_idx,\n",
    "        'best_cart': results[best_f1_idx]['cart'],\n",
    "        'all_results': results\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "def display_results_table(stats_depth_2, stats_depth_3):\n",
    "    print(f\"\\n📊 TABLA DE RESULTADOS (Promedio ± Desviación Estándar)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Métrica':<25} {'Profundidad 2':<25} {'Profundidad 3':<25}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"{'Accuracy':<25} \"\n",
    "          f\"{stats_depth_2['accuracy_mean']:.4f} ± {stats_depth_2['accuracy_std']:.4f} \"\n",
    "          f\"{stats_depth_3['accuracy_mean']:.4f} ± {stats_depth_3['accuracy_std']:.4f}\")\n",
    "\n",
    "    print(f\"{'F1-Score':<25} \"\n",
    "          f\"{stats_depth_2['f1_mean']:.4f} ± {stats_depth_2['f1_std']:.4f:} \"\n",
    "          f\"{stats_depth_3['f1_mean']:.4f} ± {stats_depth_3['f1_std']:.4f}\")\n",
    "\n",
    "    print(f\"{'Tiempo Entren. (s)':<25} \"\n",
    "          f\"{stats_depth_2['train_time_mean']:.4f} ± {stats_depth_2['train_time_std']:.4f} \"\n",
    "          f\"{stats_depth_3['train_time_mean']:.4f} ± {stats_depth_3['train_time_std']:.4f}\")\n",
    "\n",
    "    print(f\"{'Tiempo Eval. (s)':<25} \"\n",
    "          f\"{stats_depth_2['eval_time_mean']:.4f} ± {stats_depth_2['eval_time_std']:.4f:} \"\n",
    "          f\"{stats_depth_3['eval_time_mean']:.4f} ± {stats_depth_3['eval_time_std']:.4f}\")\n",
    "\n",
    "\n",
    "def generate_tree_visualization(cart, filename):\n",
    "    xml_content = cart.to_xml(filename)\n",
    "    print(f\"   📄 Árbol guardado en: {filename}\")\n",
    "    print(f\"   🌐 Para visualizar: abrir {filename} en navegador web\")\n",
    "    return xml_content\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARTE 3 - PUNTO 2: EVALUACIÓN CON 10 PARTICIONES ALEATORIAS (70%-30%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📋 Configuración:\")\n",
    "print(f\"   • Dataset: {complete_dataset.shape[0]} observaciones\")\n",
    "print(f\"   • Particiones: 10 corridas aleatorias\")\n",
    "print(f\"   • División: 70% entrenamiento, 30% prueba\")\n",
    "print(f\"   • Profundidades a evaluar: 2 y 3\")\n",
    "print(f\"   • Mínimo 2 observaciones por hoja\")\n",
    "\n",
    "stats_depth_2 = evaluate_multiple_runs(complete_dataset, max_depth=2, n_runs=10, min_observations=2)\n",
    "stats_depth_3 = evaluate_multiple_runs(complete_dataset, max_depth=3, n_runs=10, min_observations=2)\n",
    "\n",
    "display_results_table(stats_depth_2, stats_depth_3)\n",
    "\n",
    "print(f\"\\n🏆 MEJORES CORRIDAS:\")\n",
    "print(f\"   • Profundidad 2: Corrida {stats_depth_2['best_run_idx'] + 1} (F1-Score: {stats_depth_2['f1_mean']:.4f})\")\n",
    "print(f\"   • Profundidad 3: Corrida {stats_depth_3['best_run_idx'] + 1} (F1-Score: {stats_depth_3['f1_mean']:.4f})\")\n",
    "\n",
    "best_overall = stats_depth_2 if stats_depth_2['f1_mean'] > stats_depth_3['f1_mean'] else stats_depth_3\n",
    "best_depth = 2 if stats_depth_2['f1_mean'] > stats_depth_3['f1_mean'] else 3\n",
    "\n",
    "print(f\"\\n🌳 GENERANDO VISUALIZACIÓN DEL MEJOR ÁRBOL:\")\n",
    "print(f\"   • Mejor profundidad general: {best_depth}\")\n",
    "\n",
    "xml_file_depth_2 = f\"mejor_arbol_profundidad_2.xml\"\n",
    "xml_file_depth_3 = f\"mejor_arbol_profundidad_3.xml\"\n",
    "\n",
    "generate_tree_visualization(stats_depth_2['best_cart'], xml_file_depth_2)\n",
    "generate_tree_visualization(stats_depth_3['best_cart'], xml_file_depth_3)\n",
    "\n",
    "print(f\"\\n📈 ANÁLISIS COMPARATIVO:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if stats_depth_3['accuracy_mean'] > stats_depth_2['accuracy_mean']:\n",
    "    print(\n",
    "        f\"   • ✅ Profundidad 3 tiene mejor accuracy promedio (+{stats_depth_3['accuracy_mean'] - stats_depth_2['accuracy_mean']:.4f})\")\n",
    "else:\n",
    "    print(\n",
    "        f\"   • ✅ Profundidad 2 tiene mejor accuracy promedio (+{stats_depth_2['accuracy_mean'] - stats_depth_3['accuracy_mean']:.4f})\")\n",
    "\n",
    "if stats_depth_3['f1_mean'] > stats_depth_2['f1_mean']:\n",
    "    print(\n",
    "        f\"   • ✅ Profundidad 3 tiene mejor F1-Score promedio (+{stats_depth_3['f1_mean'] - stats_depth_2['f1_mean']:.4f})\")\n",
    "else:\n",
    "    print(\n",
    "        f\"   • ✅ Profundidad 2 tiene mejor F1-Score promedio (+{stats_depth_2['f1_mean'] - stats_depth_3['f1_mean']:.4f})\")\n",
    "\n",
    "print(\n",
    "    f\"   • Variabilidad Accuracy - Prof. 2: {stats_depth_2['accuracy_std']:.4f}, Prof. 3: {stats_depth_3['accuracy_std']:.4f}\")\n",
    "print(f\"   • Variabilidad F1-Score - Prof. 2: {stats_depth_2['f1_std']:.4f}, Prof. 3: {stats_depth_3['f1_std']:.4f}\")\n",
    "\n",
    "more_stable_acc = \"Profundidad 2\" if stats_depth_2['accuracy_std'] < stats_depth_3['accuracy_std'] else \"Profundidad 3\"\n",
    "more_stable_f1 = \"Profundidad 2\" if stats_depth_2['f1_std'] < stats_depth_3['f1_std'] else \"Profundidad 3\"\n",
    "\n",
    "print(f\"   • Más estable en Accuracy: {more_stable_acc}\")\n",
    "print(f\"   • Más estable en F1-Score: {more_stable_f1}\")\n",
    "\n",
    "print(\n",
    "    f\"   • Tiempo entrenamiento - Prof. 2: {stats_depth_2['train_time_mean']:.4f}s, Prof. 3: {stats_depth_3['train_time_mean']:.4f}s\")\n",
    "print(\n",
    "    f\"   • Tiempo evaluación - Prof. 2: {stats_depth_2['eval_time_mean']:.4f}s, Prof. 3: {stats_depth_3['eval_time_mean']:.4f}s\")\n",
    "\n",
    "print(f\"\\n💡 PROPUESTA DE OPTIMIZACIÓN CON JENSEN-SHANNON:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"   La distancia Jensen-Shannon calculada en la Parte 1 podría usarse para:\")\n",
    "print(f\"   1. Pre-seleccionar características más discriminativas antes del entrenamiento\")\n",
    "print(f\"   2. Reducir el espacio de búsqueda en select_best_feature_and_thresh\")\n",
    "print(f\"   3. Priorizar splits en características con mayor separabilidad entre clases\")\n",
    "print(f\"   4. Implementar poda temprana basada en distancias JS bajas\")\n",
    "print(f\"   5. Usar JS como criterio alternativo al Gini para splits más informativos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
