{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBIp6DT8si_1"
      },
      "source": [
        "# Quiz 4\n",
        "\n",
        "Estudiantes:\n",
        "\n",
        "- Alonso Araya Calvo\n",
        "- Pedro Soto\n",
        "- Sofia Oviedo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLfMPjjz6Tqf"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lh6x4GjDbzO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import true_divide\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TqpxBWGYveZ"
      },
      "source": [
        "# Cargando dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pn_P8kZ6a4G",
        "outputId": "94a7e508-5a88-4569-8249-c3dda8223835"
      },
      "outputs": [],
      "source": [
        "#tomado de https://www.kaggle.com/code/wailinnoo/intrusion-detection-system-using-kdd99-dataset\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "try:\n",
        "    path = get_file('kddcup.data_10_percent.gz',\n",
        "                    origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
        "except:\n",
        "    print('Error downloading')\n",
        "    raise\n",
        "\n",
        "print(path)\n",
        "\n",
        "# This file is a CSV, just no CSV extension or headers\n",
        "# Download from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
        "pd_data_frame = pd.read_csv(path, header=None)\n",
        "\n",
        "# The CSV file has no column heads, so add them\n",
        "pd_data_frame.columns = [\n",
        "    'duration',\n",
        "    'protocol_type',\n",
        "    'service',\n",
        "    'flag',\n",
        "    'src_bytes',\n",
        "    'dst_bytes',\n",
        "    'land',\n",
        "    'wrong_fragment',\n",
        "    'urgent',\n",
        "    'hot',\n",
        "    'num_failed_logins',\n",
        "    'logged_in',\n",
        "    'num_compromised',\n",
        "    'root_shell',\n",
        "    'su_attempted',\n",
        "    'num_root',\n",
        "    'num_file_creations',\n",
        "    'num_shells',\n",
        "    'num_access_files',\n",
        "    'num_outbound_cmds',\n",
        "    'is_host_login',\n",
        "    'is_guest_login',\n",
        "    'count',\n",
        "    'srv_count',\n",
        "    'serror_rate',\n",
        "    'srv_serror_rate',\n",
        "    'rerror_rate',\n",
        "    'srv_rerror_rate',\n",
        "    'same_srv_rate',\n",
        "    'diff_srv_rate',\n",
        "    'srv_diff_host_rate',\n",
        "    'dst_host_count',\n",
        "    'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate',\n",
        "    'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate',\n",
        "    'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_serror_rate',\n",
        "    'dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate',\n",
        "    'dst_host_srv_rerror_rate',\n",
        "    'outcome'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyxCEYNuY0Dc"
      },
      "source": [
        "# Limpieza del dataset y generación de subset del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc6X0vQv5ad6"
      },
      "outputs": [],
      "source": [
        "# For now, just drop NA's (rows with missing values), in case there are\n",
        "pd_data_frame.dropna(inplace=True, axis=1)\n",
        "\n",
        "# Checking for DUPLICATE values\n",
        "pd_data_frame.drop_duplicates(keep='first', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_m74GC69apZ"
      },
      "outputs": [],
      "source": [
        "# Filter the DataFrame to keep only 'normal.' and 'back.' outcomes\n",
        "filtered_df = pd_data_frame[pd_data_frame['outcome'].isin(['normal.', 'back.'])].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8NO_C3d8ox2"
      },
      "outputs": [],
      "source": [
        "list_nominal_features = [\"flag\", \"protocol_type\", \"service\"]\n",
        "\n",
        "# Apply one-hot encoding to the nominal features\n",
        "df_encoded = pd.get_dummies(filtered_df, columns=list_nominal_features)\n",
        "\n",
        "# Convert boolean columns (from one-hot encoding) to integers (0 or 1) in df_encoded\n",
        "for col in df_encoded.columns:\n",
        "    if df_encoded[col].dtype == 'bool':\n",
        "        df_encoded[col] = df_encoded[col].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUfTputwfB4Z"
      },
      "outputs": [],
      "source": [
        "# Separo los datos en ataques y normales en diferentes dataframes\n",
        "df_attacks = df_encoded[df_encoded['outcome'] == 'back.'].copy()\n",
        "df_no_attacks = df_encoded[df_encoded['outcome'] == 'normal.'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOz9Gw99aKm7"
      },
      "source": [
        "# Funciones Utilitarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3urpFgWpaKm7"
      },
      "outputs": [],
      "source": [
        "# Util para mostrar dataframes completos sin el resumen que se hace por defecto\n",
        "def display_dataframe_completely(df):\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt8L5f-a9YbX"
      },
      "source": [
        "# Parte 1\n",
        "\n",
        "Elija las 5 caracteristicas con mayor distancia entre las densidades de los ataques y mensajes normales en el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtrwKuOi9fXs"
      },
      "outputs": [],
      "source": [
        "# Es necesario eliminar la columna de outcome para poder utilizar el dataset en pytorch ya que es categorica\n",
        "# y no es necesario realmente tenerla para calculos numericos en la parte de Jensen Shannonya que al separar los datasets\n",
        "# ya se sabe de cual clase proviene.\n",
        "df_attacks_without_outcome = df_attacks.drop('outcome', axis=1)\n",
        "df_normal_without_outcome = df_no_attacks.drop('outcome', axis=1)\n",
        "\n",
        "# Genero los tensores de cada dataframe para poder usarlos en pytorch\n",
        "attack_without_outcomes_column_names = df_attacks_without_outcome.columns\n",
        "attack_tensor = torch.tensor(df_attacks_without_outcome.values, dtype=torch.float32)\n",
        "\n",
        "normal_without_outcomes_column_names = df_normal_without_outcome.columns\n",
        "no_attack_tensor = torch.tensor(df_normal_without_outcome.values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcfCJ7wyaKm9"
      },
      "outputs": [],
      "source": [
        "def generate_feature_histogram_and_calculate_jensen_shannon(df_normal, df_backdoor, feature_names, bins=30):\n",
        "    \"\"\"\n",
        "    Calcula la divergencia de Jensen-Shannon entre características de tráfico normal y ataques backdoor.\n",
        "    \n",
        "    Esta función calcula histogramas para cada característica en los conjuntos de datos normales \n",
        "    y de ataques backdoor, los normaliza para crear distribuciones de probabilidad y calcula \n",
        "    la divergencia de Jensen-Shannon para medir cada característica.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    df_normal : DataFrame que contiene datos de tráfico de red normal\n",
        "    df_backdoor : DataFrame que contiene datos de tráfico de ataques backdoor\n",
        "    feature_names : Lista de nombres de columnas de características a analizar\n",
        "    bins : Número de bins a usar para el cálculo del histograma\n",
        "        \n",
        "    Retorna:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame con columnas:\n",
        "        - 'Feature': Nombres de las características\n",
        "        - 'DistanciaJS': Distancias de Jensen-Shannon, ordenadas de mayor a menor\n",
        "    \"\"\"\n",
        "    js_distances = []\n",
        "\n",
        "    for feat in feature_names:\n",
        "        # Se obtienen los valores numericos de las dos clases\n",
        "        normal_df_values = df_normal[feat].values\n",
        "        backdoor_df_values = df_backdoor[feat].values\n",
        "\n",
        "        # Se obtiene el rango común de histograma para ambas clases\n",
        "        limits_histogram = (min(normal_df_values.min(), backdoor_df_values.min()),\n",
        "                            max(normal_df_values.max(), backdoor_df_values.max()))\n",
        "\n",
        "        # Se calcula el histograma de las dos clases y se normaliza para obtener las densidades\n",
        "        hist_normal, _ = np.histogram(normal_df_values, bins=bins, range=limits_histogram)\n",
        "        hist_backdoor, _ = np.histogram(backdoor_df_values, bins=bins, range=limits_histogram)\n",
        "\n",
        "        hist_normal = hist_normal / hist_normal.sum()\n",
        "        hist_backdoor = hist_backdoor / hist_backdoor.sum()\n",
        "\n",
        "        # Se calcula la distancia de Jensen Shannon entre las dos clases\n",
        "        jsd = jensenshannon(hist_normal, hist_backdoor)\n",
        "        js_distances.append(jsd)\n",
        "\n",
        "    # Dataframe con tabla de distancias de Jensen Shannon para todas las features\n",
        "    js_df = pd.DataFrame({\n",
        "        \"Feature\": feature_names,\n",
        "        \"DistanciaJS\": js_distances\n",
        "    }).sort_values(by=\"DistanciaJS\", ascending=False)\n",
        "\n",
        "    return js_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicacion del Codigo\n",
        "\n",
        "Esta funcion nos va ayudar a poder medir la distancia de Jensen Shannon entre nuestros dos conjuntos de datos de\n",
        "trafico normal y de ataque backdoor. Para este efecto se obtiene los valores numericos de cada caracteristica y\n",
        "se generan los histogramas correspondientes de cada conjunto por medio de la funcion `historgram` de numpy,\n",
        "lo siguiente es que se normalizan los valores y se utilizan de entrada para la funcion `jensenshannon` de scipy\n",
        "para calcular la distancia entre las dos caracteristicas, todo esto se realiza para todas las caracteristicas del\n",
        "dataset.\n",
        "\n",
        "Finalmente retorna una lista ordenada de las caracteristicas con mayor distancia Jensen Shannon. Este calculo va de 0 a 1 y permite recopilar cuales son las caracteristicas mas discriminativas que podrian ser mas utiles para utilizar en un arbol de decision, esto va ayudar a descartar caracteristicas que no sean de mucho interes, optimizar el arbol con las caracteristicas mas importantes y ser mas eficiente al utilizar menos datos a tomar en cuenta.\n",
        "\n",
        "## Resultados\n",
        "\n",
        "En este caso como se puede observar en los resultados de la ejecucion de la siguiente celda, las cinco mejores caracteristicas fueron:\n",
        "\n",
        "```\n",
        "\tFeature\t                  DistanciaJS\n",
        "6\thot\t                    0.813179\n",
        "37\tdst_host_srv_rerror_rate  0.466678\n",
        "36\tdst_host_rerror_rate\t  0.459732\n",
        "24\tsrv_rerror_rate\t          0.373477\n",
        "60\tservice_http\t          0.346864\n",
        "```\n",
        "\n",
        "Proponiendo a estas cinco como las que podrian ayudar a distinguir mejor el trafico entre normal y backdoor, dado que el calculo de Jensen Shannon nos dice cuales podrian ser mas discriminativas entre la clase normal y de ataque como candidatas a para tener mejor rendimiento para el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bwlckW8saKm9",
        "outputId": "91a58f7a-f0ed-49ee-decf-7cac6f84fac9"
      },
      "outputs": [],
      "source": [
        "js_df = generate_feature_histogram_and_calculate_jensen_shannon(df_normal_without_outcome, df_attacks_without_outcome,\n",
        "                                                        attack_without_outcomes_column_names)\n",
        "display_dataframe_completely(js_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 2\n",
        "\n",
        "Parta los datos en una particion de entrenamiento (70%) y otra de prueba (30 %).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicacion del Codigo \n",
        "\n",
        "En la siguiente celda se utiliza la funcion de scikit learn con el fin de poder generar dos particiones, una de entrenamiento y una de prueba con una proporcion de 70%/30%. Para este efecto se separo las etiquetas de la clase outcome, con los datos del dataset y se enviaron a la funcion `train_test_split` la cual genera una particion de los datos en conjunto con una semilla para poder experimentar en otras iteraciones y con estratificacion que permite mantener las proporciones de las clases, algo especial para este dataset ya que KD99 es un poco desbalanceado en tamaño de datos por clase.\n",
        "\n",
        "## Resultados\n",
        "\n",
        "En la salida se pueden observar datos importantes del dataset, pero se adjuntan en el siguiente bloque como referencia:\n",
        "\n",
        "```\n",
        "Conjunto de Entrenamiento (70%):\n",
        "- Observaciones: 62159\n",
        "- Características: 75\n",
        "- Distribución de clases: {'normal.': 61481, 'back.': 678}\n",
        "- Proporción normal/backdoor: {'normal.': 0.9890924886178992, 'back.': 0.010907511382100742}\n",
        "\n",
        "Conjunto de Prueba (30%):\n",
        "- Observaciones: 26640\n",
        "- Características: 75\n",
        "- Distribución de clases: {'normal.': 26350, 'back.': 290}\n",
        "- Proporción normal/backdoor: {'normal.': 0.9891141141141141, 'back.': 0.010885885885885885}\n",
        "\n",
        "Verificación de proporciones:\n",
        "- Dataset original: {'normal.': 0.9890990990990991, 'back.': 0.0109009009009009}\n",
        "- Entrenamiento: {'normal.': 0.9890924886178992, 'back.': 0.010907511382100742}\n",
        "- Prueba: {'normal.': 0.9891141141141141, 'back.': 0.010885885885885885}\n",
        "```\n",
        "\n",
        "Como se puede observar KD99 presenta una desproporcion en las clases, en este caso la normal es mucho mas grande que la de tipo backdoor, pero presenta una realidad en la ciberseguridad en la cual los ataques son mucho menores al trafico normal. Ademas se muestra la proporcion de las clases en el dataset original entrenamiento y prueba, demostrando que todos tienen una estructura similar.\n",
        "\n",
        "Estos datos nos van a ayudar en los siguientes puntos del quiz para poder utilizar datos de evaluacion y prueba para generar los calculos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_encoded.drop('outcome', axis=1)  \n",
        "y = df_encoded['outcome']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.3,\n",
        "    train_size=0.7,      \n",
        "    random_state=42,    \n",
        "    stratify=y          \n",
        ")\n",
        "\n",
        "print(f\"\\nConjunto de Entrenamiento (70%):\")\n",
        "print(f\"- Observaciones: {len(X_train)}\")\n",
        "print(f\"- Características: {X_train.shape[1]}\")\n",
        "print(f\"- Distribución de clases: {y_train.value_counts().to_dict()}\")\n",
        "print(f\"- Proporción normal/backdoor: {y_train.value_counts(normalize=True).to_dict()}\")\n",
        "\n",
        "print(f\"\\nConjunto de Prueba (30%):\")\n",
        "print(f\"- Observaciones: {len(X_test)}\")\n",
        "print(f\"- Características: {X_test.shape[1]}\")  \n",
        "print(f\"- Distribución de clases: {y_test.value_counts().to_dict()}\")\n",
        "print(f\"- Proporción normal/backdoor: {y_test.value_counts(normalize=True).to_dict()}\")\n",
        "\n",
        "# Verificar que las proporciones se mantuvieron\n",
        "print(f\"\\nVerificación de proporciones:\")\n",
        "print(f\"- Dataset original: {y.value_counts(normalize=True).to_dict()}\")\n",
        "print(f\"- Entrenamiento: {y_train.value_counts(normalize=True).to_dict()}\")\n",
        "print(f\"- Prueba: {y_test.value_counts(normalize=True).to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 3\n",
        "\n",
        "- Por cada caracteristica, ajuste los parametros de un modelo Gaussiano o un modelo parametrico\n",
        "mas adecuado, para los datos de mensajes normales. Reporte los parametros del modelo\n",
        "para cada caracteristica y grafiquelos junto con el histograma de los datos. Use los datos de\n",
        "entrenamiento para ello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener las top 5 características calculadas anteriormente en la funcion de distancia de Jensen Shannon\n",
        "top_5_features = js_df.head(5)['Feature'].tolist()\n",
        "print(f\"Top 5 características más discriminativas según Jensen-Shannon:\")\n",
        "for i, feature in enumerate(top_5_features, 1):\n",
        "    js_distance = js_df[js_df['Feature'] == feature]['DistanciaJS'].iloc[0]\n",
        "    print(f\"  {i}. {feature} (JS = {js_distance:.6f})\")\n",
        "\n",
        "print(f\"\\n=== Filtrado de datos normales de entrenamiento ===\")\n",
        "# Filtrar solo los datos normales del conjunto de entrenamiento\n",
        "X_train_normal = X_train[y_train == 'normal.']\n",
        "print(f\"Datos normales de entrenamiento: {len(X_train_normal)} muestras\")\n",
        "\n",
        "exponential_params_top5 = {}\n",
        "\n",
        "def plot_histogram_with_exponential_top5(data, feature_name, loc, scale, js_distance):\n",
        "    \"\"\"\n",
        "    Genera un histograma normalizado superpuesto con una curva de distribución exponencial ajustada.\n",
        "    \n",
        "    Esta función visualiza la distribución de una característica específica junto con\n",
        "    su modelo exponencial teórico, mostrando métricas de calidad del ajuste.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : Los datos observados de la característica a visualizar. Debe contener valores numéricos.\n",
        "    feature_name :Nombre descriptivo de la característica que será usado como título del gráfico.\n",
        "    loc : Parámetro de localización de la distribución exponencial, es el punto de inicio de la distribución.\n",
        "    scale : Parámetro de escala de la distribución exponencial (1/λ donde λ es la tasa de decaimiento).\n",
        "    js_distance : Distancia Jensen-Shannon que mide la similitud entre la distribución y la \n",
        "        exponencial teórica.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Histograma normalizado\n",
        "    plt.hist(data, bins=50, density=True, alpha=0.7, color='lightgreen', \n",
        "             edgecolor='black', label='Datos observados (normales)')\n",
        "    \n",
        "    # Curva exponencial ajustada\n",
        "    x = np.linspace(data.min(), data.max(), 1000)\n",
        "    exponential_curve = stats.expon.pdf(x, loc=loc, scale=scale)\n",
        "    plt.plot(x, exponential_curve, 'red', linewidth=3, \n",
        "             label=f'Distribución Exponencial ajustada\\nλ = {1/scale:.4f} (rate)\\nEscala = {scale:.4f}')\n",
        "    \n",
        "    plt.title(f'{feature_name}\\n(Jensen-Shannon = {js_distance:.6f})', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Valor de la característica', fontsize=12)\n",
        "    plt.ylabel('Densidad de probabilidad', fontsize=12)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Procesar cada una de las 5 mejores características\n",
        "print(f\"\\n=== Ajuste de parametros exponenciales ===\")\n",
        "for i, feature in enumerate(top_5_features, 1):\n",
        "    print(f\"\\n--- Característica {i}/5: {feature} ---\")\n",
        "    \n",
        "    # Obtener los datos numericos de la característica\n",
        "    data = X_train_normal[feature].values\n",
        "    \n",
        "    # Remover valores NaN si existen\n",
        "    data_clean = data[~np.isnan(data)]\n",
        "    \n",
        "    # Para distribución exponencial, los datos deben ser no negativos\n",
        "    # por ende se transforman los datos para que sean no negativos\n",
        "    if np.min(data_clean) < 0:\n",
        "        print(f\"  Datos con valores negativos. Aplicando transformación: x' = x - min(x)\")\n",
        "        data_transformed = data_clean - np.min(data_clean)\n",
        "    else:\n",
        "        data_transformed = data_clean\n",
        "    \n",
        "    if len(data_transformed) == 0:\n",
        "        print(f\"  Sin datos válidos para {feature}\")\n",
        "        continue\n",
        "    \n",
        "    # Calcular estadísticas\n",
        "    mean_val = np.mean(data_transformed)\n",
        "    std_val = np.std(data_transformed, ddof=1)\n",
        "    min_val = np.min(data_transformed)\n",
        "    max_val = np.max(data_transformed)\n",
        "    \n",
        "    # Ajustar distribución exponencial usando Maximum Likelihood Estimation\n",
        "    # stats.expon.fit devuelve (loc, scale) donde scale = 1/λ\n",
        "    loc, scale = stats.expon.fit(data_transformed)\n",
        "    lambda_param = 1/scale  # Parametro de tasa\n",
        "    \n",
        "    # Obtener la distancia Jensen-Shannon que calculamos anteriormente\n",
        "    js_distance = js_df[js_df['Feature'] == feature]['DistanciaJS'].iloc[0]\n",
        "    \n",
        "    # Almacenar parámetros\n",
        "    exponential_params_top5[feature] = {\n",
        "        'lambda': lambda_param,\n",
        "        'scale': scale,\n",
        "        'loc': loc,\n",
        "        'mean_theoretical': scale,  # Para exponencial, media = scale = 1/λ\n",
        "        'mean_empirical': mean_val,\n",
        "        'std_empirical': std_val,\n",
        "        'min': min_val,\n",
        "        'max': max_val,\n",
        "        'n_samples': len(data_transformed),\n",
        "        'jensen_shannon': js_distance,\n",
        "        'data_transformed': np.min(data_clean) < 0\n",
        "    }\n",
        "    \n",
        "    print(f\"  Estadísticas descriptivas:\")\n",
        "    print(f\"    - Media muestral: {mean_val:.6f}\")\n",
        "    print(f\"    - Desv. estándar muestral: {std_val:.6f}\")\n",
        "    print(f\"    - Rango: [{min_val:.6f}, {max_val:.6f}]\")\n",
        "    print(f\"    - Número de muestras: {len(data_transformed)}\")\n",
        "    if np.min(data_clean) < 0:\n",
        "        print(f\"    - Transformación aplicada: x' = x - {np.min(data_clean):.6f}\")\n",
        "    \n",
        "    print(f\"  Parámetros del modelo Exponencial (MLE):\")\n",
        "    print(f\"    - λ (rate parameter): {lambda_param:.6f}\")\n",
        "    print(f\"    - Escala (1/λ): {scale:.6f}\")\n",
        "    print(f\"    - Localización: {loc:.6f}\")\n",
        "    print(f\"    - Media teórica: {scale:.6f}\")\n",
        "    print(f\"    - Distancia Jensen-Shannon: {js_distance:.6f}\")\n",
        "    \n",
        "    # Generacion de graficos\n",
        "    plot_histogram_with_exponential_top5(data_transformed, feature, loc, scale, js_distance)\n",
        "\n",
        "print(f\"\\n=== Tabla resumen de parametros exponenciales (top 5 caracteristicas) ===\")\n",
        "# Crear dataframe de resumen\n",
        "params_exp_top5_df = pd.DataFrame.from_dict(exponential_params_top5, orient='index')\n",
        "params_exp_top5_df = params_exp_top5_df.round(6)\n",
        "params_exp_top5_df = params_exp_top5_df.sort_values('jensen_shannon', ascending=False)\n",
        "\n",
        "# Seleccionar columnas más importantes para mostrar\n",
        "display_columns = ['lambda', 'scale', 'mean_theoretical', 'mean_empirical', 'jensen_shannon', 'n_samples']\n",
        "display_dataframe_completely(params_exp_top5_df[display_columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicacion del Codigo\n",
        "\n",
        "De los calculos de las celdas anteriores se puede observar la funcionalidad que nos da los resultados de ajustar los parametros de un modelo exponencial para cada caracteristicas de los datos de entrenamiento de la clase normal. Para este efecto se filtro primero los datos de entrenamiento a solo los que tuvieran la etiqueta `normal`. Despues de ello se creo una una funcion utilitaria `plot_histogram_with_exponential_top5` para poder generar los graficos, estos graficos contienen un histograma superpuesto a la curva de distribucion exponencial ajustada de la caracteristica enviada por sus parametros.\n",
        "\n",
        "Finalmente para poder generar todos los calculos de itera sobre las cinco mejores caracteristicas encontradas anteriormente en nuestra funcion de calculo de Jensen Shannon y se obtienen los datos de entrenamiento de la clase normal, se limpian si es necesario, se calculan los diferentes momentos estadisticos como maximos, minimos, media aritmetica y la desviacion estandar muestral, para ajustar la distribucion exponencial se utiliza la funcion de scipy `stats.expon.fit` y se guardan todos los parametros utilizados y los resultados en una variable para poder imprimir los resultados y una tabla de resumen \n",
        "con los parametros y cada caracteristica analizada.\n",
        "\n",
        "## Interpretacion de Resultados\n",
        "\n",
        "De los resultados se muestra como la caracteristica `hot` tiene la tasa mas alta, esto significa que los valores tienden a ser pequeños y concentrados cerca de cero, sugiriendo que son mas difiles de observar en el trafico normal siendo esto una oportunidad para utilizarlos en la deteccion ya que una anomalia se podria ver mas facilmente. En cambio la caracteristica `service_http` tiene la mas baja, lo que sugiere que es mas distribuido y con una mayor variabilidad para los datos normales.\n",
        "\n",
        "En el valor de escala tambien es posible ver en promedio cuales caracteristicas son esperadas por trafico en cada porcentaje, como se puede observar la mayoria de caracteristicas tienen un valor bajo, sin embargo `service_http` es un valor con porcentaje alto, lo que indica que es mas tipico ver en el trafico normal, confirmando que podria no ser tan buen candidato para discriminar.\n",
        "\n",
        "De los resultados de la celda siguiente se puede observar nada mas un resumen de los graficos calculados anteriormente y un resumen comparativo de los resultados de utilizar el modelo exponencial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== Grafico conjunto de las 5 mejores caracteristicas con modelos exponenciales ===\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(top_5_features):\n",
        "    data = X_train_normal[feature].values\n",
        "    data_clean = data[~np.isnan(data)]\n",
        "    \n",
        "    # Aplicar transformación si es necesaria\n",
        "    if np.min(data_clean) < 0:\n",
        "        data_transformed = data_clean - np.min(data_clean)\n",
        "    else:\n",
        "        data_transformed = data_clean\n",
        "    \n",
        "    lambda_param = exponential_params_top5[feature]['lambda']\n",
        "    scale = exponential_params_top5[feature]['scale']\n",
        "    loc = exponential_params_top5[feature]['loc']\n",
        "    js_distance = exponential_params_top5[feature]['jensen_shannon']\n",
        "    \n",
        "    # Histograma\n",
        "    axes[i].hist(data_transformed, bins=30, density=True, alpha=0.7, \n",
        "                color='lightgreen', edgecolor='black', label='Datos observados')\n",
        "    \n",
        "    # Curva exponencial\n",
        "    x = np.linspace(data_transformed.min(), data_transformed.max(), 200)\n",
        "    exponential_curve = stats.expon.pdf(x, loc=loc, scale=scale)\n",
        "    axes[i].plot(x, exponential_curve, 'red', linewidth=2, \n",
        "                label=f'Exp(λ={lambda_param:.3f})')\n",
        "    \n",
        "    axes[i].set_title(f'{feature}\\nJS = {js_distance:.4f}', fontsize=11, fontweight='bold')\n",
        "    axes[i].legend(fontsize=9)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "# Ocultar el último subplot si no se usa\n",
        "if len(top_5_features) < 6:\n",
        "    axes[5].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Análisis comparativo de los parámetros\n",
        "print(f\"\\n=== Analisis comparativo de parametros exponenciales ===\")\n",
        "print(f\"Característica con mayor tasa (λ): {params_exp_top5_df['lambda'].idxmax()} (λ = {params_exp_top5_df['lambda'].max():.4f})\")\n",
        "print(f\"Característica con menor tasa (λ): {params_exp_top5_df['lambda'].idxmin()} (λ = {params_exp_top5_df['lambda'].min():.4f})\")\n",
        "print(f\"Media de tasas λ: {params_exp_top5_df['lambda'].mean():.4f}\")\n",
        "print(f\"Desviación estándar de tasas λ: {params_exp_top5_df['lambda'].std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 4\n",
        "\n",
        "Escoja 3 umbrales diferentes de aceptacion para comportamientos normales y con los datos\n",
        "de prueba, usando cada una de las 5 caracteristicas por separado, calcule la verosimilitud y\n",
        "discrimine entre ataques y comportamientos normales. Calcule la matriz de confusion. Comente\n",
        "los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir los 3 umbrales de aceptación (percentiles de la distribución)\n",
        "thresholds_percentiles = [5, 10, 20]\n",
        "print(f\"\\nUmbrales basados en percentiles de la distribución exponencial: {thresholds_percentiles}%\")\n",
        "\n",
        "classification_results = {}\n",
        "\n",
        "def calculate_likelihood_and_classify(data, loc, scale, threshold_value):\n",
        "    \"\"\"\n",
        "    Calcular verosimilitud y clasificar usando umbral\n",
        "    \"\"\"\n",
        "    # Aplicar transformación si es necesaria (igual que en entrenamiento)\n",
        "    if loc > 0:\n",
        "        data_transformed = data - loc\n",
        "    else:\n",
        "        data_transformed = data.copy()\n",
        "    \n",
        "    # Asegurar valores no negativos\n",
        "    data_transformed = np.maximum(data_transformed, 1e-10)\n",
        "    \n",
        "    # Calcular log-verosimilitud\n",
        "    log_likelihood = stats.expon.logpdf(data_transformed, loc=0, scale=scale)\n",
        "    \n",
        "    # Convertir a verosimilitud\n",
        "    likelihood = np.exp(log_likelihood)\n",
        "    \n",
        "    # Clasificar: si likelihood >= threshold → Normal, sino → Ataque\n",
        "    predictions = np.where(likelihood >= threshold_value, 'normal.', 'back.')\n",
        "    \n",
        "    return likelihood, predictions\n",
        "\n",
        "def plot_confusion_matrix(cm, feature_name, threshold_perc, accuracy):\n",
        "    \"\"\"Función para plotear matriz de confusión\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Predicted Normal', 'Predicted Attack'],\n",
        "                yticklabels=['Actual Normal', 'Actual Attack'])\n",
        "    plt.title(f'Matriz de Confusión: {feature_name}\\nUmbral: {threshold_perc}% - Accuracy: {accuracy:.4f}')\n",
        "    plt.ylabel('Etiqueta Real')\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Procesar cada característica con cada umbral\n",
        "for i, feature in enumerate(top_5_features, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Caracteristica {i}/5: {feature}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Obtener parámetros del modelo ajustado\n",
        "    lambda_param = exponential_params_top5[feature]['lambda']\n",
        "    scale = exponential_params_top5[feature]['scale']\n",
        "    loc = exponential_params_top5[feature]['loc']\n",
        "    js_distance = exponential_params_top5[feature]['jensen_shannon']\n",
        "    \n",
        "    print(f\"Parámetros del modelo: λ={lambda_param:.4f}, escala={scale:.4f}\")\n",
        "    print(f\"Distancia Jensen-Shannon: {js_distance:.6f}\")\n",
        "    \n",
        "    # Obtener datos de prueba para esta característica\n",
        "    test_data = X_test[feature].values\n",
        "    test_data_clean = test_data[~np.isnan(test_data)]\n",
        "    y_test_clean = y_test[~np.isnan(test_data)]\n",
        "    \n",
        "    # Calcular umbrales basados en percentiles de la distribución\n",
        "    thresholds_values = []\n",
        "    for perc in thresholds_percentiles:\n",
        "        # Umbral como percentil de la distribución exponencial\n",
        "        threshold_val = stats.expon.ppf(perc/100, loc=loc, scale=scale)\n",
        "        # Convertir a likelihood\n",
        "        threshold_likelihood = stats.expon.pdf(threshold_val, loc=loc, scale=scale)\n",
        "        thresholds_values.append(threshold_likelihood)\n",
        "    \n",
        "    print(f\"Umbrales de verosimilitud calculados: {[f'{t:.6f}' for t in thresholds_values]}\")\n",
        "    \n",
        "    classification_results[feature] = {}\n",
        "    \n",
        "    # Evaluar con cada umbral\n",
        "    for j, (threshold_perc, threshold_val) in enumerate(zip(thresholds_percentiles, thresholds_values), 1):\n",
        "        print(f\"\\n--- Umbral {j}/3: {threshold_perc}% (likelihood ≥ {threshold_val:.6f}) ---\")\n",
        "        \n",
        "        # Calcular verosimilitud y clasificar\n",
        "        likelihoods, predictions = calculate_likelihood_and_classify(\n",
        "            test_data_clean, loc, scale, threshold_val\n",
        "        )\n",
        "        \n",
        "        # Calcular métricas\n",
        "        accuracy = accuracy_score(y_test_clean, predictions)\n",
        "        cm = confusion_matrix(y_test_clean, predictions, labels=['normal.', 'back.'])\n",
        "        \n",
        "        # Calcular métricas adicionales\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        \n",
        "        # Almacenar resultados\n",
        "        classification_results[feature][f'threshold_{threshold_perc}%'] = {\n",
        "            'threshold_value': threshold_val,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'specificity': specificity,\n",
        "            'f1_score': f1_score,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': predictions,\n",
        "            'likelihoods': likelihoods\n",
        "        }\n",
        "        \n",
        "        print(f\"Resultados:\")\n",
        "        print(f\"  - Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  - Precision (detección de ataques): {precision:.4f}\")\n",
        "        print(f\"  - Recall (sensibilidad): {recall:.4f}\")\n",
        "        print(f\"  - Specificity (especificidad): {specificity:.4f}\")\n",
        "        print(f\"  - F1-Score: {f1_score:.4f}\")\n",
        "        print(f\"  - Matriz de confusión:\")\n",
        "        print(f\"    TN={tn} (Normal→Normal), FP={fp} (Normal→Ataque)\")\n",
        "        print(f\"    FN={fn} (Ataque→Normal), TP={tp} (Ataque→Ataque)\")\n",
        "        \n",
        "        # Graficar matriz de confusión\n",
        "        plot_confusion_matrix(cm, feature, threshold_perc, accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Tabla resumen comparativa ===\")\n",
        "\n",
        "summary_data = []\n",
        "for feature in top_5_features:\n",
        "    js_dist = exponential_params_top5[feature]['jensen_shannon']\n",
        "    for threshold_key, results in classification_results[feature].items():\n",
        "        summary_data.append({\n",
        "            'Característica': feature,\n",
        "            'Jensen-Shannon': js_dist,\n",
        "            'Umbral': threshold_key,\n",
        "            'Accuracy': results['accuracy'],\n",
        "            'Precision': results['precision'],\n",
        "            'Recall': results['recall'],\n",
        "            'Specificity': results['specificity'],\n",
        "            'F1-Score': results['f1_score']\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.round(4)\n",
        "\n",
        "print(\"Tabla completa de resultados:\")\n",
        "display_dataframe_completely(summary_df)\n",
        "\n",
        "print(f\"\\n=== Analisis de mejor rendimiento ===\")\n",
        "\n",
        "# Mejor accuracy\n",
        "best_accuracy = summary_df.loc[summary_df['Accuracy'].idxmax()]\n",
        "print(f\"Mejor Accuracy: {best_accuracy['Accuracy']:.4f}\")\n",
        "print(f\"  - Característica: {best_accuracy['Característica']}\")\n",
        "print(f\"  - Umbral: {best_accuracy['Umbral']}\")\n",
        "\n",
        "# Mejor F1-Score\n",
        "best_f1 = summary_df.loc[summary_df['F1-Score'].idxmax()]\n",
        "print(f\"Mejor F1-Score: {best_f1['F1-Score']:.4f}\")\n",
        "print(f\"  - Característica: {best_f1['Característica']}\")\n",
        "print(f\"  - Umbral: {best_f1['Umbral']}\")\n",
        "\n",
        "# Mejor balance (Precision + Recall)\n",
        "summary_df['Balance'] = (summary_df['Precision'] + summary_df['Recall']) / 2\n",
        "best_balance = summary_df.loc[summary_df['Balance'].idxmax()]\n",
        "print(f\"Mejor Balance (Precision+Recall)/2: {best_balance['Balance']:.4f}\")\n",
        "print(f\"  - Característica: {best_balance['Característica']}\")\n",
        "print(f\"  - Umbral: {best_balance['Umbral']}\")\n",
        "\n",
        "# Gráfico comparativo\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Accuracy por característica y umbral\n",
        "plt.subplot(2, 2, 1)\n",
        "summary_pivot = summary_df.pivot(index='Característica', columns='Umbral', values='Accuracy')\n",
        "sns.heatmap(summary_pivot, annot=True, cmap='YlOrRd', fmt='.3f')\n",
        "plt.title('Accuracy por Característica y Umbral')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Subplot 2: F1-Score\n",
        "plt.subplot(2, 2, 2)\n",
        "summary_pivot_f1 = summary_df.pivot(index='Característica', columns='Umbral', values='F1-Score')\n",
        "sns.heatmap(summary_pivot_f1, annot=True, cmap='YlGnBu', fmt='.3f')\n",
        "plt.title('F1-Score por Característica y Umbral')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Subplot 3: Precision vs Recall\n",
        "plt.subplot(2, 2, 3)\n",
        "for threshold in summary_df['Umbral'].unique():\n",
        "    subset = summary_df[summary_df['Umbral'] == threshold]\n",
        "    plt.scatter(subset['Recall'], subset['Precision'], \n",
        "                label=threshold, s=100, alpha=0.7)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision vs Recall por Umbral')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Jensen-Shannon vs Accuracy\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.scatter(summary_df['Jensen-Shannon'], summary_df['Accuracy'], \n",
        "            c=summary_df['Umbral'].astype('category').cat.codes, \n",
        "            cmap='viridis', s=100, alpha=0.7)\n",
        "plt.xlabel('Distancia Jensen-Shannon')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Jensen-Shannon vs Accuracy')\n",
        "plt.colorbar(label='Umbral')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicacion del codigo\n",
        "\n",
        "De esta seccion cuatro se realizo una logica en la cual se definieron tres umbrales en el orden de 5%, 10% y 20%, despues se definio la funcion `calculate_likelihood_and_classify` la cual calcula la verosimilitud por medio de la funcion de scipy `stats.expon.logpdf` para calcular la log-verosimilitud y despues se convierte a verosimilitud usando la funcion de numpy `np.exp`, la logica de esta funcion es que si la verosimilitud es mayor o igual al umbral se clasifica como una prediccion de trafico normal y si no de ataque.\n",
        "\n",
        "En si la logica principal itera por la salida de las cinco mejores caracteristicas y se obtienen los parametros de los resultados de la parte 3 donde se pueden reutilizar los balores de lambda, scale, loc y jensen_shannon para cada caracteristica, ademas se optinenen los datos de la particion de prueba para la evaluacion. Se calculan los umbrales adaptativos en el cual se itera por los umbrales escogidos y se calcula el umbral como percentil de la distribucion exponencial por medio de `stats.expon.ppf` y se convierte finalmente a verosimilitud nuevamente por medio de la misma funcion.\n",
        "\n",
        "Finalmente se calculan las metricas por cada umbral, se utiliza la funcion `calculate_likelihood_and_classify` para calcular la verosimilitud y la prediccion del trafico, se recopila el accuracy, matriz de confusion y las metricas de precision, recall, specificity y el F1-Score para almacenarlos en una variable por cada umbral y caracteristica, generando como salida los resultados del umbral y caracteristica, asi como un plot de la matriz de confusion.\n",
        "\n",
        "Como resumen tambien se genero una tabla con todos los datos por caracteristica y umbral para comparacion, analisis de mejor accuracy, f1-score y balanca, y unos graficos comparativos de las metricas recopiladas.\n",
        "\n",
        "## Interpretacion de Resultados\n",
        "\n",
        "Se definieron tres umbrales 5%, 10% y 20%, estos son usados en conjunto con la verosimilitud para calcular si se pertenece a trafico normal o de ataque, si es mayor o igual al umbral es trafico normal y si es menor es trafico de ataque.\n",
        "\n",
        "Se utiliza la verosimilitud para indicar que tan \"normal\" es ese valor, por ejemplo si es un valor alto significa que es un valor tipico para el trafico normal indicando que probablemente sea trafico normal, si es bajo significa que es un valor raro de ver en el trafico normal y podria significar trafico.\n",
        "\n",
        "En conjunto, los umbrales y la verosimilitud me indican si la verosimilitud calculada para una caracteristica es indicativa de trafico normal o de ataque respecto al umbral.\n",
        "\n",
        "En cuanto a los umbrales, se utilizan percentiles relativamente bajos de 5%, 10% y 20%, ya que los ataques son eventos poco comunes, lo cual es acorde al dataset donde el tráfico normal representa el 98.9% y los ataques solo el 1.1%. El umbral del 5% es el más conservador, establece un punto de corte muy estricto, clasificando como ataques únicamente aquellas conexiones con verosimilitudes extremadamente bajas, esto minimiza las falsas alarmas pero puede perder ataques sutiles. El umbral del 10% representa un balance entre sensibilidad y especificidad, siendo apropiado para uso general. El umbral del 20% es el más permisivo, usa un punto de corte más bajo, detectando como ataques incluso conexiones con verosimilitudes moderadamente bajas, esto maximiza la detección de ataques reales al costo de generar más falsas alarmas.\n",
        "\n",
        "La caracteristica `hot` fue la que obtuvo un mejor rendimiento ya que en los tres umbrales genero los mismos resultados, obteniendo un accuracy de 0.9940, precision de 0.6443, recall de 0.9931 y F1-Score de 0.7815, estas metricas fueron las mas altas de todas las caracteristicas, inclusive en el caso de la matriz de confusion se clasifico correctamente el 99.31% de los ataques reales y solo un 1.1% de falsos positivos, generando un buen resultado y demostrando que es una caracteristica que logra separar correctamente las clases.\n",
        "\n",
        "Los siguientes resultados fueron los de las caracteristicas `dst_host_srv_rerror_rate`, `dst_host_rerror_rate`, `srv_rerror_rate`, estas presentan resultados similar que presentan un buen accuracy entre 0.90 y 0.93, ademas de un recall bueno en su mayoria de 0.7 a 0.9, excepto en el valor srv_rerror_rate que tiene una precision de 0.3. Otro punto negativo en estas caracteristicas es el F1-Score que presenta un valor de 0.11 a 0.17. Estas metricas indican que tienen una alta tasa de falsos positivos al tener una precision baja y podrian clasificar incorrectamente las clases. En el caso de los umbrales para estas caracteristicas el valor de 5% y 10% tienen valores iguales, excepto el 20% que si mejora las metricas un poco. \n",
        "\n",
        "En comparacion con `hot`, `dst_host_srv_rerror_rate`, `dst_host_rerror_rate` y `srv_rerror_rate` presentan un alza en los falsos positivos y falsos negativos, en el caso de `dst_host_srv_rerror_rate` se encuentra en su mejor umbral de 20% alrededor de 1997 falsos positivos y 60 falsos negativos, para `dst_host_rerror_rate` se encontro alredeor de 2184 falsos positivos y 60 falsos negativos en su 20% de umbral, y para `srv_rerror_rate` se encontró en su mejor umbral de 20% un total de falsos positivos de 1606 y falsos negativos de 177. En comparacion `hot` tuvo 159 falsos positivos y 2 falsos negativo, siendo una diferencia muy visible respecto a las demas caracteristicas.\n",
        "\n",
        "La peor caracteristica en este analisis fue la de `service_http` que obtuvo los mismos valores en todos los umbrales con un accuracy de 0.3138, precision de 0.0156, recall de 1 y F1-Score de 0.0308 con un total de 18281 falsos positivos y 0 falsos negativos, esto apoya la puntuacion baja de Jensen Shannon que tenia esta caracteristica y demuestra lo limitada que es la separacion de clases entre las distribuciones de trafico normal y de ataque."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 5\n",
        "\n",
        "Realice lo mismo del item anterior, pero usando como puntaje el promedio de las verosimilitudes\n",
        "de las 5 caracteristicas. Calcule la matriz de confusion. Comente los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Clasificacion usando promedio de verosimilitudes (5 caracteristicas) ===\")\n",
        "\n",
        "def calculate_combined_likelihood_scores(test_data_dict, exponential_params):\n",
        "    \"\"\"\n",
        "    Calcular el promedio de verosimilitudes para las 5 características\n",
        "    \"\"\"\n",
        "    n_samples = len(next(iter(test_data_dict.values())))\n",
        "    combined_scores = np.zeros(n_samples)\n",
        "    \n",
        "    print(\"Calculando verosimilitudes individuales...\")\n",
        "    for i, feature in enumerate(top_5_features):\n",
        "        # Obtener parámetros del modelo\n",
        "        scale = exponential_params[feature]['scale']\n",
        "        loc = exponential_params[feature]['loc']\n",
        "        \n",
        "        # Obtener datos de prueba\n",
        "        data = test_data_dict[feature]\n",
        "        \n",
        "        # Aplicar transformación si es necesaria\n",
        "        if loc > 0:\n",
        "            data_transformed = data - loc\n",
        "        else:\n",
        "            data_transformed = data.copy()\n",
        "        \n",
        "        # Asegurar valores no negativos\n",
        "        data_transformed = np.maximum(data_transformed, 1e-10)\n",
        "        \n",
        "        # Calcular verosimilitud\n",
        "        likelihood = stats.expon.pdf(data_transformed, loc=0, scale=scale)\n",
        "        \n",
        "        # Agregar al promedio (evitar valores infinitos)\n",
        "        likelihood = np.clip(likelihood, 1e-15, 1e15)\n",
        "        combined_scores += likelihood\n",
        "        \n",
        "        print(f\"  {i+1}. {feature}: verosimilitud promedio = {np.mean(likelihood):.8f}\")\n",
        "    \n",
        "    # Promediar las verosimilitudes\n",
        "    combined_scores = combined_scores / len(top_5_features)\n",
        "    \n",
        "    return combined_scores\n",
        "\n",
        "# Preparar datos de prueba para las 5 características\n",
        "test_data_dict = {}\n",
        "valid_indices = None\n",
        "\n",
        "for feature in top_5_features:\n",
        "    test_data = X_test[feature].values\n",
        "    \n",
        "    # Encontrar índices válidos (sin NaN) comunes a todas las características\n",
        "    if valid_indices is None:\n",
        "        valid_indices = ~np.isnan(test_data)\n",
        "    else:\n",
        "        valid_indices = valid_indices & ~np.isnan(test_data)\n",
        "    \n",
        "    test_data_dict[feature] = test_data\n",
        "\n",
        "# Aplicar índices válidos\n",
        "for feature in top_5_features:\n",
        "    test_data_dict[feature] = test_data_dict[feature][valid_indices]\n",
        "\n",
        "y_test_clean = y_test[valid_indices]\n",
        "\n",
        "print(f\"Datos de prueba limpios: {len(y_test_clean)} muestras\")\n",
        "print(f\"Distribución de clases en prueba: {pd.Series(y_test_clean).value_counts().to_dict()}\")\n",
        "\n",
        "# Calcular puntajes combinados\n",
        "combined_likelihood_scores = calculate_combined_likelihood_scores(test_data_dict, exponential_params_top5)\n",
        "\n",
        "print(f\"\\nEstadísticas de puntajes combinados:\")\n",
        "print(f\"  - Promedio: {np.mean(combined_likelihood_scores):.8f}\")\n",
        "print(f\"  - Mediana: {np.median(combined_likelihood_scores):.8f}\")\n",
        "print(f\"  - Desv. estándar: {np.std(combined_likelihood_scores):.8f}\")\n",
        "print(f\"  - Rango: [{np.min(combined_likelihood_scores):.8f}, {np.max(combined_likelihood_scores):.8f}]\")\n",
        "\n",
        "# Definir umbrales basados en percentiles de los puntajes combinados\n",
        "print(f\"\\n=== Definicion de umbrales para puntajes combinados ===\")\n",
        "threshold_percentiles = [5, 10, 20]\n",
        "combined_thresholds = []\n",
        "\n",
        "for perc in threshold_percentiles:\n",
        "    threshold_val = np.percentile(combined_likelihood_scores, perc)\n",
        "    combined_thresholds.append(threshold_val)\n",
        "    print(f\"Umbral {perc}%: {threshold_val:.8f}\")\n",
        "\n",
        "# Evaluar con cada umbral\n",
        "combined_results = {}\n",
        "\n",
        "print(f\"\\n=== Evaluacion con diferentes umbrales ===\")\n",
        "for i, (threshold_perc, threshold_val) in enumerate(zip(threshold_percentiles, combined_thresholds), 1):\n",
        "    print(f\"\\n--- UMBRAL {i}/3: {threshold_perc}% (puntaje ≥ {threshold_val:.8f}) ---\")\n",
        "    \n",
        "    # Clasificar basado en el umbral\n",
        "    predictions_combined = np.where(combined_likelihood_scores >= threshold_val, 'normal.', 'back.')\n",
        "    \n",
        "    # Calcular métricas\n",
        "    accuracy = accuracy_score(y_test_clean, predictions_combined)\n",
        "    cm = confusion_matrix(y_test_clean, predictions_combined, labels=['normal.', 'back.'])\n",
        "    \n",
        "    # Métricas detalladas\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    # Almacenar resultados\n",
        "    combined_results[f'threshold_{threshold_perc}%'] = {\n",
        "        'threshold_value': threshold_val,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'specificity': specificity,\n",
        "        'f1_score': f1_score,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': predictions_combined\n",
        "    }\n",
        "    \n",
        "    print(f\"Resultados:\")\n",
        "    print(f\"  - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  - Precision (detección de ataques): {precision:.4f}\")\n",
        "    print(f\"  - Recall (sensibilidad): {recall:.4f}\")\n",
        "    print(f\"  - Specificity (especificidad): {specificity:.4f}\")\n",
        "    print(f\"  - F1-Score: {f1_score:.4f}\")\n",
        "    print(f\"  - Matriz de confusión:\")\n",
        "    print(f\"    TN={tn} (Normal→Normal), FP={fp} (Normal→Ataque)\")\n",
        "    print(f\"    FN={fn} (Ataque→Normal), TP={tp} (Ataque→Ataque)\")\n",
        "    \n",
        "    # Graficar matriz de confusión\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
        "                xticklabels=['Predicted Normal', 'Predicted Attack'],\n",
        "                yticklabels=['Actual Normal', 'Actual Attack'])\n",
        "    plt.title(f'Matriz de Confusión: PROMEDIO DE 5 CARACTERÍSTICAS\\nUmbral: {threshold_perc}% - Accuracy: {accuracy:.4f}')\n",
        "    plt.ylabel('Etiqueta Real')\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparación con resultados individuales\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Comparacion: individual vs promedio combinado\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Tabla comparativa\n",
        "comparison_data = []\n",
        "\n",
        "# Agregar resultados del promedio combinado\n",
        "for threshold_key, results in combined_results.items():\n",
        "    comparison_data.append({\n",
        "        'Método': 'PROMEDIO_5_CARACTERÍSTICAS',\n",
        "        'Umbral': threshold_key,\n",
        "        'Accuracy': results['accuracy'],\n",
        "        'Precision': results['precision'],\n",
        "        'Recall': results['recall'],\n",
        "        'F1-Score': results['f1_score']\n",
        "    })\n",
        "\n",
        "# Agregar mejores resultados individuales para comparación\n",
        "for threshold_key in ['threshold_5%', 'threshold_10%', 'threshold_20%']:\n",
        "    # Encontrar la mejor característica individual para cada umbral\n",
        "    best_individual = None\n",
        "    best_f1 = -1\n",
        "    \n",
        "    for feature in top_5_features:\n",
        "        if threshold_key in classification_results[feature]:\n",
        "            f1 = classification_results[feature][threshold_key]['f1_score']\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_individual = {\n",
        "                    'feature': feature,\n",
        "                    'results': classification_results[feature][threshold_key]\n",
        "                }\n",
        "    \n",
        "    if best_individual:\n",
        "        comparison_data.append({\n",
        "            'Método': f'MEJOR_INDIVIDUAL_{best_individual[\"feature\"][:15]}',\n",
        "            'Umbral': threshold_key,\n",
        "            'Accuracy': best_individual['results']['accuracy'],\n",
        "            'Precision': best_individual['results']['precision'],\n",
        "            'Recall': best_individual['results']['recall'],\n",
        "            'F1-Score': best_individual['results']['f1_score']\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.round(4)\n",
        "\n",
        "print(\"Tabla comparativa:\")\n",
        "display_dataframe_completely(comparison_df)\n",
        "\n",
        "# Gráfico comparativo\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Accuracy\n",
        "plt.subplot(2, 2, 1)\n",
        "combined_data = comparison_df[comparison_df['Método'] == 'PROMEDIO_5_CARACTERÍSTICAS']\n",
        "individual_data = comparison_df[comparison_df['Método'] != 'PROMEDIO_5_CARACTERÍSTICAS']\n",
        "\n",
        "x_pos = np.arange(len(threshold_percentiles))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x_pos - width/2, combined_data['Accuracy'], width, \n",
        "        label='Promedio Combinado', color='darkgreen', alpha=0.8)\n",
        "plt.bar(x_pos + width/2, individual_data['Accuracy'], width, \n",
        "        label='Mejor Individual', color='orange', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Umbral')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy: Promedio vs Mejor Individual')\n",
        "plt.xticks(x_pos, [f'{p}%' for p in threshold_percentiles])\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Subplot 2: F1-Score\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(x_pos - width/2, combined_data['F1-Score'], width, \n",
        "        label='Promedio Combinado', color='darkgreen', alpha=0.8)\n",
        "plt.bar(x_pos + width/2, individual_data['F1-Score'], width, \n",
        "        label='Mejor Individual', color='orange', alpha=0.8)\n",
        "\n",
        "plt.xlabel('Umbral')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.title('F1-Score: Promedio vs Mejor Individual')\n",
        "plt.xticks(x_pos, [f'{p}%' for p in threshold_percentiles])\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Subplot 3: Precision vs Recall - Combinado\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.scatter(combined_data['Recall'], combined_data['Precision'], \n",
        "           s=150, c='darkgreen', alpha=0.8, label='Promedio Combinado')\n",
        "plt.scatter(individual_data['Recall'], individual_data['Precision'], \n",
        "           s=150, c='orange', alpha=0.8, label='Mejor Individual')\n",
        "\n",
        "for i, txt in enumerate([f'{p}%' for p in threshold_percentiles]):\n",
        "    plt.annotate(txt, (combined_data.iloc[i]['Recall'], combined_data.iloc[i]['Precision']), \n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "    plt.annotate(txt, (individual_data.iloc[i]['Recall'], individual_data.iloc[i]['Precision']), \n",
        "                xytext=(5, -15), textcoords='offset points')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision vs Recall')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Distribución de puntajes por clase\n",
        "plt.subplot(2, 2, 4)\n",
        "normal_scores = combined_likelihood_scores[y_test_clean == 'normal.']\n",
        "attack_scores = combined_likelihood_scores[y_test_clean == 'back.']\n",
        "\n",
        "plt.hist(normal_scores, bins=50, alpha=0.7, label='Normal', color='blue', density=True)\n",
        "plt.hist(attack_scores, bins=50, alpha=0.7, label='Backdoor', color='red', density=True)\n",
        "\n",
        "# Agregar líneas de umbrales\n",
        "for i, (perc, threshold) in enumerate(zip(threshold_percentiles, combined_thresholds)):\n",
        "    plt.axvline(threshold, color='black', linestyle='--', alpha=0.7, \n",
        "                label=f'Umbral {perc}%' if i == 0 else '')\n",
        "\n",
        "plt.xlabel('Puntaje Combinado (Promedio Verosimilitudes)')\n",
        "plt.ylabel('Densidad')\n",
        "plt.title('Distribución de Puntajes por Clase')\n",
        "plt.legend()\n",
        "plt.yscale('log')  # Escala logarítmica para mejor visualización\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== Analisis de resultados ===\")\n",
        "\n",
        "# Encontrar mejor resultado combinado\n",
        "best_combined = max(combined_results.items(), key=lambda x: x[1]['f1_score'])\n",
        "best_threshold, best_metrics = best_combined\n",
        "\n",
        "print(f\"Mejor resultado con promedio combinado:\")\n",
        "print(f\"  - Umbral: {best_threshold}\")\n",
        "print(f\"  - Accuracy: {best_metrics['accuracy']:.4f}\")\n",
        "print(f\"  - F1-Score: {best_metrics['f1_score']:.4f}\")\n",
        "print(f\"  - Precision: {best_metrics['precision']:.4f}\")\n",
        "print(f\"  - Recall: {best_metrics['recall']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar promedios de F1-Score\n",
        "combined_avg_f1 = np.mean([r['f1_score'] for r in combined_results.values()])\n",
        "individual_avg_f1 = np.mean([\n",
        "    max([classification_results[feat][f'threshold_{p}%']['f1_score'] \n",
        "         for feat in top_5_features]) \n",
        "    for p in threshold_percentiles\n",
        "])\n",
        "\n",
        "print(f\"F1-Score promedio - Combinado: {combined_avg_f1:.4f}\")\n",
        "print(f\"F1-Score promedio - Mejor Individual: {individual_avg_f1:.4f}\")\n",
        "\n",
        "if combined_avg_f1 > individual_avg_f1:\n",
        "    print(\" El método combinado muestra mejor rendimiento promedio\")\n",
        "else:\n",
        "    print(\" Los métodos individuales muestran mejor rendimiento promedio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicacion del Codigo\n",
        "\n",
        "Para este punto se definio la funcion llamada `calculate_combined_likelihood_scores` que se encarga de calcular el promedio de verosimilitudes para las cinco caracteristicas, para este efecto itera sobre cada una y obtiene los parametros del modelo exponencial como `scale` y `loc`, se obtienen los datos de prueba, se transforman datos si es necesario como valores no negativos y se calcula la verosimilitud con `stats.expon.pdf` y se suma a una variable que guarda las puntuaciones combinadas de cada caracteristica, finalmente esta divide el puntaje total encontrado por el numero de caracteristicas para obtener el promedio.\n",
        "\n",
        "Despues de la definicion de la funcion anterior se preparan los datos de prueba en el cual se extraen los valores numericos por cada caracteristica y se validan que sean validos y no NaN.\n",
        "\n",
        "Lo siguiente es que se calcula los umbrales adaptativos, estos utilizan los mismos procentajes de 5%, 10% y 20% del enunciado anterior y se calculan directamente de los percentiles de los puntajes combinados por medio de `np.percentile` y se guardan todos estos umbrales en una lista.\n",
        "\n",
        "Finalmente para el calculo con los umbrales, se itera sobre cada uno y se determina por medio de la puntuacion de verosimilitud combinada si es mayor o igual al umbral, si este es mayor o igual es trafico normal, de lo contrario es de tipo back. Se generan las metricas de accuracy, matriz de confusion, precision, recall, especificidad en conjunto con el F1-Score, para ser almacenados en una variable para su uso posterior.\n",
        "\n",
        "Para generar una tabla de resumen, en base a los resultados guardados anteriormente se calcula cual umbral optuvo las mejores metricas y se compara con el mejor individual de los resultados anteriores. Ademas se generan graficos que comparan el accuracy, F1-Score, precision y recall, asi como la distribucion de los puntajes combinados.\n",
        "\n",
        "## Interpretacion de Resultados\n",
        "\n",
        "Se puede encontrar en los resultados que la mejor salida utilizando el promedio de verosimilitud fue el umbral de 10%, este obtuvo un accuracy de 0.9110, precision de 0.1090, recall de 1 y F1-Score de 0.1966, en general fue el mejor, solo su accuracy fue sobrepasado por el umbral 5%, sin embargo el umbral 5% presenta algunas deficiencias en el precision y recall que lo hacen inferior al umbral 10%, de igual manera el resultado del umbral 20% empeora un poco con respecto al 10% sin embargo es mejor que el de 5%. Si se puede salvar que favorece mucho al recall ya que los umbrales 10% y 20% tienen 1 de recall, sin embargo la precision toma valores muy bajos.\n",
        "\n",
        "Cuando vamos a ver las matrices de confusion, para el umbral de 5% este obtuvo aparantemente buenos resultados a pesar de sus metricas bajas dado que optuvo verderos negativos en 25119, falsos positivos en 1231, falsos negativos en 189 y verdaderos positivos en 101. En comparacion con el umbral de mejores metricas que fue el de 10% que obtuvo 23980 de verdaderos negativos, 2370 de falsos positivos, 0 de falsos negativos y 290 de verdaderos positicos. Comparando las matrices se puede observar que los falsos negativos disminuyen con los umbrales mas altos pero los falsos positivos tienden a aumentar conforme se aumenta el umbral.\n",
        "\n",
        "Al compararlo con los resultados del paso anterior que utiliza la verosimilitud de cada caracteristica, se puede ver como es mucho mejor los resultados individuales de la caracteristica `hot` respecto a cualquier umbral utilizando los promedios de verosimilitud, ya que valores como accuracy, precision y F1-Score tienen un valor mucho mas alto que la contraparte que usa el promedio de las cinco caracteristicas, el unico valor muy similar fue el recall.\n",
        "\n",
        "Al ver que los resultados individuales como por ejemplo el mejor caso el mejor que fue `hot` se puede ver como al utilizar el promedio puede cambiar la eficiencia ya que se utilizan caracteristicas menos discriminativas bajando su poder, especialmente se observo que caracteristicas como `service_http` que individualmente dio resultados pobres, es posible que esta y otras caracteristicas con bajos resultados hicieran los resultados bajaran un poco.\n",
        "\n",
        "En el caso de utilizarlo en un sistema real los resultados combinados proveen un muy buen recall que por medio de las pruebas podria perder muy pocos ataques reales, sin embargo tiene muy poca precision lo que genera muchas falsas alarmas siendo detrimental en un sistema de deteccion.\n",
        "\n",
        "En general para las pruebas realizadas lo mejor seria utilizar la caracteristica `hot` o alguna otra que obtuvo un resultado alto, los resultados con promedio no dieron resultados optimos por lo que no serian candidatos a utilizar en un sistema de deteccion para el dataset utilizado que fue el KDD99."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
