{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KDD99 Decision Tree Analysis with Scikit-Learn\n",
        "\n",
        "**Comparing Custom Implementation with SKLearn**\n",
        "\n",
        "This notebook implements the same KDD99 decision tree analysis using scikit-learn's optimized implementation, allowing us to compare performance and results with our custom CART implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🚀 KDD99 Decision Tree Analysis with Scikit-Learn\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "Loading the same KDD99 dataset used in the original analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the KDD99 dataset\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "try:\n",
        "    path = get_file('kddcup.data_10_percent.gz',\n",
        "                    origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
        "except:\n",
        "    print('Error downloading')\n",
        "    raise\n",
        "\n",
        "print(f\"Dataset path: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and setup DataFrame\n",
        "pd_data_frame = pd.read_csv(path, header=None)\n",
        "\n",
        "# Add column names\n",
        "pd_data_frame.columns = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
        "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
        "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
        "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
        "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'outcome'\n",
        "]\n",
        "\n",
        "print(f\"📊 Dataset original: {pd_data_frame.shape}\")\n",
        "print(f\"📊 Columnas: {len(pd_data_frame.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean data\n",
        "pd_data_frame.dropna(inplace=True, axis=1)\n",
        "pd_data_frame.drop_duplicates(keep='first', inplace=True)\n",
        "\n",
        "# Filter for normal and backdoor only\n",
        "filtered_df = pd_data_frame[pd_data_frame['outcome'].isin(['normal.', 'back.'])].copy()\n",
        "print(f\"📊 Dataset filtrado (normal + backdoor): {filtered_df.shape}\")\n",
        "\n",
        "# Show class distribution\n",
        "print(f\"📊 Distribución de clases:\")\n",
        "print(filtered_df['outcome'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding for categorical features\n",
        "list_nominal_features = [\"flag\", \"protocol_type\", \"service\"]\n",
        "df_encoded = pd.get_dummies(filtered_df, columns=list_nominal_features)\n",
        "\n",
        "# Convert boolean columns to integers\n",
        "for col in df_encoded.columns:\n",
        "    if df_encoded[col].dtype == 'bool':\n",
        "        df_encoded[col] = df_encoded[col].astype(int)\n",
        "\n",
        "print(f\"📊 Dataset después de encoding: {df_encoded.shape}\")\n",
        "print(f\"📊 Distribución de clases final:\")\n",
        "print(df_encoded['outcome'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation for SKLearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_sklearn_dataset(df_encoded):\n",
        "    \"\"\"\n",
        "    Prepara los datos para scikit-learn\n",
        "    \"\"\"\n",
        "    # Separar features y target\n",
        "    X = df_encoded.drop('outcome', axis=1)\n",
        "    y = df_encoded['outcome']\n",
        "    \n",
        "    # Codificar target: normal. -> 0, back. -> 1\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    \n",
        "    print(f\"🔢 Features shape: {X.shape}\")\n",
        "    print(f\"🔢 Target distribution: {np.bincount(y_encoded)}\")\n",
        "    print(f\"🔢 Class mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
        "    \n",
        "    return X, y_encoded, label_encoder, X.columns.tolist()\n",
        "\n",
        "X, y, label_encoder, feature_names = prepare_sklearn_dataset(df_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Evaluation Function with SKLearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_sklearn_tree(X, y, max_depth, min_samples_leaf=2, test_size=0.3, random_state=42):\n",
        "    \"\"\"\n",
        "    Evalúa un árbol de decisión de sklearn\n",
        "    \"\"\"\n",
        "    # Train-test split si test_size > 0, sino usa todo el dataset\n",
        "    if test_size > 0:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "        )\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = X, X, y, y\n",
        "    \n",
        "    # Crear modelo\n",
        "    clf = DecisionTreeClassifier(\n",
        "        max_depth=max_depth,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=random_state,\n",
        "        criterion='gini'  # Equivalente a nuestro Gini personalizado\n",
        "    )\n",
        "    \n",
        "    # Entrenar\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Predecir\n",
        "    start_time = time.time()\n",
        "    y_pred = clf.predict(X_test)\n",
        "    evaluation_time = time.time() - start_time\n",
        "    \n",
        "    # Métricas\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    \n",
        "    return {\n",
        "        'model': clf,\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'training_time': training_time,\n",
        "        'evaluation_time': evaluation_time,\n",
        "        'train_size': len(X_train),\n",
        "        'test_size': len(X_test),\n",
        "        'y_test': y_test,\n",
        "        'y_pred': y_pred\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Same Dataset for Training and Testing\n",
        "\n",
        "Equivalent to our custom implementation's first evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PART 1: SAME DATASET FOR TRAINING AND TESTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_part1 = {}\n",
        "\n",
        "for max_depth in [3, 4]:\n",
        "    print(f\"\\n🌳 Evaluating Decision Tree with max_depth={max_depth}\")\n",
        "    \n",
        "    result = evaluate_sklearn_tree(X, y, max_depth=max_depth, min_samples_leaf=2, test_size=0)\n",
        "    results_part1[max_depth] = result\n",
        "    \n",
        "    print(f\"   📊 Results:\")\n",
        "    print(f\"      • Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\")\n",
        "    print(f\"      • F1-Score (macro): {result['f1_score']:.4f}\")\n",
        "    print(f\"      • Training time: {result['training_time']:.4f} seconds\")\n",
        "    print(f\"      • Evaluation time: {result['evaluation_time']:.4f} seconds\")\n",
        "    print(f\"      • Tree depth: {result['model'].get_depth()}\")\n",
        "    print(f\"      • Number of leaves: {result['model'].get_n_leaves()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison table Part 1\n",
        "print(f\"\\n📊 COMPARISON TABLE - PART 1\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Metric':<20} {'Depth 3':<15} {'Depth 4':<15}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Accuracy':<20} {results_part1[3]['accuracy']:<15.4f} {results_part1[4]['accuracy']:<15.4f}\")\n",
        "print(f\"{'F1-Score':<20} {results_part1[3]['f1_score']:<15.4f} {results_part1[4]['f1_score']:<15.4f}\")\n",
        "print(f\"{'Train Time (s)':<20} {results_part1[3]['training_time']:<15.4f} {results_part1[4]['training_time']:<15.4f}\")\n",
        "print(f\"{'Eval Time (s)':<20} {results_part1[3]['evaluation_time']:<15.4f} {results_part1[4]['evaluation_time']:<15.4f}\")\n",
        "print(f\"{'Tree Depth':<20} {results_part1[3]['model'].get_depth():<15} {results_part1[4]['model'].get_depth():<15}\")\n",
        "print(f\"{'Num Leaves':<20} {results_part1[3]['model'].get_n_leaves():<15} {results_part1[4]['model'].get_n_leaves():<15}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: 10 Random Splits (70%-30%)\n",
        "\n",
        "Equivalent to our custom implementation's second evaluation with cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multiple_runs_sklearn(X, y, max_depth, n_runs=10, min_samples_leaf=2):\n",
        "    \"\"\"\n",
        "    Ejecuta múltiples evaluaciones con particiones aleatorias\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    print(f\"🔄 Running {n_runs} evaluations with max_depth={max_depth}\")\n",
        "    \n",
        "    for run in range(n_runs):\n",
        "        result = evaluate_sklearn_tree(\n",
        "            X, y, \n",
        "            max_depth=max_depth, \n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            test_size=0.3, \n",
        "            random_state=42+run\n",
        "        )\n",
        "        results.append(result)\n",
        "        print(f\"   Run {run+1:2d}: Acc={result['accuracy']:.3f}, F1={result['f1_score']:.3f}\")\n",
        "    \n",
        "    # Statistics\n",
        "    accuracies = [r['accuracy'] for r in results]\n",
        "    f1_scores = [r['f1_score'] for r in results]\n",
        "    train_times = [r['training_time'] for r in results]\n",
        "    eval_times = [r['evaluation_time'] for r in results]\n",
        "    \n",
        "    # Find best run by F1-score\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    \n",
        "    return {\n",
        "        'accuracy_mean': np.mean(accuracies),\n",
        "        'accuracy_std': np.std(accuracies),\n",
        "        'f1_mean': np.mean(f1_scores),\n",
        "        'f1_std': np.std(f1_scores),\n",
        "        'train_time_mean': np.mean(train_times),\n",
        "        'train_time_std': np.std(train_times),\n",
        "        'eval_time_mean': np.mean(eval_times),\n",
        "        'eval_time_std': np.std(eval_times),\n",
        "        'best_idx': best_idx,\n",
        "        'best_model': results[best_idx]['model'],\n",
        "        'best_f1': f1_scores[best_idx],\n",
        "        'all_results': results\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PART 2: 10 RANDOM SPLITS (70% TRAIN - 30% TEST)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Execute multiple runs\n",
        "results_part2 = {}\n",
        "for max_depth in [2, 3]:\n",
        "    results_part2[max_depth] = multiple_runs_sklearn(X, y, max_depth=max_depth, n_runs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Results table Part 2\n",
        "print(f\"\\n📊 RESULTS TABLE - PART 2 (Mean ± Std)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<20} {'Depth 2':<25} {'Depth 3':<25}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for metric in ['accuracy', 'f1', 'train_time', 'eval_time']:\n",
        "    depth2_mean = results_part2[2][f'{metric}_mean']\n",
        "    depth2_std = results_part2[2][f'{metric}_std']\n",
        "    depth3_mean = results_part2[3][f'{metric}_mean']\n",
        "    depth3_std = results_part2[3][f'{metric}_std']\n",
        "    \n",
        "    metric_name = metric.replace('_', ' ').title()\n",
        "    if 'time' in metric:\n",
        "        metric_name += ' (s)'\n",
        "    \n",
        "    print(f\"{metric_name:<20} {depth2_mean:.4f} ± {depth2_std:.4f:<12} {depth3_mean:.4f} ± {depth3_std:.4f}\")\n",
        "\n",
        "# Best runs summary\n",
        "print(f\"\\n🏆 BEST RUNS SUMMARY:\")\n",
        "for depth in [2, 3]:\n",
        "    best_idx = results_part2[depth]['best_idx']\n",
        "    best_f1 = results_part2[depth]['best_f1']\n",
        "    print(f\"   • Depth {depth}: Run {best_idx+1} with F1-Score = {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tree Visualization\n",
        "\n",
        "Visualizing the best decision trees from our analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_best_tree(model, feature_names, title, max_depth):\n",
        "    \"\"\"\n",
        "    Visualiza el mejor árbol de decisión\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plot_tree(model, \n",
        "             feature_names=feature_names,\n",
        "             class_names=['Normal', 'Backdoor'],\n",
        "             filled=True,\n",
        "             rounded=True,\n",
        "             fontsize=10)\n",
        "    plt.title(f'{title} (Max Depth: {max_depth})', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Also print text representation\n",
        "    print(f\"\\n📝 Text representation of {title}:\")\n",
        "    print(\"-\" * 50)\n",
        "    tree_rules = export_text(model, feature_names=feature_names, show_weights=True)\n",
        "    print(tree_rules[:1000] + \"...\" if len(tree_rules) > 1000 else tree_rules)\n",
        "\n",
        "print(f\"\\n🌳 TREE VISUALIZATIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Visualize best trees from Part 2\n",
        "for depth in [2, 3]:\n",
        "    best_model = results_part2[depth]['best_model']\n",
        "    best_f1 = results_part2[depth]['best_f1']\n",
        "    visualize_best_tree(\n",
        "        best_model, \n",
        "        feature_names, \n",
        "        f'Best Tree Depth {depth} (F1={best_f1:.3f})', \n",
        "        depth\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance Analysis\n",
        "\n",
        "Analyzing which features are most important for the decision trees.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_feature_importance(model, feature_names, title):\n",
        "    \"\"\"\n",
        "    Analiza la importancia de las características\n",
        "    \"\"\"\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    \n",
        "    print(f\"\\n📊 Feature Importance - {title}\")\n",
        "    print(\"-\" * 40)\n",
        "    for i in range(min(10, len(indices))):  # Top 10 features\n",
        "        idx = indices[i]\n",
        "        print(f\"{i+1:2d}. {feature_names[idx]:<30} {importances[idx]:.4f}\")\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = min(15, len(indices))\n",
        "    plt.barh(range(top_features), importances[indices[:top_features]])\n",
        "    plt.yticks(range(top_features), [feature_names[indices[i]] for i in range(top_features)])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'Top {top_features} Feature Importances - {title}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return indices, importances\n",
        "\n",
        "# Analyze feature importance for best models\n",
        "feature_importance_results = {}\n",
        "for depth in [2, 3]:\n",
        "    indices, importances = analyze_feature_importance(\n",
        "        results_part2[depth]['best_model'], \n",
        "        feature_names, \n",
        "        f'Depth {depth}'\n",
        "    )\n",
        "    feature_importance_results[depth] = {'indices': indices, 'importances': importances}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison and Analysis\n",
        "\n",
        "Analysis of advantages and differences between SKLearn and custom implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    \"\"\"\n",
        "    Plotea la matriz de confusión\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Normal', 'Backdoor'],\n",
        "                yticklabels=['Normal', 'Backdoor'])\n",
        "    plt.title(f'Confusion Matrix - {title}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print classification report\n",
        "    print(f\"\\n📊 Classification Report - {title}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(classification_report(y_true, y_pred, target_names=['Normal', 'Backdoor']))\n",
        "\n",
        "# Show confusion matrices for best models\n",
        "print(\"🔍 CONFUSION MATRIX ANALYSIS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for depth in [2, 3]:\n",
        "    best_result = results_part2[depth]['all_results'][results_part2[depth]['best_idx']]\n",
        "    plot_confusion_matrix(\n",
        "        best_result['y_test'], \n",
        "        best_result['y_pred'], \n",
        "        f'Best Depth {depth} Model'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"⚖️  COMPARISON: SKLEARN vs CUSTOM IMPLEMENTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"🔍 Key Observations:\")\n",
        "print(f\"   • SKLearn trees are typically faster due to optimized C implementation\")\n",
        "print(f\"   • Both should achieve similar accuracy/F1 scores with same parameters\")\n",
        "print(f\"   • SKLearn provides built-in feature importance and visualization\")\n",
        "print(f\"   • Custom implementation gives more control over splitting criteria\")\n",
        "\n",
        "print(f\"\\n💡 Advantages of SKLearn:\")\n",
        "print(f\"   ✅ Much faster training and inference\")\n",
        "print(f\"   ✅ Built-in pruning and optimization\")\n",
        "print(f\"   ✅ Extensive visualization tools\")\n",
        "print(f\"   ✅ Feature importance calculation\")\n",
        "print(f\"   ✅ Well-tested and optimized\")\n",
        "print(f\"   ✅ Cross-validation utilities\")\n",
        "\n",
        "print(f\"\\n💡 Advantages of Custom Implementation:\")\n",
        "print(f\"   ✅ Full control over splitting logic\")\n",
        "print(f\"   ✅ Custom stopping criteria\")\n",
        "print(f\"   ✅ Educational value - understanding internals\")\n",
        "print(f\"   ✅ Ability to modify Gini calculation\")\n",
        "print(f\"   ✅ Custom node features and XML export\")\n",
        "print(f\"   ✅ Integration with PyTorch tensors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Results Summary\n",
        "\n",
        "Comprehensive summary of the analysis and conclusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n🎯 FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Find overall best configuration\n",
        "best_depth = 2 if results_part2[2]['f1_mean'] > results_part2[3]['f1_mean'] else 3\n",
        "best_stats = results_part2[best_depth]\n",
        "\n",
        "print(f\"🏆 BEST OVERALL CONFIGURATION:\")\n",
        "print(f\"   • Best Depth: {best_depth}\")\n",
        "print(f\"   • Mean Accuracy: {best_stats['accuracy_mean']:.4f} ± {best_stats['accuracy_std']:.4f}\")\n",
        "print(f\"   • Mean F1-Score: {best_stats['f1_mean']:.4f} ± {best_stats['f1_std']:.4f}\")\n",
        "print(f\"   • Mean Training Time: {best_stats['train_time_mean']:.4f}s ± {best_stats['train_time_std']:.4f}s\")\n",
        "\n",
        "print(f\"\\n📊 COMPARISON BETWEEN DEPTHS:\")\n",
        "acc_diff = abs(results_part2[3]['accuracy_mean'] - results_part2[2]['accuracy_mean'])\n",
        "f1_diff = abs(results_part2[3]['f1_mean'] - results_part2[2]['f1_mean'])\n",
        "time_diff = abs(results_part2[3]['train_time_mean'] - results_part2[2]['train_time_mean'])\n",
        "\n",
        "print(f\"   • Accuracy difference: {acc_diff:.4f}\")\n",
        "print(f\"   • F1-Score difference: {f1_diff:.4f}\")\n",
        "print(f\"   • Training time difference: {time_diff:.4f}s\")\n",
        "\n",
        "print(f\"\\n💡 OPTIMIZATION PROPOSAL WITH JENSEN-SHANNON DISTANCE:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"The Jensen-Shannon distance from Part 1 analysis could be used for:\")\n",
        "print(f\"   1. 🎯 Pre-select most discriminative features before training\")\n",
        "print(f\"   2. ⚡ Reduce search space in feature selection\")\n",
        "print(f\"   3. 🔍 Prioritize splits on features with higher class separability\")\n",
        "print(f\"   4. ⚙️  Implement early stopping based on low JS distances\")\n",
        "print(f\"   5. 📊 Use JS as alternative criterion to Gini for more informative splits\")\n",
        "\n",
        "print(f\"\\n🎉 CONCLUSIONS:\")\n",
        "print(f\"   • SKLearn Decision Trees provide excellent performance on KDD99 dataset\")\n",
        "print(f\"   • {best_depth}-depth trees show optimal balance of performance vs complexity\")\n",
        "print(f\"   • Feature importance analysis reveals key discriminative features\")\n",
        "print(f\"   • Results are comparable to custom implementation with better efficiency\")\n",
        "\n",
        "print(f\"\\n✅ SKLearn Decision Tree Analysis Complete!\")\n",
        "print(f\"📄 This notebook demonstrates how the same KDD99 analysis can be\")\n",
        "print(f\"   performed using scikit-learn's optimized implementation.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
